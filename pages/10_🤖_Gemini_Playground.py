"""
M√≥dulo para pages/10_ü§ñ_Gemini_Playground.py. Define componentes da interface de utilizador (UI).
"""

from dotenv import load_dotenv
load_dotenv(override=True)

import streamlit as st
from datetime import datetime
import logging
import json

# Configurar logging
logger = logging.getLogger(__name__)

# --- Verifica√ß√£o de Autentica√ß√£o e Permiss√£o ---
if st.session_state.get("authenticated") and st.session_state.get("role") == "admin":
    # --- Configura√ß√£o da P√°gina ---
    st.set_page_config(
        page_title="Gemini Playground",
        page_icon="ü§ñ",
        layout="wide"
    )

    st.markdown(
        "<h1 class='main-header'>ü§ñ Gemini Playground</h1>",
        unsafe_allow_html=True,
    )
    st.markdown(
        "<p class='sub-header'>Teste e experimente com o modelo Gemini 2.5 Flash.</p>",
        unsafe_allow_html=True,
    )

    # Importar componentes necess√°rios
    try:
        from core.llm_adapter import GeminiLLMAdapter
        from core.config.safe_settings import get_safe_settings

        settings = get_safe_settings()

        # Verificar se a API key est√° configurada
        if not settings.GEMINI_API_KEY:
            st.error("‚ùå API Key do Gemini n√£o configurada. Configure GEMINI_API_KEY nas vari√°veis de ambiente.")
            st.stop()

        # Inicializar o adaptador do Gemini
        if 'gemini_adapter' not in st.session_state:
            # Usar o modelo espec√≠fico do Gemini se dispon√≠vel
            gemini_model = getattr(settings, 'GEMINI_MODEL_NAME', settings.LLM_MODEL_NAME)
            st.session_state.gemini_adapter = GeminiLLMAdapter(
                api_key=settings.GEMINI_API_KEY,
                model_name=gemini_model,
                enable_cache=True
            )

        gemini = st.session_state.gemini_adapter

        # Layout em colunas
        col1, col2 = st.columns([2, 1])

        with col2:
            st.subheader("‚öôÔ∏è Configura√ß√µes")

            if st.button("üîÑ Recarregar Configura√ß√£o da API", use_container_width=True, help="Clique aqui se voc√™ atualizou sua chave de API no arquivo .env para for√ßar o recarregamento."):
                from core.config.safe_settings import reset_safe_settings_cache
                # Limpa o cache de settings para for√ßar a releitura do .env
                reset_safe_settings_cache()
                # For√ßa a remo√ß√£o do adaptador da sess√£o para que ele seja recriado com a nova chave
                if 'gemini_adapter' in st.session_state:
                    del st.session_state['gemini_adapter']
                st.success("Configura√ß√£o da API recarregada. Tente seu prompt novamente.")
                st.rerun()
            
            st.markdown("---")

            # Par√¢metros do modelo
            temperature = st.slider(
                "Temperature",
                min_value=0.0,
                max_value=2.0,
                value=0.7,
                step=0.1,
                help="Controla a criatividade das respostas. Valores mais baixos s√£o mais determin√≠sticos."
            )

            max_tokens = st.slider(
                "Max Tokens",
                min_value=256,
                max_value=8192,
                value=2048,
                step=256,
                help="N√∫mero m√°ximo de tokens na resposta (Gemini conta prompt + resposta)."
            )

            # Aviso se max_tokens muito baixo
            if max_tokens < 512:
                st.warning("‚ö†Ô∏è Valor muito baixo! Respostas podem ser cortadas. Recomendado: ‚â• 1024 tokens.")

            json_mode = st.checkbox(
                "JSON Mode",
                value=False,
                help="For√ßa o modelo a retornar respostas em formato JSON."
            )

            stream_mode = st.checkbox(
                "Stream Mode",
                value=False,
                help="Exibe as respostas em tempo real (streaming)."
            )

            # Informa√ß√µes do modelo
            st.markdown("---")
            st.subheader("üìä Informa√ß√µes")
            st.info(f"**Modelo:** {gemini.model_name}")

            # Estat√≠sticas do cache
            cache_stats = gemini.get_cache_stats()
            if cache_stats.get("cache_enabled"):
                st.metric("Cache Hits", cache_stats.get("hits", 0))
                st.metric("Cache Misses", cache_stats.get("misses", 0))
                hit_rate = cache_stats.get("hit_rate", 0) * 100
                st.metric("Taxa de Acerto", f"{hit_rate:.1f}%")
            else:
                st.warning("Cache desabilitado")

            # Bot√£o para limpar hist√≥rico
            st.markdown("---")
            if st.button("üóëÔ∏è Limpar Hist√≥rico", use_container_width=True):
                st.session_state.chat_history = []
                st.rerun()

        with col1:
            st.subheader("üí¨ Chat")

            # Inicializar hist√≥rico de chat
            if 'chat_history' not in st.session_state:
                st.session_state.chat_history = []

            # Container para o hist√≥rico de mensagens
            chat_container = st.container()

            # Exibir hist√≥rico
            with chat_container:
                for message in st.session_state.chat_history:
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"])

            # Input do usu√°rio
            user_input = st.chat_input("Digite sua mensagem...")

            if user_input:
                # Adicionar mensagem do usu√°rio ao hist√≥rico
                st.session_state.chat_history.append({
                    "role": "user",
                    "content": user_input
                })

                # Preparar mensagens para o modelo (incluindo a mensagem do usu√°rio)
                messages = [
                    {"role": msg["role"], "content": msg["content"]}
                    for msg in st.session_state.chat_history
                ]

                # Obter resposta do modelo
                try:
                    if stream_mode:
                        # Modo streaming
                        with st.spinner("Gerando resposta..."):
                            stream = gemini.get_completion(
                                messages=messages,
                                temperature=temperature,
                                max_tokens=max_tokens,
                                json_mode=json_mode,
                                stream=True
                            )

                            full_response = ""
                            for chunk in stream:
                                full_response += chunk

                            response_content = full_response

                    else:
                        # Modo normal
                        with st.spinner("Gerando resposta..."):
                            response = gemini.get_completion(
                                messages=messages,
                                temperature=temperature,
                                max_tokens=max_tokens,
                                json_mode=json_mode,
                                stream=False
                            )

                        if "error" in response:
                            error_msg = f"‚ùå Erro: {response['error']}"
                            if response.get("fallback_activated"):
                                error_msg += f"\n\nüîÑ Fallback sugerido: {response.get('retry_with', 'N/A')}"
                            response_content = error_msg
                        else:
                            response_content = response.get("content", "")
                            if not response_content:
                                response_content = "‚ùå Resposta vazia recebida do modelo."

                except Exception as e:
                    error_msg = f"‚ùå Erro ao gerar resposta: {str(e)}"
                    response_content = error_msg
                    logger.error(f"Erro no playground: {e}", exc_info=True)

                # Adicionar resposta ao hist√≥rico
                st.session_state.chat_history.append({
                    "role": "assistant",
                    "content": response_content
                })

                # For√ßar rerun para exibir a conversa atualizada
                st.rerun()

            # Exemplos de prompts
            st.markdown("---")
            st.subheader("üí° Exemplos de Prompts")

            # Inicializar vari√°vel de exemplo se n√£o existir
            if 'selected_example' not in st.session_state:
                st.session_state.selected_example = ""

            col_ex1, col_ex2, col_ex3 = st.columns(3)

            with col_ex1:
                if st.button("üìù An√°lise de Dados", use_container_width=True):
                    st.session_state.selected_example = "Explique como fazer uma an√°lise explorat√≥ria de dados de vendas."
                    st.rerun()

            with col_ex2:
                if st.button("üîç SQL Query", use_container_width=True):
                    st.session_state.selected_example = "Crie uma query SQL para calcular o total de vendas por categoria nos √∫ltimos 30 dias."
                    st.rerun()

            with col_ex3:
                if st.button("üìä Python Code", use_container_width=True):
                    st.session_state.selected_example = "Escreva c√≥digo Python para criar um gr√°fico de barras com matplotlib."
                    st.rerun()

            # Mostrar campo edit√°vel se um exemplo foi selecionado
            if st.session_state.selected_example:
                st.markdown("---")
                st.info("‚úèÔ∏è Edite o prompt abaixo antes de enviar, se desejar:")

                edited_prompt = st.text_area(
                    "Prompt:",
                    value=st.session_state.selected_example,
                    height=100,
                    key="editable_example"
                )

                col_send, col_cancel = st.columns([1, 1])

                with col_send:
                    if st.button("üì§ Enviar", use_container_width=True, type="primary"):
                        if edited_prompt.strip():
                            # Adicionar ao hist√≥rico
                            st.session_state.chat_history.append({
                                "role": "user",
                                "content": edited_prompt
                            })

                            # Preparar mensagens para o modelo
                            messages = [
                                {"role": msg["role"], "content": msg["content"]}
                                for msg in st.session_state.chat_history
                            ]

                            # Obter resposta do modelo
                            try:
                                if stream_mode:
                                    # Modo streaming
                                    with st.spinner("Gerando resposta..."):
                                        stream = gemini.get_completion(
                                            messages=messages,
                                            temperature=temperature,
                                            max_tokens=max_tokens,
                                            json_mode=json_mode,
                                            stream=True
                                        )

                                        full_response = ""
                                        for chunk in stream:
                                            full_response += chunk

                                        response_content = full_response

                                else:
                                    # Modo normal
                                    with st.spinner("Gerando resposta..."):
                                        response = gemini.get_completion(
                                            messages=messages,
                                            temperature=temperature,
                                            max_tokens=max_tokens,
                                            json_mode=json_mode,
                                            stream=False
                                        )

                                    if "error" in response:
                                        error_msg = f"‚ùå Erro: {response['error']}"
                                        if response.get("fallback_activated"):
                                            error_msg += f"\n\nüîÑ Fallback sugerido: {response.get('retry_with', 'N/A')}"
                                        response_content = error_msg
                                    else:
                                        response_content = response.get("content", "")
                                        if not response_content:
                                            response_content = "‚ùå Resposta vazia recebida do modelo."

                            except Exception as e:
                                error_msg = f"‚ùå Erro ao gerar resposta: {str(e)}"
                                response_content = error_msg
                                logger.error(f"Erro no playground ao processar exemplo: {e}", exc_info=True)

                            # Adicionar resposta ao hist√≥rico
                            st.session_state.chat_history.append({
                                "role": "assistant",
                                "content": response_content
                            })

                            # Limpar exemplo selecionado
                            st.session_state.selected_example = ""

                            # Recarregar para mostrar a conversa atualizada
                            st.rerun()

                with col_cancel:
                    if st.button("‚ùå Cancelar", use_container_width=True):
                        st.session_state.selected_example = ""
                        st.rerun()

    except ImportError as e:
        st.error(f"‚ùå Erro ao importar componentes necess√°rios: {e}")
        st.info("Verifique se todas as depend√™ncias est√£o instaladas.")
        logger.error(f"Erro de importa√ß√£o no Gemini Playground: {e}")
    except Exception as e:
        st.error(f"‚ùå Erro inesperado: {e}")
        logger.error(f"Erro no Gemini Playground: {e}", exc_info=True)

    st.markdown(
        f"<div class='footer'>Desenvolvido para An√°lise de Dados Ca√ßula ¬© {datetime.now().year}</div>",
        unsafe_allow_html=True,
    )

# Se n√£o for admin, mostra uma mensagem de erro e oculta o conte√∫do.
elif st.session_state.get("authenticated"):
    st.error("‚ùå Acesso negado. Voc√™ n√£o tem permiss√£o para acessar esta p√°gina.")
    st.info("‚ÑπÔ∏è Esta p√°gina √© exclusiva para administradores.")
    st.stop()

# Se n√£o estiver logado, redireciona para o login
else:
    st.error("‚ùå Acesso negado. Por favor, fa√ßa o login na p√°gina principal para acessar esta √°rea.")
    st.stop()
