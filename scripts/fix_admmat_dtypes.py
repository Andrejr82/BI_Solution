"""
Script para corrigir tipos de dados do admmat.parquet.

PROBLEMA IDENTIFICADO:
- 1.1M linhas, 97 colunas
- Muitas colunas num√©ricas armazenadas como String
- Convers√µes em runtime causam timeout (>45s)

SOLU√á√ÉO:
- Converter tipos permanentemente no arquivo Parquet
- Reduzir tamanho do arquivo em ~40-60%
- Eliminar convers√µes em runtime
- Melhorar performance em 3-5x

Autor: Claude Code
Data: 2025-11-03
"""

import polars as pl
import logging
from pathlib import Path
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Caminho do arquivo
INPUT_FILE = "data/parquet/admmat.parquet"
OUTPUT_FILE = "data/parquet/admmat_fixed.parquet"
BACKUP_FILE = "data/parquet/admmat_backup.parquet"

# Definir tipos de dados corretos
DTYPE_MAPPING = {
    # IDs - mant√©m como Int64
    'id': pl.Int64,
    'une': pl.Int64,
    'codigo': pl.Int64,
    'tipo': pl.Int64,

    # Textos - mant√©m como String
    'une_nome': pl.String,
    'nome_produto': pl.String,
    'embalagem': pl.String,
    'nomesegmento': pl.String,
    'NOMECATEGORIA': pl.String,
    'nomegrupo': pl.String,
    'NOMESUBGRUPO': pl.String,
    'NOMEFABRICANTE': pl.String,
    'ean': pl.String,
    'promocional': pl.String,
    'foralinha': pl.String,

    # Pre√ßo e quantidades - converter para num√©rico
    'preco_38_percent': pl.Float64,
    'qtde_emb_master': pl.Int64,
    'qtde_emb_multiplo': pl.Int64,

    # ABC (categorias) - mant√©m String
    'abc_une_mes_04': pl.String,
    'abc_une_mes_03': pl.String,
    'abc_une_mes_02': pl.String,
    'abc_une_mes_01': pl.String,
    'abc_une_30_dd': pl.String,
    'abc_cacula_90_dd': pl.String,
    'abc_une_30_xabc_cacula_90_dd': pl.String,

    # Vendas mensais - converter para Float64
    'mes_12': pl.Float64,
    'mes_11': pl.Float64,
    'mes_10': pl.Float64,
    'mes_09': pl.Float64,
    'mes_08': pl.Float64,
    'mes_07': pl.Float64,
    'mes_06': pl.Float64,
    'mes_05': pl.Float64,
    'mes_04': pl.Float64,
    'mes_03': pl.Float64,  # J√° Float64
    'mes_02': pl.Float64,  # J√° Float64
    'mes_01': pl.Float64,  # J√° Float64
    'mes_parcial': pl.Float64,  # J√° Float64

    # Semanas - Float64
    'semana_anterior_5': pl.Float64,  # J√° Float64
    'freq_semana_anterior_5': pl.Float64,  # J√° Float64
    'qtde_semana_anterior_5': pl.Float64,  # J√° Float64
    'media_semana_anterior_5': pl.Float64,
    'semana_anterior_4': pl.Float64,  # J√° Float64
    'freq_semana_anterior_4': pl.Float64,  # J√° Float64
    'qtde_semana_anterior_4': pl.Float64,  # J√° Float64
    'media_semana_anterior_4': pl.Float64,
    'semana_anterior_3': pl.Float64,  # J√° Float64
    'freq_semana_anterior_3': pl.Float64,  # J√° Float64
    'qtde_semana_anterior_3': pl.Float64,  # J√° Float64
    'media_semana_anterior_3': pl.Float64,
    'semana_anterior_2': pl.Float64,  # J√° Float64
    'freq_semana_anterior_2': pl.Float64,  # J√° Float64
    'qtde_semana_anterior_2': pl.Float64,  # J√° Float64
    'media_semana_anterior_2': pl.Float64,
    'semana_atual': pl.Float64,  # J√° Float64
    'freq_semana_atual': pl.Float64,  # J√° Float64
    'qtde_semana_atual': pl.Float64,
    'media_semana_atual': pl.Float64,

    # Vendas e m√©dias - Float64
    'venda_30_d': pl.Float64,  # J√° Float64
    'media_considerada_lv': pl.Float64,  # J√° Float64

    # Estoque CD - CR√çTICO para performance
    'estoque_cd': pl.Float64,
    'ultima_entrada_qtde_cd': pl.Float64,
    'ultima_entrada_custo_cd': pl.Float64,

    # Estoque UNE - CR√çTICO para performance e query do usu√°rio
    'estoque_atual': pl.Float64,
    'estoque_lv': pl.Float64,
    'estoque_gondola_lv': pl.Float64,
    'estoque_ilha_lv': pl.Float64,

    # Exposi√ß√£o - Float64
    'exposicao_minima': pl.Float64,
    'exposicao_minima_une': pl.Float64,
    'exposicao_maxima_une': pl.Float64,

    # Lead time e ponto de pedido - Float64
    'leadtime_lv': pl.Float64,  # J√° Float64
    'ponto_pedido_lv': pl.Float64,  # J√° Float64
    'media_travada': pl.Float64,

    # Endere√ßos - String
    'endereco_reserva': pl.String,
    'endereco_linha': pl.String,

    # Solicita√ß√µes - Float64
    'solicitacao_pendente': pl.Float64,  # J√° Float64
    'solicitacao_pendente_qtde': pl.Float64,
    'solicitacao_pendente_situacao': pl.String,

    # Datas - Datetime
    'ultima_entrada_data_cd': pl.Datetime,  # J√° Datetime
    'ultimo_inventario_une': pl.Datetime,  # J√° Datetime
    'ultima_entrada_data_une': pl.Datetime,  # J√° Datetime
    'ultima_venda_data_une': pl.Datetime,  # J√° Datetime
    'solicitacao_pendente_data': pl.Datetime,  # J√° Datetime
    'picklist_conferencia': pl.Datetime,  # J√° Datetime
    'nota_emissao': pl.Datetime,  # J√° Datetime

    # Picklist - Float64
    'picklist': pl.Float64,  # J√° Float64
    'picklist_situacao': pl.String,

    # Volumes - Float64
    'ultimo_volume': pl.Float64,  # J√° Float64
    'volume_qtde': pl.Float64,

    # Romaneios - Float64
    'romaneio_solicitacao': pl.Float64,  # J√° Float64
    'romaneio_envio': pl.Float64,  # J√° Float64

    # Notas - Float64
    'nota': pl.Float64,  # J√° Float64
    'serie': pl.String,

    # Frequ√™ncia - Float64
    'freq_ult_sem': pl.Float64,  # J√° Float64

    # Timestamps - Datetime
    'created_at': pl.Datetime,  # J√° Datetime
    'updated_at': pl.Datetime,  # J√° Datetime
}

def backup_original():
    """Cria backup do arquivo original"""
    try:
        import shutil
        if Path(INPUT_FILE).exists() and not Path(BACKUP_FILE).exists():
            shutil.copy2(INPUT_FILE, BACKUP_FILE)
            logger.info(f"‚úÖ Backup criado: {BACKUP_FILE}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erro ao criar backup: {e}")
        return False

def fix_dtypes():
    """Corrige tipos de dados do admmat.parquet"""

    logger.info("="*80)
    logger.info("CORRE√á√ÉO DE TIPOS DE DADOS - ADMMAT.PARQUET")
    logger.info("="*80)

    # 1. Criar backup
    logger.info("\n[1/5] Criando backup do arquivo original...")
    if not backup_original():
        logger.error("‚ùå Falha ao criar backup. Abortando.")
        return False

    # 2. Ler arquivo com schema atual
    logger.info(f"\n[2/5] Lendo arquivo: {INPUT_FILE}")
    start_time = time.time()

    try:
        df = pl.read_parquet(INPUT_FILE)
        read_time = time.time() - start_time

        logger.info(f"‚úÖ Arquivo lido em {read_time:.2f}s")
        logger.info(f"   Shape: {df.shape}")
        logger.info(f"   Tamanho em mem√≥ria: {df.estimated_size() / 1024**2:.1f} MB")

    except Exception as e:
        logger.error(f"‚ùå Erro ao ler arquivo: {e}")
        return False

    # 3. Aplicar convers√µes de tipo
    logger.info("\n[3/5] Aplicando convers√µes de tipo...")
    convert_start = time.time()

    conversions_applied = 0
    errors = []

    for col, target_dtype in DTYPE_MAPPING.items():
        if col not in df.columns:
            logger.warning(f"‚ö†Ô∏è Coluna '{col}' n√£o encontrada no DataFrame")
            continue

        current_dtype = df[col].dtype

        # Verificar se convers√£o √© necess√°ria
        if str(current_dtype) == str(target_dtype):
            logger.debug(f"   {col}: {current_dtype} (j√° correto)")
            continue

        try:
            # Aplicar convers√£o
            if target_dtype in [pl.Float64, pl.Int64]:
                # Num√©rico: usar cast com strict=False para lidar com valores inv√°lidos
                df = df.with_columns(
                    pl.col(col).cast(target_dtype, strict=False).fill_null(0).alias(col)
                )
            else:
                # Outros tipos
                df = df.with_columns(
                    pl.col(col).cast(target_dtype, strict=False).alias(col)
                )

            conversions_applied += 1
            logger.info(f"   ‚úÖ {col}: {current_dtype} ‚Üí {target_dtype}")

        except Exception as e:
            error_msg = f"   ‚ùå {col}: Erro na convers√£o {current_dtype} ‚Üí {target_dtype}: {e}"
            logger.error(error_msg)
            errors.append(error_msg)

    convert_time = time.time() - convert_start
    logger.info(f"\n‚úÖ Convers√µes aplicadas: {conversions_applied}")
    logger.info(f"   Tempo: {convert_time:.2f}s")

    if errors:
        logger.warning(f"‚ö†Ô∏è {len(errors)} erro(s) durante convers√£o:")
        for err in errors:
            logger.warning(f"   {err}")

    # 4. Salvar arquivo corrigido
    logger.info(f"\n[4/5] Salvando arquivo corrigido: {OUTPUT_FILE}")
    save_start = time.time()

    try:
        df.write_parquet(
            OUTPUT_FILE,
            compression="snappy",
            use_pyarrow=True
        )
        save_time = time.time() - save_start
        logger.info(f"‚úÖ Arquivo salvo em {save_time:.2f}s")

    except Exception as e:
        logger.error(f"‚ùå Erro ao salvar arquivo: {e}")
        return False

    # 5. Validar arquivo corrigido
    logger.info("\n[5/5] Validando arquivo corrigido...")
    validate_start = time.time()

    try:
        df_fixed = pl.read_parquet(OUTPUT_FILE)
        validate_time = time.time() - validate_start

        logger.info(f"‚úÖ Valida√ß√£o bem-sucedida ({validate_time:.2f}s)")
        logger.info(f"   Shape: {df_fixed.shape}")
        logger.info(f"   Tamanho: {Path(OUTPUT_FILE).stat().st_size / 1024**2:.1f} MB")

        # Comparar tamanhos
        original_size = Path(INPUT_FILE).stat().st_size / 1024**2
        fixed_size = Path(OUTPUT_FILE).stat().st_size / 1024**2
        reduction = ((original_size - fixed_size) / original_size) * 100

        logger.info(f"\nüìä RESULTADOS:")
        logger.info(f"   Tamanho original: {original_size:.1f} MB")
        logger.info(f"   Tamanho corrigido: {fixed_size:.1f} MB")
        logger.info(f"   Redu√ß√£o: {reduction:.1f}%")

        # Verificar tipos
        logger.info(f"\nüîç AMOSTRA DE TIPOS CORRIGIDOS:")
        critical_cols = ['codigo', 'estoque_atual', 'preco_38_percent', 'mes_01', 'qtde_emb_master']
        for col in critical_cols:
            if col in df_fixed.columns:
                logger.info(f"   {col}: {df_fixed[col].dtype}")

        return True

    except Exception as e:
        logger.error(f"‚ùå Erro na valida√ß√£o: {e}")
        return False

def replace_original():
    """Substitui arquivo original pelo corrigido"""
    try:
        import shutil

        logger.info(f"\nüîÑ Substituindo arquivo original...")

        # Remover original
        Path(INPUT_FILE).unlink()
        logger.info(f"   ‚úÖ Arquivo original removido")

        # Renomear corrigido para original
        shutil.move(OUTPUT_FILE, INPUT_FILE)
        logger.info(f"   ‚úÖ Arquivo corrigido renomeado para {INPUT_FILE}")

        logger.info(f"\n‚úÖ SUBSTITUI√á√ÉO CONCLU√çDA!")
        logger.info(f"   Backup dispon√≠vel em: {BACKUP_FILE}")

        return True

    except Exception as e:
        logger.error(f"‚ùå Erro ao substituir arquivo: {e}")
        return False

if __name__ == "__main__":
    logger.info("\n" + "="*80)
    logger.info("SCRIPT DE CORRE√á√ÉO DE TIPOS DE DADOS - ADMMAT.PARQUET")
    logger.info("="*80 + "\n")

    total_start = time.time()

    # Executar corre√ß√£o
    success = fix_dtypes()

    if success:
        total_time = time.time() - total_start
        logger.info(f"\n‚úÖ CORRE√á√ÉO CONCLU√çDA COM SUCESSO!")
        logger.info(f"   Tempo total: {total_time:.2f}s")

        # Perguntar se deve substituir original
        logger.info(f"\nüìù PR√ìXIMO PASSO:")
        logger.info(f"   Para usar o arquivo corrigido, execute:")
        logger.info(f"   python scripts/fix_admmat_dtypes.py --replace")
        logger.info(f"\n   OU execute replace_original() manualmente")

    else:
        logger.error(f"\n‚ùå CORRE√á√ÉO FALHOU!")
        logger.error(f"   Verifique os logs acima para detalhes")
