"""
Cache inteligente para agent_graph - Reduz lat√™ncia e custos
Armazena resultados de queries similares para evitar chamadas LLM repetidas.
"""

import hashlib
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional
import pickle

logger = logging.getLogger(__name__)

class AgentGraphCache:
    """Cache inteligente para o agent_graph com expira√ß√£o e similaridade"""

    def __init__(self, cache_dir: str = "data/cache_agent_graph", ttl_hours: int = 24):
        """
        Inicializa o cache.

        Args:
            cache_dir: Diret√≥rio para armazenar cache
            ttl_hours: Tempo de vida do cache em horas (padr√£o: 24h)
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.ttl = timedelta(hours=ttl_hours)

        # Cache em mem√≥ria para acesso ultra-r√°pido
        self._memory_cache: Dict[str, Dict[str, Any]] = {}

        # Verificar vers√£o do c√≥digo e invalidar cache se mudou
        self._check_code_version()

        logger.info(f"‚úÖ AgentGraphCache inicializado - TTL: {ttl_hours}h")

    def _check_code_version(self):
        """
        Verifica se a vers√£o do c√≥digo mudou e invalida cache se necess√°rio.

        Isso resolve o problema de cache desatualizado ap√≥s mudan√ßas no c√≥digo.
        """
        version_file = Path("data/cache/.code_version")
        version_cache_file = self.cache_dir / ".code_version"

        try:
            # Ler vers√£o atual do c√≥digo
            if version_file.exists():
                with open(version_file, 'r') as f:
                    current_version = f.read().strip()
            else:
                # Criar vers√£o inicial
                current_version = datetime.now().strftime("%Y%m%d_%H%M%S")
                version_file.parent.mkdir(parents=True, exist_ok=True)
                with open(version_file, 'w') as f:
                    f.write(current_version)

            # Ler vers√£o do cache
            if version_cache_file.exists():
                with open(version_cache_file, 'r') as f:
                    cached_version = f.read().strip()
            else:
                cached_version = None

            # Se vers√µes diferentes, limpar cache
            if cached_version != current_version:
                logger.warning(f"üîÑ Vers√£o do c√≥digo mudou ({cached_version} ‚Üí {current_version})")
                logger.warning(f"üßπ Invalidando cache antigo...")

                # Limpar cache em mem√≥ria
                self._memory_cache.clear()

                # Limpar cache em disco
                import shutil
                if self.cache_dir.exists():
                    for cache_file in self.cache_dir.glob("*.pkl"):
                        try:
                            cache_file.unlink()
                        except Exception as e:
                            logger.error(f"Erro ao remover {cache_file}: {e}")

                # Salvar nova vers√£o
                with open(version_cache_file, 'w') as f:
                    f.write(current_version)

                logger.info(f"‚úÖ Cache invalidado - Nova vers√£o: {current_version}")
            else:
                logger.debug(f"‚úÖ Vers√£o do c√≥digo inalterada: {current_version}")

        except Exception as e:
            logger.error(f"‚ùå Erro ao verificar vers√£o do c√≥digo: {e}")
            # Continuar mesmo com erro (n√£o quebrar inicializa√ß√£o)

    def _normalize_query(self, query: str) -> str:
        """Normaliza query para melhor matching"""
        # Remove espa√ßos extras, pontua√ß√£o, e converte para min√∫sculas
        normalized = query.lower().strip()
        normalized = ' '.join(normalized.split())  # Remove m√∫ltiplos espa√ßos
        return normalized

    def _generate_cache_key(self, query: str) -> str:
        """Gera chave √∫nica para a query"""
        normalized = self._normalize_query(query)
        return hashlib.md5(normalized.encode()).hexdigest()

    def get(self, query: str) -> Optional[Dict[str, Any]]:
        """
        Obt√©m resultado do cache se dispon√≠vel e v√°lido.

        Args:
            query: Query do usu√°rio

        Returns:
            Resultado cacheado ou None
        """
        cache_key = self._generate_cache_key(query)

        # 1. Verificar cache em mem√≥ria (mais r√°pido)
        if cache_key in self._memory_cache:
            cached = self._memory_cache[cache_key]

            # Verificar expira√ß√£o
            cached_time = datetime.fromisoformat(cached['timestamp'])
            if datetime.now() - cached_time < self.ttl:
                logger.info(f"‚úÖ CACHE HIT (memory) - Query: '{query[:50]}...'")
                cached['cache_hit'] = True
                cached['cache_source'] = 'memory'
                return cached['result']
            else:
                # Remover cache expirado
                del self._memory_cache[cache_key]
                logger.info(f"‚è∞ Cache expirado (memory) - Query: '{query[:50]}...'")

        # 2. Verificar cache em disco
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    cached = pickle.load(f)

                # Verificar expira√ß√£o
                cached_time = datetime.fromisoformat(cached['timestamp'])
                if datetime.now() - cached_time < self.ttl:
                    # Recarregar para mem√≥ria
                    self._memory_cache[cache_key] = cached

                    logger.info(f"‚úÖ CACHE HIT (disk) - Query: '{query[:50]}...'")
                    cached['cache_hit'] = True
                    cached['cache_source'] = 'disk'
                    return cached['result']
                else:
                    # Remover cache expirado
                    cache_file.unlink()
                    logger.info(f"‚è∞ Cache expirado (disk) - Query: '{query[:50]}...'")
            except Exception as e:
                logger.error(f"‚ùå Erro ao ler cache: {e}")
                cache_file.unlink(missing_ok=True)

        logger.info(f"‚ùå CACHE MISS - Query: '{query[:50]}...'")
        return None

    def set(self, query: str, result: Dict[str, Any], metadata: Optional[Dict] = None):
        """
        Armazena resultado no cache.

        Args:
            query: Query do usu√°rio
            result: Resultado do agent_graph
            metadata: Metadados adicionais (opcional)
        """
        cache_key = self._generate_cache_key(query)

        cached_data = {
            'query': query,
            'result': result,
            'timestamp': datetime.now().isoformat(),
            'metadata': metadata or {}
        }

        # 1. Armazenar em mem√≥ria
        self._memory_cache[cache_key] = cached_data

        # 2. Armazenar em disco (persist√™ncia)
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(cached_data, f)
            logger.info(f"üíæ Cache salvo - Query: '{query[:50]}...'")
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar cache: {e}")

    def clear_expired(self):
        """Remove entradas expiradas do cache"""
        now = datetime.now()
        expired_count = 0

        # Limpar mem√≥ria
        expired_keys = [
            key for key, data in self._memory_cache.items()
            if now - datetime.fromisoformat(data['timestamp']) >= self.ttl
        ]
        for key in expired_keys:
            del self._memory_cache[key]
            expired_count += 1

        # Limpar disco
        for cache_file in self.cache_dir.glob("*.pkl"):
            try:
                with open(cache_file, 'rb') as f:
                    cached = pickle.load(f)

                cached_time = datetime.fromisoformat(cached['timestamp'])
                if now - cached_time >= self.ttl:
                    cache_file.unlink()
                    expired_count += 1
            except Exception as e:
                logger.warning(f"Erro ao verificar {cache_file.name}: {e}")
                cache_file.unlink(missing_ok=True)

        if expired_count > 0:
            logger.info(f"üßπ Cache limpo - {expired_count} entradas expiradas removidas")

    def clear_all(self):
        """Remove todo o cache"""
        self._memory_cache.clear()

        for cache_file in self.cache_dir.glob("*.pkl"):
            cache_file.unlink()

        logger.info("üßπ Todo o cache foi limpo")

    def get_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas do cache"""
        return {
            'memory_entries': len(self._memory_cache),
            'disk_entries': len(list(self.cache_dir.glob("*.pkl"))),
            'cache_dir': str(self.cache_dir),
            'ttl_hours': self.ttl.total_seconds() / 3600
        }


# Inst√¢ncia global (singleton)
_cache_instance = None

def get_agent_graph_cache() -> AgentGraphCache:
    """Obt√©m inst√¢ncia singleton do cache"""
    global _cache_instance
    if _cache_instance is None:
        _cache_instance = AgentGraphCache()
    return _cache_instance
