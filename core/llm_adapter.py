import logging
from openai import OpenAI, RateLimitError
from core.utils.response_cache import ResponseCache

logger = logging.getLogger(__name__)

class GeminiLLMAdapter:
    def __init__(self, api_key: str, model_name: str, enable_cache: bool = True):
        """
        Inicializa o cliente Gemini usando OpenAI SDK com base_url customizada.
        Gemini 2.5 Flash suporta interface compat√≠vel com OpenAI.
        """
        if not api_key:
            raise ValueError("A chave da API do Gemini n√£o foi fornecida.")

        # ‚úÖ FIX CR√çTICO: Usar base_url do Gemini, n√£o da OpenAI
        # Gemini API endpoint compat√≠vel com OpenAI
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
        )
        self.model_name = model_name

        self.cache_enabled = enable_cache
        if enable_cache:
            self.cache = ResponseCache(ttl_hours=48)
            self.cache.clear_expired()
            logger.info("‚úÖ Cache de respostas ativado para Gemini - ECONOMIA DE CR√âDITOS")
        else:
            self.cache = None

        logger.info("Adaptador do GeminiLLMAdapter inicializado com sucesso.")

    def _stream_completion_generator(self, response_stream):
        for chunk in response_stream:
            content = chunk.choices[0].delta.content or ""
            yield content

    def get_completion(self, messages, model=None, temperature=0, max_tokens=1024, json_mode=False, stream=False):
        try:
            if not stream and self.cache_enabled and self.cache:
                cached_response = self.cache.get(messages, model, temperature)
                if cached_response:
                    return cached_response

            model_to_use = model or self.model_name

            params = {
                "model": model_to_use,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": stream,
            }
            if json_mode:
                params["response_format"] = {"type": "json_object"}

            logger.info(f"üí∞ Chamada API Gemini: {model_to_use} - tokens: {max_tokens}")
            response = self.client.chat.completions.create(**params)

            if stream:
                return self._stream_completion_generator(response)

            # Extrair conte√∫do com valida√ß√£o
            try:
                content = response.choices[0].message.content
                finish_reason = response.choices[0].finish_reason

                # Verificar se parou por limite de tokens sem gerar nada
                if finish_reason == 'length' and (content is None or not content):
                    completion_tokens = response.usage.completion_tokens if hasattr(response, 'usage') else 0
                    if completion_tokens == 0:
                        logger.error(f"‚ùå max_tokens muito baixo! O modelo parou antes de gerar qualquer resposta. Tokens usados: {response.usage}")
                        content = "‚ö†Ô∏è ERRO: max_tokens muito baixo. Aumente o valor de max_tokens para permitir que o modelo gere uma resposta."
                    else:
                        logger.warning(f"‚ö†Ô∏è Resposta cortada por limite de tokens. Aumente max_tokens se necess√°rio.")
                elif content is None:
                    logger.warning(f"‚ö†Ô∏è API retornou content=None. Response: {response}")
                    # Tentar pegar de outro lugar se dispon√≠vel
                    if hasattr(response.choices[0].message, 'text'):
                        content = response.choices[0].message.text
                    elif hasattr(response.choices[0], 'text'):
                        content = response.choices[0].text
                    else:
                        content = ""
                        logger.error(f"‚ùå N√£o foi poss√≠vel extrair conte√∫do. Response completo: {response.model_dump() if hasattr(response, 'model_dump') else response}")
            except (IndexError, AttributeError) as e:
                logger.error(f"‚ùå Erro ao extrair conte√∫do da resposta: {e}")
                content = ""

            result = {"content": content}

            if self.cache_enabled and self.cache:
                self.cache.set(messages, model, temperature, result)

            return result

        except RateLimitError as e:
            logger.error(f"üö® Rate limit Gemini 2.5 Flash-Lite atingido: {e}", exc_info=True)
            # ATIVA O FALLBACK AUTOM√ÅTICO PARA DEEPSEEK!
            try:
                from core.factory.component_factory import ComponentFactory
                ComponentFactory.set_gemini_unavailable(True)
                logger.warning("üîÑ Fallback ativado: Gemini ‚Üí DeepSeek")
            except ImportError:
                pass
            return {"error": "Rate limit exceeded", "fallback_activated": True, "retry_with": "deepseek"}
        except Exception as e:
            error_msg = str(e).lower()
            # Detectar outros tipos de rate limit/quota exceeded
            if any(term in error_msg for term in ["quota", "limit", "429", "rate", "exceeded"]):
                logger.error(f"üö® Quota/Rate limit detectado no Gemini: {e}")
                try:
                    from core.factory.component_factory import ComponentFactory
                    ComponentFactory.set_gemini_unavailable(True)
                    logger.warning("üîÑ Fallback ativado por quota: Gemini ‚Üí DeepSeek")
                except ImportError:
                    pass
                return {"error": "Quota exceeded", "fallback_activated": True, "retry_with": "deepseek"}

            logger.error(f"Erro ao chamar a API do Gemini: {e}", exc_info=True)
            return {"error": str(e)}

    def get_cache_stats(self):
        if not self.cache_enabled or not self.cache:
            return {"cache_enabled": False}
        stats = self.cache.get_stats()
        stats["cache_enabled"] = True
        return stats

class DeepSeekLLMAdapter:
    def __init__(self, api_key: str, model_name: str, enable_cache: bool = True):
        """
        Inicializa o cliente para a API DeepSeek.
        """
        if not api_key:
            raise ValueError("A chave da API da DeepSeek n√£o foi fornecida.")
        
        self.client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com/v1"
        )
        self.model_name = model_name

        self.cache_enabled = enable_cache
        if enable_cache:
            self.cache = ResponseCache(ttl_hours=48)
            self.cache.clear_expired()
            logger.info("‚úÖ Cache de respostas ativado para DeepSeek.")
        else:
            self.cache = None

        logger.info("Adaptador do DeepSeekLLMAdapter inicializado com sucesso.")

    def get_completion(self, messages, model=None, temperature=0, max_tokens=1024, json_mode=False):
        """
        Obt√©m uma conclus√£o do modelo DeepSeek.
        """
        try:
            if self.cache_enabled and self.cache:
                cached_response = self.cache.get(messages, model, temperature)
                if cached_response:
                    return cached_response

            # Usa o modelo da chamada ou o padr√£o da inst√¢ncia
            model_to_use = model or self.model_name

            params = {
                "model": model_to_use,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
            }
            if json_mode:
                params["response_format"] = {"type": "json_object"}

            logger.info(f"üí∞ Chamada API DeepSeek: {model_to_use} - tokens: {max_tokens}")
            response = self.client.chat.completions.create(**params)

            content = response.choices[0].message.content
            result = {"content": content}

            if self.cache_enabled and self.cache:
                self.cache.set(messages, model, temperature, result)

            return result

        except Exception as e:
            logger.error(f"Erro ao chamar a API da DeepSeek: {e}", exc_info=True)
            return {"error": str(e)}

    def get_cache_stats(self):
        if not self.cache_enabled or not self.cache:
            return {"cache_enabled": False}
        stats = self.cache.get_stats()
        stats["cache_enabled"] = True
        return stats


