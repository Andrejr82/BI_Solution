"""
MÃ³dulo de RaciocÃ­nio Conversacional - Conversational Reasoning Engine

Este mÃ³dulo implementa uma camada de raciocÃ­nio explÃ­cito que torna o agente
verdadeiramente conversacional, nÃ£o apenas um executor de queries.

Baseado em Extended Thinking patterns do Context7 Anthropic Cookbook.

Author: devAndreJr
Version: 3.0.0 - Conversational AI
"""

import logging
import json
import re
from typing import Dict, Any, Tuple, List, Optional
from core.agent_state import AgentState
from core.llm_base import BaseLLMAdapter

logger = logging.getLogger(__name__)


class ConversationalReasoningEngine:
    """
    ğŸ§  Motor de RaciocÃ­nio Conversacional

    Implementa Extended Thinking para anÃ¡lise profunda da intenÃ§Ã£o do usuÃ¡rio,
detectando contexto emocional, necessidades implÃ­citas e escolhendo o modo
    de resposta adequado (conversacional vs analÃ­tico).
    """

    def __init__(self, llm_adapter: BaseLLMAdapter):
        """
        Inicializa o motor de raciocÃ­nio.

        Args:
            llm_adapter: Adaptador LLM (Gemini ou DeepSeek)
        """
        self.llm_adapter = llm_adapter
        self.conversation_memory: List[Dict[str, str]] = []
        logger.info("ğŸ§  ConversationalReasoningEngine inicializado")

    def _get_full_conversation_context(self, messages: List, max_messages: int = 8) -> str:
        """
        Formata o histÃ³rico completo da conversa em uma Ãºnica string legÃ­vel.

        Args:
            messages: Lista de mensagens
            max_messages: NÃºmero mÃ¡ximo de mensagens recentes a incluir (padrÃ£o: 8)
        """
        if not messages:
            return "(Nenhuma mensagem na conversa)"

        # ğŸ”§ OTIMIZAÃ‡ÃƒO: Usar apenas as Ãºltimas N mensagens para evitar prompts gigantes
        recent_messages = messages[-max_messages:] if len(messages) > max_messages else messages

        history = []
        for msg in recent_messages:
            try:
                role = "UsuÃ¡rio"
                content = ""
                # Lidar com objetos de mensagem Langchain e dicionÃ¡rios do Streamlit
                if hasattr(msg, 'type'): # Langchain BaseMessage
                    if msg.type == 'ai':
                        role = "Caculinha"
                    content = str(getattr(msg, 'content', ''))
                elif isinstance(msg, dict): # Streamlit session_state message
                    if msg.get('role') == 'assistant':
                        role = "Caculinha"

                    msg_content = msg.get('content', '')
                    if isinstance(msg_content, dict):
                        # Tentar extrair conteÃºdo de um dicionÃ¡rio aninhado
                        content = msg_content.get('content', str(msg_content))
                    else:
                        content = str(msg_content)
                else: # Fallback
                    content = str(msg)

                # Truncar mensagens muito longas para manter o prompt focado
                content_preview = content.replace('\n', ' ')
                content_preview = content_preview[:200] + "..." if len(content_preview) > 200 else content_preview
                history.append(f"{role}: {content_preview}")
            except Exception as e:
                logger.warning(f"Erro ao formatar mensagem do histÃ³rico: {e}")
                continue

        # ğŸ“Š LOG: Indicar se histÃ³rico foi truncado
        if len(messages) > max_messages:
            logger.info(f"ğŸ“ HistÃ³rico truncado: {len(messages)} msgs â†’ {len(recent_messages)} msgs recentes")
            history.insert(0, f"(... {len(messages) - max_messages} mensagens anteriores omitidas ...)")

        return "\n".join(history)

    def reason_about_user_intent(self, state: AgentState) -> Tuple[str, Dict[str, Any]]:
        """
        ğŸ¯ RACIOCÃNIO PROFUNDO: Analisa a intenÃ§Ã£o do usuÃ¡rio com Extended Thinking

        Este mÃ©todo implementa a primeira camada de processamento, onde o agente:
        1. Analisa o contexto completo da conversa
        2. Detecta o tom emocional do usuÃ¡rio
        3. Identifica necessidades implÃ­citas
        4. Decide o modo de resposta apropriado

        Args:
            state: Estado atual do agente com histÃ³rico de mensagens

        Returns:
            Tupla (mode, reasoning_result):
            - mode: "conversational" ou "analytical"
            - reasoning_result: DicionÃ¡rio com anÃ¡lise detalhada
        """
        messages = state.get("messages", [])
        if not messages:
            logger.warning("âš ï¸ Nenhuma mensagem no estado")
            return "conversational", self._create_fallback_reasoning()

        # ğŸ”§ PROTEÃ‡ÃƒO ANTI-LOOP: Se conversa tem > 4 mensagens e ainda nÃ£o executou, forÃ§ar analÃ­tico
        if len(messages) > 4:
            # Contar quantas mensagens do assistente sÃ£o perguntas
            assistant_questions = 0
            for msg in messages:
                if hasattr(msg, 'type') and msg.type == 'ai':
                    content = str(getattr(msg, 'content', ''))
                    if '?' in content:
                        assistant_questions += 1
                elif isinstance(msg, dict) and msg.get('role') == 'assistant':
                    content = str(msg.get('content', ''))
                    if '?' in content:
                        assistant_questions += 1

            # Se jÃ¡ fez 2+ perguntas, FORÃ‡AR modo analÃ­tico
            if assistant_questions >= 2:
                logger.warning(f"ğŸš¨ ANTI-LOOP ATIVADO: {assistant_questions} perguntas feitas, forÃ§ando modo analÃ­tico")
                return "analytical", {
                    "mode": "analytical",
                    "reasoning": "Conversa muito longa sem execuÃ§Ã£o. ForÃ§ando anÃ¡lise com defaults.",
                    "emotional_tone": "neutro",
                    "confidence": 0.8,
                    "needs_clarification": False,
                    "next_action": {
                        "type": "use_tool",
                        "response_style": "technical"
                    }
                }

        # âœ… CORREÃ‡ÃƒO: Usar o contexto completo da conversa
        full_conversation_context = self._get_full_conversation_context(messages)
        log_context = full_conversation_context.replace('\n', ' ')
        logger.info(f"ğŸ§  Analisando intent (contexto completo): '{log_context[-150:]}...'")

        # ğŸ§  PROMPT DE RACIOCÃNIO PROFUNDO
        reasoning_prompt = self._build_reasoning_prompt(full_conversation_context)

        # ğŸ“Š LOG: Tamanho do prompt de raciocÃ­nio
        prompt_size = len(reasoning_prompt)
        logger.info(f"ğŸ“ Tamanho do prompt de reasoning: {prompt_size} chars (~{prompt_size//4} tokens estimados)")

        try:
            # ğŸ”¥ CORREÃ‡ÃƒO: Temperatura BAIXA para decisÃµes consistentes
            response = self.llm_adapter.get_completion(
                messages=[{"role": "user", "content": reasoning_prompt}],
                json_mode=True,
                temperature=0.3,  # ğŸ”§ FIX: Baixa temp para decisÃµes consistentes (era 0.8)
                max_tokens=1500,  # ğŸ”§ OTIMIZADO: de 2000 para 1500 (suficiente para JSON de raciocÃ­nio)
                cache_context={"operation": "conversational_reasoning", "stage": "intent_analysis"}
            )

            # âœ… VERIFICAR ERRO NA RESPOSTA
            if response.get("error"):
                error_msg = response.get('error', 'Erro desconhecido')
                logger.error(f"âŒ Erro na API ao gerar raciocÃ­nio: {error_msg}")
                return "conversational", self._create_fallback_reasoning()

            reasoning_result = self._parse_reasoning(response.get("content", "{}"))

            # ğŸ“Š LOGGING DETALHADO
            mode = reasoning_result.get("mode", "conversational")
            emotional_tone = reasoning_result.get("emotional_tone", "neutro")
            confidence = reasoning_result.get("confidence", 0.5)

            logger.info(f"ğŸ¯ Mode: {mode} | Emotion: {emotional_tone} | Confidence: {confidence:.2f}")
            logger.info(f"ğŸ’­ Reasoning: {reasoning_result.get('reasoning', 'N/A')[:100]}...")

            return mode, reasoning_result

        except Exception as e:
            logger.error(f"âŒ Erro no raciocÃ­nio: {e}", exc_info=True)
            return "conversational", self._create_fallback_reasoning()

    def generate_conversational_response(
        self,
        reasoning: Dict[str, Any],
        state: AgentState
    ) -> str:
        """
        ğŸ’¬ MODO CONVERSACIONAL: Gera resposta natural e humana

        Este mÃ©todo Ã© acionado quando o usuÃ¡rio:
        - EstÃ¡ conversando casualmente
        - Precisa de clarificaÃ§Ã£o
        - Fez uma saudaÃ§Ã£o/agradecimento
        - EstÃ¡ frustrado e precisa de empatia

        Args:
            reasoning: Resultado da anÃ¡lise de raciocÃ­nio
            state: Estado atual do agente

        Returns:
            Resposta conversacional natural em portuguÃªs
        """
        messages = state.get("messages", [])
        full_conversation_context = self._get_full_conversation_context(messages)
        emotional_tone = reasoning.get("emotional_tone", "neutro")
        
        # ğŸ¨ PROMPT CONVERSACIONAL com temperatura mÃ¡xima
        conversational_prompt = self._build_conversational_prompt(
            full_conversation_context,
            emotional_tone,
            reasoning
        )

        logger.info(f"ğŸ’¬ Gerando resposta conversacional (tom: {emotional_tone})")

        # ğŸ“Š LOG: Tamanho do prompt
        prompt_size = len(conversational_prompt)
        logger.info(f"ğŸ“ Tamanho do prompt conversacional: {prompt_size} chars (~{prompt_size//4} tokens estimados)")

        try:
            response = self.llm_adapter.get_completion(
                messages=[{"role": "user", "content": conversational_prompt}],
                temperature=1.0,  # ğŸ”¥ TEMPERATURA MÃXIMA = respostas mais humanas
                max_tokens=1500,  # ğŸ”§ AUMENTADO: de 800 para 1500 (mais espaÃ§o para resposta)
                cache_context={"operation": "conversational_response", "tone": emotional_tone}
            )

            # âœ… TRATAMENTO: Verificar se hÃ¡ mensagem de erro do LLM
            if response.get("error"):
                error_msg = response.get('error', 'Erro desconhecido')
                logger.error(f"âŒ Erro na API ao gerar resposta conversacional: {error_msg}")

                # Se houver user_message, usar ela
                if response.get("user_message"):
                    logger.warning(f"âš ï¸ Usando mensagem de fallback da API")
                    return response.get("user_message")

                # SenÃ£o, usar fallback baseado no tom
                return self._get_fallback_response(emotional_tone)

            response_text = response.get("content", "")

            # Verificar se resposta estÃ¡ vazia
            if not response_text or len(response_text.strip()) == 0:
                logger.warning(f"âš ï¸ Resposta vazia recebida do LLM. Usando fallback.")
                return self._get_fallback_response(emotional_tone)

            # Remover possÃ­veis tags JSON se houver
            response_text = self._clean_response(response_text)

            logger.info(f"âœ… Resposta conversacional gerada: {len(response_text)} chars")
            return response_text

        except Exception as e:
            logger.error(f"âŒ Erro ao gerar resposta conversacional: {e}", exc_info=True)
            return self._get_fallback_response(emotional_tone)

    def _build_reasoning_prompt(self, full_conversation: str) -> str:
        """ConstrÃ³i o prompt de raciocÃ­nio profundo"""

        return f"""# ğŸ§  ANÃLISE DE INTENÃ‡ÃƒO CONVERSACIONAL

VocÃª Ã© a Caculinha, uma assistente de BI conversacional. VocÃª NÃƒO Ã© um robÃ´ executor de queries.

## ğŸ“š CONTEXTO DA CONVERSA COMPLETA
{full_conversation}

## ğŸ¤” TAREFA: PENSAR PROFUNDAMENTE

Analise a **conversa completa** e responda estas perguntas em seu raciocÃ­nio:

1. **IntenÃ§Ã£o Real**: O que o usuÃ¡rio REALMENTE quer? Considere o histÃ³rico para resolver ambiguidades na Ãºltima mensagem.
2. **Tom Emocional**: Como ele estÃ¡ se sentindo? (frustrado/curioso/casual/urgente/neutro/confuso)
3. **Contexto**: A Ãºltima mensagem Ã© uma continuaÃ§Ã£o da conversa anterior ou um novo tÃ³pico?
4. **Clareza**: O usuÃ¡rio jÃ¡ forneceu todas as informaÃ§Ãµes necessÃ¡rias ou ainda faltam detalhes?
5. **Tipo de Resposta**: Com base em tudo, devo conversar ou executar uma anÃ¡lise tÃ©cnica?

## ğŸ¯ CATEGORIZAÃ‡ÃƒO

**MODO CONVERSACIONAL** - Use quando:
- SaudaÃ§Ãµes/agradecimentos/despedidas/conversa casual.
- Perguntas sobre suas capacidades ("o que vocÃª faz?", "pode me ajudar com...").
- Feedback emocional ("nÃ£o entendi", "estÃ¡ confuso", "muito obrigado").
- InformaÃ§Ã£o estÃ¡ GENUINAMENTE INSUFICIENTE (falta UNE, perÃ­odo, produto especÃ­fico, etc).

**MODO ANALÃTICO** - Use quando:
- O pedido para dados/anÃ¡lise estÃ¡ CLARO e COMPLETO, considerando todo o histÃ³rico.
- O usuÃ¡rio forneceu a Ãºltima informaÃ§Ã£o que faltava para uma anÃ¡lise.
- A query Ã© tÃ©cnica e bem definida (ex: "MC do produto 123 na UNE SCR").
- O usuÃ¡rio jÃ¡ respondeu a uma pergunta de clarificaÃ§Ã£o sua.
- Pedidos como "grÃ¡fico de TODOS os segmentos" sÃ£o CLAROS (use mÃ©trica padrÃ£o: vendas).

## âš ï¸ REGRAS ANTI-LOOP (CRÃTICO)

ğŸ”´ **NUNCA faÃ§a perguntas repetidas!**
- Se vocÃª JÃ PERGUNTOU algo no histÃ³rico e o usuÃ¡rio respondeu â†’ vÃ¡ para MODO ANALÃTICO
- Se o usuÃ¡rio disse "todos" (ex: "todos os segmentos") â†’ NÃƒO pergunte qual! Use TODOS mesmo.
- Se a conversa tem > 3 mensagens e ainda nÃ£o executou â†’ FORCE modo analÃ­tico com defaults razoÃ¡veis

ğŸ”´ **Defaults Inteligentes:**
- "grÃ¡fico de segmentos" â†’ assumir mÃ©trica de vendas (padrÃ£o)
- "todos os produtos" â†’ assumir top 10 ou 20
- Falta detalhes menores â†’ assumir defaults e EXECUTAR

## ğŸ“¤ RESPOSTA (JSON)

```json
{{
  "mode": "conversational" ou "analytical",
  "reasoning": "Seu raciocÃ­nio em 2-3 frases explicando POR QUE escolheu este modo, com base no histÃ³rico.",
  "emotional_tone": "frustrado/curioso/casual/urgente/neutro/confuso",
  "confidence": 0.0-1.0,
  "needs_clarification": true/false,
  "clarification_question": "pergunta natural se needs_clarification=true, senÃ£o null",
  "missing_info": ["lista", "de", "informaÃ§Ãµes", "faltando"] ou null,
  "next_action": {{
    "type": "respond_directly" ou "use_tool" ou "ask_clarification",
    "response_style": "friendly/empathetic/technical/excited/patient"
  }}
}}
```

**IMPORTANTE**:
- Se a Ãºltima mensagem do usuÃ¡rio for uma resposta a uma pergunta sua, use o histÃ³rico para decidir se agora vocÃª tem informaÃ§Ã£o suficiente para o **MODO ANALÃTICO**.
- ğŸ”§ **NOVO:** Prefira "analytical" quando houver informaÃ§Ã£o SUFICIENTE, mesmo que nÃ£o seja PERFEITA. Use defaults inteligentes em vez de perguntar tudo.
"""

    def _build_conversational_prompt(
        self,
        full_conversation: str,
        emotional_tone: str,
        reasoning: Dict[str, Any]
    ) -> str:
        """ConstrÃ³i o prompt para resposta conversacional"""

        clarification_question = reasoning.get("clarification_question", "")
        missing_info = reasoning.get("missing_info", [])
        response_style = reasoning.get("next_action", {}).get("response_style", "friendly")

        # ğŸ¨ Exemplos de tom baseados na emoÃ§Ã£o detectada
        tone_examples = {
            "frustrado": '''
**Tom EmpÃ¡tico:**
"Opa, vi que vocÃª tÃ¡ tentando hÃ¡ um tempo e nÃ£o deu certo. ğŸ˜• Deixa eu te ajudar de outro jeito..."
"Poxa, desculpa pela confusÃ£o! Vou te explicar melhor..."
            ''',
            "curioso": '''
**Tom Entusiasmado:**
"Boa pergunta! ğŸ˜Š Vou te mostrar algo interessante sobre isso..."
"Olha sÃ³ que legal! Isso que vocÃª perguntou Ã© super importante porque..."
            ''',
            "casual": '''
**Tom Leve:**
"Claro! Vou dar uma olhada nisso pra vocÃª ğŸ‘€"
"Tranquilo! Deixa comigo..."
            ''',
            "urgente": '''
**Tom Ãgil:**
"Entendi, vou resolver isso rapidinho pra vocÃª! âš¡"
"Pode deixar, jÃ¡ vou te trazer essa informaÃ§Ã£o!"
            ''',
            "confuso": '''
**Tom Paciente:**
"Sem problemas! Deixa eu te explicar melhor... ğŸ˜Š"
"Vou te guiar passo a passo, fica tranquilo!"
            ''',
            "neutro": '''
**Tom Profissional AmigÃ¡vel:**
"Claro! Vou te ajudar com isso."
"Entendi. Deixa eu te mostrar..."
            '''
        }

        tone_example = tone_examples.get(emotional_tone, tone_examples["neutro"])

        # Construir seÃ§Ã£o de informaÃ§Ãµes faltando
        missing_info_section = ""
        if missing_info:
            missing_list = ', '.join(missing_info)
            missing_info_section = f"## â“ INFORMAÃ‡Ã•ES FALTANDO\n{missing_list}\n\n"

        # Construir seÃ§Ã£o de clarificaÃ§Ã£o ou resposta
        if clarification_question:
            task_section = f"## ğŸ¯ CLARIFICAÃ‡ÃƒO NECESSÃRIA\n\n{clarification_question}\n\nResponda de forma natural fazendo esta pergunta, mas reformule com suas palavras!"
        else:
            task_section = "## ğŸ¯ RESPONDA Ã€ MENSAGEM\n\nResponda naturalmente Ã  Ãºltima mensagem do usuÃ¡rio no tom apropriado, considerando todo o contexto."

        return f'''# ğŸ’¬ RESPOSTA CONVERSACIONAL DA CACULINHA

VocÃª Ã© a Caculinha. Responda de forma COMPLETAMENTE NATURAL e HUMANA.

## ğŸ­ CONTEXTO EMOCIONAL
UsuÃ¡rio estÃ¡: **{emotional_tone}**
Estilo de resposta: **{response_style}**

## ğŸ“œ CONVERSA COMPLETA
{full_conversation}

## ğŸ§  SEU RACIOCÃNIO
{reasoning.get('reasoning', '')}

{missing_info_section}## ğŸ¨ EXEMPLOS DE TOM APROPRIADO
{tone_example}

## âœï¸ INSTRUÃ‡Ã•ES PARA SUA RESPOSTA

1. **Seja vocÃª mesma**: Fale como uma PESSOA REAL, nÃ£o um assistente formal
2. **Use o tom certo**: Adapte-se ao estado emocional detectado ({emotional_tone})
3. **Seja conversacional**:
   - Use contraÃ§Ãµes naturais (tÃ¡, nÃ©, pra, vou dar uma olhada)
   - Emojis moderados quando apropriado
   - Linguagem do dia-a-dia
4. **Mostre personalidade**: VocÃª Ã© prestativa, curiosa e GOSTA de ajudar

{task_section}

**IMPORTANTE:**
- NÃƒO use linguagem corporativa ("prezado usuÃ¡rio", "conforme solicitado")
- NÃƒO comece com "De acordo com..." ou "Conforme..."
- NÃƒO seja robÃ³tica
- SEJA genuÃ­na e humana
- Se precisar pedir informaÃ§Ã£o, pergunte de forma natural

**RESPONDA APENAS O TEXTO (sem JSON, sem tags):**
'''

    def _parse_reasoning(self, content: str) -> Dict[str, Any]:
        """Parse do resultado de raciocÃ­nio com validaÃ§Ã£o robusta"""

        # Limpar markdown se presente
        if "```json" in content:
            match = re.search(r"```json\n(.*?)""", content, re.DOTALL)
            if match:
                content = match.group(1).strip()
        elif "```" in content:
            match = re.search(r"```\n(.*?)""", content, re.DOTALL)
            if match:
                content = match.group(1).strip()

        try:
            result = json.loads(content)

            # Validar campos obrigatÃ³rios
            if "mode" not in result:
                result["mode"] = "conversational"  # Default seguro

            if "emotional_tone" not in result:
                result["emotional_tone"] = "neutro"

            if "reasoning" not in result:
                result["reasoning"] = "AnÃ¡lise automÃ¡tica da intenÃ§Ã£o"

            return result

        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ Erro ao parsear reasoning JSON: {e}")
            return self._create_fallback_reasoning()

    def _create_fallback_reasoning(self) -> Dict[str, Any]:
        """Cria um reasoning fallback para casos de erro"""
        return {
            "mode": "conversational",
            "reasoning": "NÃ£o foi possÃ­vel analisar completamente a intenÃ§Ã£o. Vou responder de forma conversacional.",
            "emotional_tone": "neutro",
            "confidence": 0.5,
            "needs_clarification": False,
            "next_action": {
                "type": "respond_directly",
                "response_style": "friendly"
            }
        }

    def _clean_response(self, response: str) -> str:
        """Remove tags JSON ou markdown indesejadas da resposta"""

        # Remover blocos JSON
        response = re.sub(r'```json.*?```', '', response, flags=re.DOTALL)
        response = re.sub(r'```.*?```', '', response, flags=re.DOTALL)

        # Remover objetos JSON soltos
        response = re.sub(r'\{["\']content["\']\s*:\s*["\'].*?["\']\}', '', response, flags=re.DOTALL)

        return response.strip()

    def _get_fallback_response(self, emotional_tone: str) -> str:
        """Retorna uma resposta fallback baseada no tom emocional"""

        fallback_responses = {
            "frustrado": "Poxa, desculpa pela dificuldade! ğŸ˜• Pode me explicar de novo o que vocÃª precisa? Vou tentar ajudar de outra forma.",
            "curioso": "Boa pergunta! ğŸ˜Š Deixa eu te ajudar com isso. Pode me dar mais detalhes sobre o que vocÃª quer saber?",
            "casual": "Claro! ğŸ‘ Como posso te ajudar?",
            "urgente": "Entendi! Vou te ajudar rapidinho. Pode me dar mais detalhes?",
            "confuso": "Sem problemas! Vou te explicar melhor. O que vocÃª gostaria de saber?",
            "neutro": "OlÃ¡! Sou a Caculinha, sua assistente de dados. Como posso te ajudar?"
        }

        return fallback_responses.get(emotional_tone, fallback_responses["neutro"])