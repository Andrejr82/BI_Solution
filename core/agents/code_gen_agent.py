"""
M√≥dulo para core/agents/code_gen_agent.py. Define a classe principal 'CodeGenAgent'. Fornece as fun√ß√µes: generate_and_execute_code, worker.
"""

# core/agents/code_gen_agent.py
import logging
import os
import json
import re
import pandas as pd
import dask.dataframe as dd  # Dask para lazy loading
import time

# ‚úÖ NOVO: Import Polars (condicional)
try:
    import polars as pl
    POLARS_AVAILABLE = True
except ImportError:
    pl = None
    POLARS_AVAILABLE = False
import plotly.express as px
from typing import List, Dict, Any, Tuple # Import necessary types
import threading
from queue import Queue
import pickle
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import io
import sys
import plotly.io as pio
import uuid
from core.utils.json_utils import _clean_json_values # Import the cleaning function

from core.llm_base import BaseLLMAdapter
from core.learning.pattern_matcher import PatternMatcher
from core.validation.code_validator import CodeValidator
from core.learning.dynamic_prompt import DynamicPrompt
from core.learning.self_healing_system import SelfHealingSystem
from core.config.column_mapping import normalize_column_name, validate_columns, get_essential_columns
from core.utils.column_validator import (
    validate_query_code,
    extract_columns_from_query,
    ColumnValidationError
)
from core.rag.query_retriever import QueryRetriever
from core.rag.example_collector import ExampleCollector
from core.agents.polars_load_data import create_optimized_load_data

class CodeGenAgent:
    """
    Agente especializado em gerar e executar c√≥digo Python para an√°lise de dados.
    """
    def __init__(self, llm_adapter: BaseLLMAdapter, data_adapter: any = None):
        """
        Inicializa o agente com o adaptador LLM e opcionalmente o adaptador de dados.

        Args:
            llm_adapter: Adaptador LLM para gera√ß√£o de c√≥digo
            data_adapter: (Opcional) Adaptador de dados para inje√ß√£o de load_data()
                         Se None, load_data() usar√° path padr√£o do Parquet
        """
        self.logger = logging.getLogger(__name__)
        self.llm = llm_adapter
        self.data_adapter = data_adapter  # Pode ser None (fallback para path padr√£o)
        self.code_cache = {}

        # ‚úÖ CORRE√á√ÉO: Usar nomes REAIS do Parquet (confirmados via read_parquet_schema em 2025-10-27)
        self.column_descriptions = {
            "codigo": "C√≥digo √∫nico do produto (COLUNA PARQUET: codigo)",
            "nome_produto": "Nome/descri√ß√£o do produto (COLUNA PARQUET: nome_produto)",
            "nomesegmento": "Segmento do produto (COLUNA PARQUET: nomesegmento) - Ex: TECIDOS, PAPELARIA, etc.",
            "NOMECATEGORIA": "Categoria do produto (COLUNA PARQUET: NOMECATEGORIA)",
            "nomegrupo": "Grupo do produto (COLUNA PARQUET: nomegrupo)",
            "NOMESUBGRUPO": "Subgrupo do produto (COLUNA PARQUET: NOMESUBGRUPO)",
            "NOMEFABRICANTE": "Fabricante do produto (COLUNA PARQUET: NOMEFABRICANTE)",
            "venda_30_d": "Total de vendas nos √∫ltimos 30 dias (COLUNA PARQUET: venda_30_d)",
            "estoque_atual": "Quantidade em estoque total da UNE (COLUNA PARQUET: estoque_atual)",
            "estoque_lv": "Estoque na Linha Verde/√°rea de venda (COLUNA PARQUET: estoque_lv)",
            "estoque_cd": "Estoque no Centro de Distribui√ß√£o (COLUNA PARQUET: estoque_cd)",
            "preco_38_percent": "Pre√ßo de venda com 38% de margem (COLUNA PARQUET: preco_38_percent)",
            "une": "ID num√©rico da loja/unidade (COLUNA PARQUET: une) - Ex: 1, 2586, 2720",
            "une_nome": "Nome da loja/unidade (COLUNA PARQUET: une_nome) - Ex: SCR, MAD, 261, ALC, NIL",
            "tipo": "Tipo de produto (COLUNA PARQUET: tipo)",
            "embalagem": "Embalagem do produto (COLUNA PARQUET: embalagem)",
            "ean": "C√≥digo de barras (COLUNA PARQUET: ean)",
            "media_considerada_lv": "M√©dia de vendas considerada para reposi√ß√£o (COLUNA PARQUET: media_considerada_lv)",
            "abc_une_30_dd": "Classifica√ß√£o ABC da UNE nos √∫ltimos 30 dias (COLUNA PARQUET: abc_une_30_dd)",
            # üìä COLUNAS TEMPORAIS - Vendas mensais (mes_01 = m√™s mais recente)
            "mes_01": "Vendas do m√™s mais recente (m√™s 1)",
            "mes_02": "Vendas de 2 meses atr√°s",
            "mes_03": "Vendas de 3 meses atr√°s",
            "mes_04": "Vendas de 4 meses atr√°s",
            "mes_05": "Vendas de 5 meses atr√°s",
            "mes_06": "Vendas de 6 meses atr√°s",
            "mes_07": "Vendas de 7 meses atr√°s",
            "mes_08": "Vendas de 8 meses atr√°s",
            "mes_09": "Vendas de 9 meses atr√°s",
            "mes_10": "Vendas de 10 meses atr√°s",
            "mes_11": "Vendas de 11 meses atr√°s",
            "mes_12": "Vendas de 12 meses atr√°s (m√™s mais antigo)"
        }

        # Inicializar pattern_matcher, code_validator e RAG
        try:
            self.query_retriever = QueryRetriever()
            self.example_collector = ExampleCollector()
            self.rag_enabled = True
            self.logger.info("Sistema RAG inicializado com sucesso")
        except Exception as e:
            self.logger.warning(f"RAG n√£o dispon√≠vel: {e}. Continuando sem RAG.")
            self.query_retriever = None
            self.example_collector = None
            self.rag_enabled = False

        # Inicializar pattern_matcher and code_validator
        from collections import defaultdict
        try:
            self.pattern_matcher = PatternMatcher()
            self.logger.info("‚úÖ PatternMatcher inicializado (Few-Shot Learning ativo)")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è PatternMatcher n√£o dispon√≠vel: {e}")
            self.pattern_matcher = None

        self.code_validator = CodeValidator()
        self.error_counts = defaultdict(int)
        self.logs_dir = os.path.join(os.getcwd(), "data", "learning")
        os.makedirs(self.logs_dir, exist_ok=True)

        # Inicializar DynamicPrompt (Pilar 4)
        try:
            self.dynamic_prompt = DynamicPrompt()
            self.logger.info("‚úÖ DynamicPrompt inicializado (Pilar 4 ativo)")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è DynamicPrompt n√£o dispon√≠vel: {e}")
            self.dynamic_prompt = None

        # Inicializar Self-Healing System (Auto-corre√ß√£o)
        try:
            self.self_healing = SelfHealingSystem(
                llm_adapter=llm_adapter,
                schema_validator=True
            )
            self.logger.info("‚úÖ SelfHealingSystem inicializado (Auto-corre√ß√£o ativa)")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è SelfHealingSystem n√£o dispon√≠vel: {e}")
            self.self_healing = None

        # ‚ö° SOLU√á√ÉO ZERO-CLICK: Cache √© gerenciado 100% automaticamente
        # Usu√°rio N√ÉO precisa clicar em nada, deslogar ou recarregar p√°gina

        # 1. Limpar cache antigo (5 minutos - MUITO curto para for√ßar regenera√ß√£o r√°pida)
        self._clean_old_cache(max_age_hours=0.08)  # ~5 minutos

        # 2. Invalidar cache quando prompt/c√≥digo muda (detecta corre√ß√µes automaticamente)
        self._check_and_invalidate_cache_if_prompt_changed()

        self.logger.info("CodeGenAgent inicializado.")

    def _execute_generated_code(self, code: str, local_scope: Dict[str, Any]):
        q = Queue()
        output_capture = io.StringIO()
        original_stdout = sys.stdout
        original_stderr = sys.stderr

        # Fun√ß√£o helper para ser injetada no escopo de execu√ß√£o
        def load_data(filters: Dict[str, Any] = None):
            """
            üöÄ OTIMIZADO: Carrega o dataframe usando PolarsDaskAdapter (h√≠brido Polars/Dask).

            Args:
                filters: Dicion√°rio opcional de filtros para aplicar ANTES de carregar dados.
                        Ex: {'UNE': 'MAD'}, {'NOMESEGMENTO': 'TECIDOS'}, {'PRODUTO': 12345}
                        Filtros reduzem drasticamente mem√≥ria e tempo de carregamento.

            Returns:
                pandas DataFrame (j√° filtrado se filters fornecido)

            IMPORTANTE:
            - COM filtros: Usa PolarsDaskAdapter (predicate pushdown, 5-10x mais r√°pido)
            - SEM filtros: Carrega apenas 10k linhas (prote√ß√£o contra OOM)
            """
            import pandas as pd
            import os

            if self.data_adapter and filters:
                # ‚úÖ USAR ADAPTER COM FILTROS (Polars/Dask com predicate pushdown)
                import time
                start_time = time.time()

                self.logger.info("=" * 80)
                self.logger.info("üîç PLANO A - LOAD_DATA() COM FILTROS")
                self.logger.info(f"   Filtros aplicados: {filters}")
                self.logger.info(f"   Adapter: PolarsDaskAdapter (predicate pushdown)")

                try:
                    # Delegar para adapter (usa Polars ou Dask automaticamente)
                    result_list = self.data_adapter.execute_query(filters)
                    elapsed = time.time() - start_time

                    self.logger.info(f"‚úÖ SUCESSO - {len(result_list):,} registros carregados em {elapsed:.2f}s")
                    self.logger.info(f"   Performance: {len(result_list)/elapsed:.0f} registros/segundo")
                    self.logger.info("=" * 80)

                    return pd.DataFrame(result_list)

                except Exception as e:
                    elapsed = time.time() - start_time
                    self.logger.error("=" * 80)
                    self.logger.error(f"‚ùå ERRO ao carregar com filtros (ap√≥s {elapsed:.2f}s)")
                    self.logger.error(f"   Tipo: {type(e).__name__}")
                    self.logger.error(f"   Mensagem: {str(e)}")
                    self.logger.error("=" * 80)
                    # Fallback para modo sem filtros (limitado)
                    self.logger.warning("‚ö†Ô∏è  Caindo para modo sem filtros (limitado a 10k linhas)")
                    filters = None  # Trigger fallback abaixo

            if not filters:
                # ‚ö†Ô∏è SEM FILTROS - Modo de prote√ß√£o (limitar a 10k linhas)
                self.logger.warning("‚ö†Ô∏è  load_data() SEM filtros - LIMITANDO a 10.000 linhas para evitar OOM")
                self.logger.warning("   RECOMENDA√á√ÉO: Passe filtros para carregar dados completos")
                self.logger.warning("   Exemplo: load_data(filters={'UNE': 'MAD', 'NOMESEGMENTO': 'TECIDOS'})")

                import dask.dataframe as dd

                # Definir parquet_path para uso no fallback
                parquet_path = None

                if self.data_adapter:
                    file_path = getattr(self.data_adapter, 'file_path', None)
                    if file_path:
                        parquet_path = file_path  # Salvar para fallback
                        ddf = dd.read_parquet(file_path, engine='pyarrow')
                    else:
                        raise AttributeError(f"Adapter {type(self.data_adapter).__name__} n√£o tem file_path")
                else:
                    # Fallback: carregar diretamente do Parquet
                    parquet_dir = os.path.join(os.getcwd(), "data", "parquet")
                    parquet_pattern = os.path.join(parquet_dir, "*.parquet")
                    if not os.path.exists(parquet_dir):
                        raise FileNotFoundError(f"Diret√≥rio Parquet n√£o encontrado em {parquet_dir}")
                    parquet_path = parquet_pattern  # Salvar para fallback
                    ddf = dd.read_parquet(parquet_pattern, engine='pyarrow')

                # Normalizar colunas
                column_mapping = {
                    'une': 'UNE_ID',
                    'nomesegmento': 'NOMESEGMENTO',
                    'codigo': 'PRODUTO',
                    'nome_produto': 'NOME',
                    'une_nome': 'UNE',
                    'nomegrupo': 'NOMEGRUPO',
                    'ean': 'EAN',
                    'preco_38_percent': 'LIQUIDO_38',
                    'venda_30_d': 'VENDA_30DD',
                    'estoque_atual': 'ESTOQUE_UNE',
                    'embalagem': 'EMBALAGEM',
                    'tipo': 'TIPO',
                    'NOMECATEGORIA': 'NOMECATEGORIA',
                    'NOMESUBGRUPO': 'NOMESUBGRUPO',
                    'NOMEFABRICANTE': 'NOMEFABRICANTE'
                }

                rename_dict = {k: v for k, v in column_mapping.items() if k in ddf.columns}
                ddf = ddf.rename(columns=rename_dict)

                # Converter tipos
                if 'ESTOQUE_UNE' in ddf.columns:
                    ddf['ESTOQUE_UNE'] = dd.to_numeric(ddf['ESTOQUE_UNE'], errors='coerce').fillna(0)

                for i in range(1, 13):
                    col_name = f'mes_{i:02d}'
                    if col_name in ddf.columns:
                        ddf[col_name] = dd.to_numeric(ddf[col_name], errors='coerce').fillna(0)

                # ‚ö†Ô∏è LIMITAR A 10K LINHAS (prote√ß√£o contra OOM)
                self.logger.info(f"‚ö° load_data(): Limitando a 10.000 linhas (sem filtros)")
                import time as time_module
                start_compute = time_module.time()

                try:
                    # Computar apenas primeiras 10k linhas
                    df_pandas = ddf.head(10000, npartitions=-1)
                except Exception as compute_error:
                    self.logger.error(f"‚ùå Erro ao computar Dask: {compute_error}")
                    self.logger.warning("üîÑ Tentando fallback: carregar direto do Parquet com pandas (modo otimizado)")

                    # Estrat√©gia de fallback melhorada
                    try:
                        if parquet_path:
                            # Estrat√©gia 1: Usar pandas com limite de linhas e colunas essenciais
                            self.logger.info("   Tentando carregar com pandas (apenas colunas essenciais)...")

                            # Resolver wildcard pattern
                            import glob
                            if '*' in parquet_path:
                                parquet_files = glob.glob(parquet_path)
                                if not parquet_files:
                                    raise FileNotFoundError(f"Nenhum arquivo encontrado em: {parquet_path}")
                                parquet_path = parquet_files[0]  # Usar primeiro arquivo
                                self.logger.info(f"üìÅ Usando arquivo: {os.path.basename(parquet_path)}")

                            # Colunas essenciais para an√°lises b√°sicas (NOMES CORRETOS DO PARQUET)
                            essential_cols = get_essential_columns()
                            self.logger.info(f"   Carregando colunas essenciais: {essential_cols}")

                            df_pandas = pd.read_parquet(
                                parquet_path,
                                engine='pyarrow',
                                columns=essential_cols
                            ).head(10000)

                            self.logger.info(f"‚úÖ Fallback bem-sucedido: {len(df_pandas)} registros carregados com {len(df_pandas.columns)} colunas")
                        else:
                            raise FileNotFoundError(f"Parquet path n√£o dispon√≠vel: {parquet_path}")

                    except Exception as fallback_error:
                        self.logger.error(f"‚ùå Fallback otimizado falhou: {fallback_error}")

                        # Estrat√©gia 2: Carregar apenas 1000 linhas como √∫ltimo recurso
                        try:
                            self.logger.warning("   Tentando carregar apenas 1000 linhas como √∫ltimo recurso...")
                            df_pandas = ddf.head(1000, npartitions=-1)
                            self.logger.info(f"‚ö†Ô∏è  Carregado dataset MUITO reduzido: {len(df_pandas)} linhas")
                        except:
                            self.logger.error(f"‚ùå Todas as estrat√©gias de fallback falharam")
                            # Mensagem amig√°vel ao usu√°rio (sem stacktrace t√©cnico)
                            error_msg = (
                                "‚ùå **Erro ao Processar Consulta**\n\n"
                                "O sistema est√° com recursos limitados no momento.\n\n"
                                "**üí° Sugest√µes:**\n"
                                "- Tente uma consulta mais espec√≠fica (ex: filtre por UNE ou segmento)\n"
                                "- Divida sua an√°lise em partes menores\n"
                                "- Aguarde alguns segundos e tente novamente\n\n"
                                "**Exemplo de consulta espec√≠fica:**\n"
                                "`Top 10 produtos da UNE SCR do segmento TECIDOS`"
                            )
                            raise RuntimeError(error_msg)

                end_compute = time_module.time()
                self.logger.info(f"‚úÖ load_data(): {len(df_pandas)} registros carregados (LIMITADO) em {end_compute - start_compute:.2f}s")

                return df_pandas

        # ‚úÖ NOVO: Usar load_data otimizada com Polars
        try:
            # Usar pattern correto: admmat*.parquet (n√£o admmat_une*.parquet)
            parquet_path = os.path.join("data", "parquet", "admmat*.parquet")
            optimized_load_data = create_optimized_load_data(parquet_path, self.data_adapter)
            local_scope['load_data'] = optimized_load_data
            self.logger.info("‚úÖ Using optimized Polars load_data()")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao criar load_data otimizada: {e}. Usando vers√£o antiga.")
            local_scope['load_data'] = load_data  # Fallback para vers√£o antiga

        local_scope['dd'] = dd  # Adicionar Dask ao escopo para c√≥digo gerado (se necess√°rio)
        local_scope['time'] = __import__('time')  # Adicionar m√≥dulo time ao escopo para evitar UnboundLocalError
        local_scope['pl'] = pl  # ‚úÖ NOVO: Adicionar Polars ao escopo
        local_scope['pd'] = pd  # Adicionar Pandas para compatibilidade

        def worker():
            sys.stdout = output_capture
            sys.stderr = output_capture
            try:
                exec(code, local_scope)
                q.put(local_scope.get('result'))
            except KeyError as e:
                # ‚úÖ TRATAMENTO ESPEC√çFICO: Erro de coluna n√£o encontrada
                error_msg = str(e)

                # Detectar se √© erro de coluna
                if "nome_produto" in error_msg or "KeyError" in str(type(e).__name__):
                    self.logger.error(f"‚ùå Erro de coluna n√£o encontrada: {e}")

                    # Tentar extrair nome da coluna do erro
                    import re
                    col_match = re.search(r"['\"]([^'\"]+)['\"]", error_msg)
                    if col_match:
                        missing_col = col_match.group(1)
                        self.logger.error(f"   Coluna faltante: '{missing_col}'")

                    # Criar erro mais informativo
                    enhanced_error = ColumnValidationError(
                        missing_col if col_match else "desconhecida",
                        suggestions=[],
                        available_columns=[]
                    )
                    q.put(enhanced_error)
                else:
                    q.put(e)

            except Exception as e:
                # ‚úÖ TRATAMENTO GEN√âRICO: Capturar outros erros do Polars
                error_type = type(e).__name__

                # Detectar erros comuns do Polars
                if any(err in error_type for err in ["ColumnNotFoundError", "SchemaError", "ComputeError"]):
                    self.logger.error(f"‚ùå Erro do Polars: {error_type} - {e}")

                q.put(e)
            finally:
                sys.stdout = original_stdout
                sys.stderr = original_stderr

        thread = threading.Thread(target=worker)
        thread.start()
        thread.join(timeout=120.0)

        captured_output = output_capture.getvalue()
        if captured_output:
            self.logger.info(f"Sa√≠da do c√≥digo gerado:\n{captured_output}")

        if thread.is_alive():
            raise TimeoutError("A execu√ß√£o do c√≥digo gerado excedeu o tempo limite.")
        else:
            result = q.get()
            if isinstance(result, Exception):
                raise result
            return result

    def _normalize_query(self, query: str) -> str:
        """
        Normaliza query para melhorar cache hit rate.
        Remove stopwords e varia√ß√µes irrelevantes, mantendo sem√¢ntica.
        """
        query = query.lower().strip()

        # Stopwords comuns em portugu√™s que n√£o afetam a sem√¢ntica da query
        stopwords = [
            'qual', 'quais', 'mostre', 'me', 'gere', 'por favor', 'por gentileza',
            'poderia', 'pode', 'consegue', 'voc√™', 'o', 'a', 'os', 'as',
            'um', 'uma', 'uns', 'umas', 'de', 'da', 'do', 'das', 'dos'
        ]

        # Remover stopwords
        words = query.split()
        filtered_words = [w for w in words if w not in stopwords]
        query = ' '.join(filtered_words)

        # Normalizar varia√ß√µes comuns
        replacements = {
            'gr√°fico': 'graf',
            'gr√°ficos': 'graf',
            'grafico': 'graf',
            'graficos': 'graf',
            'ranking': 'rank',
            'rankings': 'rank',
            'top 5': 'top5',
            'top 10': 'top10',
            'top 20': 'top20',
            '√∫ltimos': 'ultimos',
            '√∫ltimo': 'ultimo',
            'an√°lise': 'analise',
            'an√°lises': 'analise',
        }

        for old, new in replacements.items():
            query = query.replace(old, new)

        # Remover espa√ßos extras
        query = ' '.join(query.split())

        return query

    def _detect_complex_query(self, query: str) -> bool:
        """
        Detecta se query requer racioc√≠nio multi-step (chain-of-thought).

        Baseado em: Context7 - OpenAI Prompt Engineering Best Practices
        """
        complex_keywords = [
            'an√°lise abc', 'distribui√ß√£o', 'sazonalidade', 'tend√™ncia',
            'comparar', 'compara√ß√£o', 'correla√ß√£o', 'previs√£o',
            'alertas', 'insights', 'padr√µes', 'anomalias'
        ]
        query_lower = query.lower()
        return any(kw in query_lower for kw in complex_keywords)

    def _build_structured_prompt(self, user_query: str, rag_examples: list = None) -> str:
        """
        Constr√≥i prompt estruturado seguindo OpenAI best practices.

        Baseado em: Context7 - Developer Message Pattern + Few-Shot Learning

        Hierarquia:
        1. Developer message - Identidade e comportamento do agente
        2. Few-shot examples - Exemplos rotulados (do RAG)
        3. User message - Query atual com instru√ß√µes espec√≠ficas

        Args:
            user_query: Query do usu√°rio
            rag_examples: Lista de exemplos similares do RAG (opcional)

        Returns:
            Prompt estruturado em formato string
        """

        # 1Ô∏è‚É£ DEVELOPER MESSAGE - Identidade e Comportamento
        developer_context = f"""# ü§ñ IDENTIDADE E COMPORTAMENTO

Voc√™ √© um especialista em an√°lise de dados Python com foco em:
- **Pandas/Polars**: Manipula√ß√£o eficiente de DataFrames
- **Plotly**: Visualiza√ß√µes interativas de alta qualidade
- **An√°lise de Neg√≥cios**: Varejo, vendas, estoque, categoriza√ß√£o

## üéØ Seu Objetivo

Gerar c√≥digo Python **limpo, eficiente e seguro** que responda √† pergunta do usu√°rio usando o dataset de vendas fornecido.

## üìä CONTEXTO DO DOM√çNIO

**Dataset**: Vendas de varejo (produtos, UNEs/lojas, categorias, estoques)
**Per√≠odo**: 12 meses de hist√≥rico (mes_01 = mais recente, mes_12 = mais antigo)
**M√©tricas Principais**:
- `venda_30_d`: Vendas dos √∫ltimos 30 dias (M√âTRICA PRIM√ÅRIA)
- `estoque_atual`: Estoque total dispon√≠vel
- `preco_38_percent`: Pre√ßo de venda com margem de 38%
- `mes_01` a `mes_12`: Vendas mensais (s√©rie temporal)

## üóÇÔ∏è SCHEMA DE COLUNAS DISPON√çVEIS

{json.dumps(self.column_descriptions, indent=2, ensure_ascii=False)}

## ‚ö†Ô∏è REGRAS CR√çTICAS

1. **Nomes de Colunas**: SEMPRE use nomes EXATOS do schema (case-sensitive)
2. **Valida√ß√£o**: SEMPRE valide colunas antes de usar (ex: `if 'une_nome' in df.columns`)
3. **Performance**: SEMPRE use Polars para grandes datasets (scan_parquet com lazy evaluation)
4. **Seguran√ßa**: NUNCA use `eval()` ou `exec()` com input do usu√°rio
5. **Output**: SEMPRE retorne resultados em formato estruturado (dict, DataFrame ou Plotly Figure)
6. **Coment√°rios**: SEMPRE adicione coment√°rios explicativos no c√≥digo

## üéØ REGRAS DE RANKING (TOP N vs TODOS)

**DETEC√á√ÉO DE INTEN√á√ÉO:**
- **"top 10", "top 5", "maiores", "menores" + N√öMERO** ‚Üí Use `.head(N)` para limitar
- **"ranking de TODAS", "ranking COMPLETO", "TODAS as unes/produtos"** ‚Üí N√ÉO use `.head()`, mostre TODOS
- **"ranking" gen√©rico SEM "todas/todos" E SEM n√∫mero** ‚Üí Use `.head(10)` como padr√£o (melhor visualiza√ß√£o)

**EXEMPLOS:**

```python
# ‚úÖ CASO 1: "gere gr√°fico ranking de vendas das unes" (SEM "top N", SEM "todas")
df = load_data()
ranking = df.groupby('une_nome')['venda_30_d'].sum().sort_values(ascending=False).reset_index()
df_top10 = ranking.head(10)  # Padr√£o: top 10 para visualiza√ß√£o limpa
result = px.bar(df_top10, x='une_nome', y='venda_30_d')

# ‚úÖ CASO 2: "gere gr√°fico ranking de TODAS as unes" (EXPLICITAMENTE "todas")
df = load_data()
ranking_completo = df.groupby('une_nome')['venda_30_d'].sum().sort_values(ascending=False).reset_index()
# N√ÉO usar .head() quando usu√°rio pede "todas"
result = px.bar(ranking_completo, x='une_nome', y='venda_30_d')

# ‚úÖ CASO 3: "top 5 unes por vendas" (N√∫mero EXPL√çCITO)
df = load_data()
ranking = df.groupby('une_nome')['venda_30_d'].sum().sort_values(ascending=False).reset_index()
df_top5 = ranking.head(5)
result = px.bar(df_top5, x='une_nome', y='venda_30_d')
```

**PALAVRAS-CHAVE DE DETEC√á√ÉO:**
- **Limitar**: "top", "maiores", "principais", "primeiros" + N√öMERO
- **N√£o limitar**: "todas", "todos", "completo", "completa", "integral"
"""

        # 2Ô∏è‚É£ FEW-SHOT EXAMPLES - Exemplos Rotulados do RAG
        few_shot_section = ""
        if rag_examples and len(rag_examples) > 0:
            few_shot_section = "\n\n# üìö EXEMPLOS DE QUERIES BEM-SUCEDIDAS (Few-Shot Learning)\n\n"
            few_shot_section += "Use os exemplos abaixo como refer√™ncia para gerar c√≥digo similar:\n\n"

            for i, ex in enumerate(rag_examples[:3], 1):  # M√°ximo 3 exemplos
                similarity = ex.get('similarity_score', 0)
                few_shot_section += f"""## Exemplo {i} (Similaridade: {similarity:.1%})

**Query do Usu√°rio:** "{ex.get('query_user', 'N/A')}"

**C√≥digo Python Gerado:**
```python
{ex.get('code_generated', 'N/A')}
```

**Resultado:** {ex.get('result_type', 'success')} | {ex.get('rows_returned', 0)} registros retornados

---

"""

        # 3Ô∏è‚É£ CHAIN-OF-THOUGHT (para queries complexas)
        cot_section = ""
        if self._detect_complex_query(user_query):
            cot_section = """

## üß† RACIOC√çNIO PASSO-A-PASSO (Chain of Thought)

Esta √© uma query complexa. Divida o problema em etapas:

**Etapa 1: An√°lise da Query**
- Qual a m√©trica principal? (vendas, estoque, pre√ßo)
- Qual a dimens√£o de an√°lise? (produto, UNE, categoria, tempo)
- H√° filtros espec√≠ficos? (segmento, categoria, per√≠odo, UNE)

**Etapa 2: Planejamento do C√≥digo**
- Quais colunas do schema ser√£o necess√°rias?
- Quais transforma√ß√µes? (group by, pivot, melt, c√°lculos)
- Qual visualiza√ß√£o? (gr√°fico de barras, linha, pizza, tabela)

**Etapa 3: Implementa√ß√£o**
- C√≥digo Python otimizado com valida√ß√£o
- Tratamento de valores NA/null
- Coment√°rios explicativos

Execute cada etapa mentalmente antes de gerar o c√≥digo final.

"""

        # 4Ô∏è‚É£ USER MESSAGE - Query Atual
        user_message = f"""

## üéØ QUERY ATUAL DO USU√ÅRIO

**Pergunta:** {user_query}

## üìù INSTRU√á√ïES DE GERA√á√ÉO

1. **Analise** a query e identifique:
   - Tipo de an√°lise (ranking, filtro, agrega√ß√£o, visualiza√ß√£o)
   - Colunas necess√°rias do schema
   - Filtros aplic√°veis

2. **Gere c√≥digo Python** que:
   - Use a fun√ß√£o `load_data()` para carregar dados
   - Valide colunas antes de usar
   - Implemente a l√≥gica de an√°lise solicitada
   - Retorne resultado em vari√°vel `result`

3. **Formato de Sa√≠da**:
   - Para tabelas: `result` = DataFrame
   - Para gr√°ficos: `result` = Plotly Figure (px.bar, px.pie, px.line)
   - Para m√©tricas: `result` = dict com valores

## üíª C√ìDIGO PYTHON A SER GERADO:

```python
# Seu c√≥digo aqui
```
"""

        # CONCATENAR TODAS AS SE√á√ïES
        full_prompt = developer_context + few_shot_section + cot_section + user_message

        return full_prompt

    def generate_and_execute_code(self, input_data: Dict[str, Any]) -> dict:
        """
        Gera, executa e retorna o resultado do c√≥digo Python para uma dada consulta.
        Esta vers√£o foi refatorada para usar diretamente o prompt fornecido e injetar uma fun√ß√£o `load_data`.
        """
        prompt = input_data.get("query", "")
        raw_data = input_data.get("raw_data", [])
        user_query = input_data.get("query", "")  # Definir no in√≠cio para evitar UnboundLocalError

        # üéØ Cache inteligente V2: Normalizar query para maior hit rate
        # Isso permite que "Mostre o ranking de papelaria" = "ranking papelaria" = "top 10 papelaria"
        normalized_query = self._normalize_query(user_query)
        query_lower = user_query.lower()
        intent_markers = []

        # Detectar tipo de an√°lise
        if any(word in query_lower for word in ['gr√°fico', 'chart', 'visualiza√ß√£o', 'plot', 'graf']):
            intent_markers.append('viz')
        if any(word in query_lower for word in ['ranking', 'top', 'rank']):
            intent_markers.append('rank')

        # Detectar segmento espec√≠fico (extrair para evitar cache cruzado)
        import re as regex_module
        segment_match = regex_module.search(r'(tecido|papelaria|armarinho|festas|artes|casa|decora√ß√£o|higiene|beleza|esporte|lazer|bazar|el√©trica|limpeza|sazonais|inform√°tica|embalagens)', query_lower)
        if segment_match:
            intent_markers.append(f'seg_{segment_match.group(1)}')

        # Gerar chave de cache √∫nica baseada em query NORMALIZADA + inten√ß√£o
        # Usar query normalizada aumenta hit rate em ~30-50%
        cache_key = hash(normalized_query + '_'.join(intent_markers) + (json.dumps(raw_data, sort_keys=True) if raw_data else ""))

        self.logger.debug(f"Cache: query_original='{user_query}' ‚Üí normalized='{normalized_query}' ‚Üí key={cache_key}")

        if cache_key in self.code_cache:
            code_to_execute = self.code_cache[cache_key]
            self.logger.info(f"C√≥digo recuperado do cache.")
        else:
            # ‚úÖ CORRE√á√ÉO: Usar nomes REAIS do Parquet (confirmados via read_parquet_schema)
            important_columns = [
                "codigo", "nome_produto", "nomesegmento", "NOMECATEGORIA", "nomegrupo", "NOMESUBGRUPO",
                "NOMEFABRICANTE", "venda_30_d", "estoque_atual", "preco_38_percent",
                "une", "une_nome", "tipo", "embalagem", "ean",
                # Colunas temporais para gr√°ficos de evolu√ß√£o
                "mes_01", "mes_02", "mes_03", "mes_04", "mes_05", "mes_06",
                "mes_07", "mes_08", "mes_09", "mes_10", "mes_11", "mes_12",
                # Colunas de an√°lise adicional
                "media_considerada_lv", "estoque_lv", "estoque_cd", "abc_une_30_dd"
            ]

            column_context = "üìä COLUNAS DISPON√çVEIS:\n"
            for col in important_columns:
                if col in self.column_descriptions:
                    column_context += f"- {col}: {self.column_descriptions[col]}\n"

            # Adicionar valores v√°lidos de segmentos com mapeamento inteligente
            valid_segments = """
**VALORES V√ÅLIDOS DE SEGMENTOS (NOMESEGMENTO):**
Use EXATAMENTE estes valores no c√≥digo Python (incluindo acentos e plural/singular):

1. 'TECIDOS' ‚Üí se usu√°rio mencionar: tecido, tecidos, segmento tecido, tecidos e armarinhos
2. 'ARMARINHO E CONFEC√á√ÉO' ‚Üí se usu√°rio mencionar: armarinho, confec√ß√£o, aviamentos
3. 'PAPELARIA' ‚Üí se usu√°rio mencionar: papelaria, papel, cadernos
4. 'CASA E DECORA√á√ÉO' ‚Üí se usu√°rio mencionar: casa, decora√ß√£o, utilidades dom√©sticas
5. 'ARTES' ‚Üí se usu√°rio mencionar: artes, artesanato, pintura
6. 'SAZONAIS' ‚Üí se usu√°rio mencionar: sazonais, p√°scoa, natal, datas comemorativas
7. 'FESTAS' ‚Üí se usu√°rio mencionar: festas, anivers√°rio, bal√µes
8. 'INFORM√ÅTICA' ‚Üí se usu√°rio mencionar: inform√°tica, eletr√¥nica, computadores
9. 'HIGIENE E BELEZA' ‚Üí se usu√°rio mencionar: higiene, beleza, cosm√©ticos
10. 'ESPORTE E LAZER' ‚Üí se usu√°rio mencionar: esporte, lazer, brinquedos
11. 'EMBALAGENS E DESCART√ÅVEIS' ‚Üí se usu√°rio mencionar: embalagens, descart√°veis
12. 'BAZAR' ‚Üí se usu√°rio mencionar: bazar, utilidades
13. 'EL√âTRICA E MANUTEN√á√ÉO' ‚Üí se usu√°rio mencionar: el√©trica, manuten√ß√£o, ferramentas
14. 'MATERIAL DE LIMPEZA' ‚Üí se usu√°rio mencionar: limpeza, produtos de limpeza

**REGRA DE OURO:** Interprete a inten√ß√£o do usu√°rio e mapeie para o valor EXATO da lista acima!
"""

            # ‚úÖ CORRE√á√ÉO CR√çTICA: Atualizar instru√ß√µes para usar 'une_nome' (n√£o 'UNE')
            valid_unes = """
**üö® VALORES V√ÅLIDOS DE LOJAS/UNIDADES (SCHEMA CORRETO DO PARQUET):**

O Parquet possui DUAS colunas relacionadas a UNE:
1. **une** (int) - ID num√©rico da loja (ex: 1, 2586, 2720)
2. **une_nome** (str) - Nome da loja (ex: 'SCR', 'MAD', '261')

**NOMES V√ÅLIDOS (coluna une_nome):**
'SCR', 'ALC', 'DC', 'CFR', 'PET', 'VVL', 'VIL', 'REP', 'JFA', 'NIT',
'CGR', 'OBE', 'CXA', '261', 'BGU', 'ALP', 'BAR', 'CP2', 'JRD', 'NIG',
'ITA', 'MAD', 'JFJ', 'CAM', 'VRD', 'SGO', 'NFR', 'TIJ', 'ANG', 'BON',
'IPA', 'BOT', 'NIL', 'TAQ', 'RDO', '3RS', 'STS', 'NAM'

**‚úÖ EXEMPLOS CORRETOS (usar une_nome, N√ÉO 'UNE'):**
```python
# Filtrar por nome de loja (use une_nome!)
df_mad = df[df['une_nome'] == 'MAD']
df_scr = df[df['une_nome'] == 'SCR']
df_261 = df[df['une_nome'] == '261']
df_nil = df[df['une_nome'] == 'NIL']
```

**‚ùå ERRADO (N√ÉO use 'UNE', essa coluna N√ÉO EXISTE no Parquet!):**
```python
df_mad = df[df['UNE'] == 'MAD']  # ‚ùå KeyError: 'UNE'
```

**REGRA DE OURO:** SEMPRE use 'une_nome' para filtrar por loja!
Se precisar do ID num√©rico, use a coluna 'une' (min√∫sculo).
"""

            # üéØ PILAR 2: Injetar exemplos contextuais baseados em padr√µes (Few-Shot Learning)
            examples_context = ""
            if self.pattern_matcher:
                try:
                    # Buscar padr√£o similar √† query do usu√°rio
                    matched_pattern = self.pattern_matcher.match_pattern(user_query)
                    if matched_pattern:
                        # Formatar exemplos para inje√ß√£o no prompt
                        examples_context = self.pattern_matcher.format_examples_for_prompt(matched_pattern, max_examples=2)
                        self.logger.info(f"üéØ Few-Shot Learning: Padr√£o '{matched_pattern.pattern_name}' identificado com {len(matched_pattern.examples)} exemplos")
                    else:
                        self.logger.debug("‚ÑπÔ∏è Nenhum padr√£o espec√≠fico identificado para esta query")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao buscar padr√µes: {e}")

            # üéØ PILAR 2.5: RAG - Busca sem√¢ntica de queries similares (expandir Few-Shot Learning)
            rag_examples = []
            if self.rag_enabled and self.query_retriever:
                try:
                    similar_queries = self.query_retriever.find_similar_queries(user_query, top_k=3)
                    if similar_queries:
                        # Filtrar apenas exemplos com alta similaridade (> 0.7)
                        rag_examples = [ex for ex in similar_queries if ex.get('similarity_score', 0) > 0.7]

                        if rag_examples:
                            self.logger.info(f"üîç RAG: {len(rag_examples)} queries similares de alta qualidade encontradas (melhor match: {similar_queries[0].get('similarity_score', 0):.2%})")
                        else:
                            self.logger.debug("‚ÑπÔ∏è RAG: Nenhuma query com similaridade > 0.7 encontrada")
                    else:
                        self.logger.debug("‚ÑπÔ∏è RAG: Nenhuma query similar encontrada no banco")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao buscar queries similares (RAG): {e}")

            # ‚úÖ NOVO: Construir prompt estruturado usando Context7 best practices
            # Developer message + Few-shot learning + Chain-of-thought
            system_prompt = self._build_structured_prompt(user_query, rag_examples=rag_examples)

            # Adicionar contexto de exemplos do PatternMatcher (se houver)
            if examples_context:
                system_prompt += f"\n\n{examples_context}"

            self.logger.info(f"‚úÖ Prompt estruturado gerado: {len(system_prompt)} caracteres, {len(rag_examples)} exemplos RAG")

            # üöÄ PILAR 4: Adicionar avisos din√¢micos baseados em erros recentes
            if self.dynamic_prompt:
                try:
                    enhanced_prompt = self.dynamic_prompt.get_enhanced_prompt()
                    # Adicionar avisos ao system_prompt
                    system_prompt = system_prompt + "\n\n" + enhanced_prompt
                    self.logger.info("‚úÖ Prompt enriquecido com DynamicPrompt (Pilar 4)")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao enriquecer prompt: {e}")

            # O agente agora usa o prompt diretamente, sem construir um novo.
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]

            start_llm_query = time.time()
            llm_response = self.llm.get_completion(messages=messages)
            end_llm_query = time.time()
            self.logger.info(f"Tempo de consulta LLM: {end_llm_query - start_llm_query:.4f} segundos")

            if "error" in llm_response:
                self.logger.error(f"Erro ao obter resposta do LLM: {llm_response['error']}")
                return {"type": "error", "output": "N√£o foi poss√≠vel gerar o c√≥digo de an√°lise."}

            code_to_execute = self._extract_python_code(llm_response.get("content", ""))

            if not code_to_execute:
                self.logger.warning("Nenhum c√≥digo Python foi gerado pelo LLM.")
                return {"type": "text", "output": "N√£o consegui gerar um script para responder √† sua pergunta."}

            # üöÄ QUICK WIN 1: Validar e corrigir Top N automaticamente
            # user_query j√° foi definido no in√≠cio da fun√ß√£o
            code_to_execute = self._validate_top_n(code_to_execute, user_query)

            # üîß SELF-HEALING: Valida√ß√£o e auto-corre√ß√£o PR√â-execu√ß√£o
            if self.self_healing:
                try:
                    # Obter schema de colunas dispon√≠veis
                    schema_columns = list(self.column_descriptions.keys())

                    healing_context = {
                        'query': user_query,
                        'schema_columns': schema_columns
                    }

                    # Validar e tentar curar
                    is_valid, healed_code, feedback = self.self_healing.validate_and_heal(
                        code_to_execute,
                        healing_context
                    )

                    if feedback:
                        for msg in feedback:
                            self.logger.info(f"üîß Self-Healing: {msg}")

                    if healed_code != code_to_execute:
                        self.logger.info("‚úÖ C√≥digo auto-corrigido pelo SelfHealingSystem")
                        code_to_execute = healed_code

                    if not is_valid:
                        self.logger.warning("‚ö†Ô∏è Self-Healing detectou problemas que podem causar erro")

                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro no Self-Healing pr√©-execu√ß√£o: {e}")

            # ‚úÖ FASE 1: Validar c√≥digo antes de executar
            validation_result = self.code_validator.validate(code_to_execute, user_query)

            if not validation_result['valid']:
                self.logger.warning(f"‚ö†Ô∏è C√≥digo com problemas: {validation_result['errors']}")

                # Tentar corre√ß√£o autom√°tica
                fix_result = self.code_validator.auto_fix(validation_result, user_query)

                if fix_result['fixed']:
                    self.logger.info(f"‚úÖ C√≥digo corrigido automaticamente: {fix_result['fixes_applied']}")
                    code_to_execute = fix_result['code']
                else:
                    self.logger.warning(f"‚ö†Ô∏è Corre√ß√£o autom√°tica falhou. Erros restantes: {fix_result.get('remaining_errors', [])}")
                    # Continuar mesmo assim, mas com log

            # Valida√ß√µes adicionais com warnings (n√£o bloqueiam execu√ß√£o)
            if validation_result.get('warnings'):
                self.logger.info(f"‚ÑπÔ∏è Avisos: {validation_result['warnings']}")

            if validation_result.get('suggestions'):
                self.logger.debug(f"üí° Sugest√µes: {validation_result['suggestions']}")

            self.code_cache[cache_key] = code_to_execute

        self.logger.info(f"\nC√≥digo a ser executado:\n---\n{code_to_execute}\n---")

        try:
            # ‚ö†Ô∏è IMPORTANTE: Reutilizar a fun√ß√£o load_data() definida em _execute_generated_code
            # que j√° usa Dask e l√™ TODOS os arquivos Parquet (*.parquet)

            local_scope = {
                "pd": pd,
                "px": px,
                "result": None,
                "df_raw_data": pd.DataFrame(raw_data) if raw_data else None,
                # load_data ser√° injetado em _execute_generated_code
            }
            
            px.defaults.template = "plotly_white"

            start_code_execution = time.time()
            result = self._execute_generated_code(code_to_execute, local_scope)
            end_code_execution = time.time()
            self.logger.info(f"Tempo de execu√ß√£o do c√≥digo: {end_code_execution - start_code_execution:.4f} segundos")

            # ‚ö†Ô∏è VALIDA√á√ÉO CR√çTICA: Verificar se resultado √© Dask n√£o computado
            if hasattr(result, '_name') and 'dask' in str(type(result)).lower():
                self.logger.error(f"‚ùå ERRO: C√≥digo retornou Dask object n√£o computado: {type(result)}")
                self.logger.error(f"   O c√≥digo gerado deve chamar .compute() antes de retornar o resultado!")
                return {
                    "type": "error",
                    "output": "Erro interno: O c√≥digo gerou um resultado Dask n√£o computado. Tentando novamente..."
                }

            # An√°lise do tipo de resultado
            if isinstance(result, pd.DataFrame):
                self.logger.info(f"Resultado: DataFrame com {len(result)} linhas.")
                # üöÄ QUICK WIN 2: Registrar query bem-sucedida
                self._log_successful_query(user_query, code_to_execute, len(result))
                return {"type": "dataframe", "output": result}
            elif isinstance(result, pd.Series):
                self.logger.info(f"Resultado: Series com {len(result)} elementos.")
                # Converter Series para DataFrame para consist√™ncia
                result_df = result.reset_index()
                self._log_successful_query(user_query, code_to_execute, len(result_df))
                return {"type": "dataframe", "output": result_df}
            elif isinstance(result, list) and len(result) > 0 and 'plotly' in str(type(result[0])):
                # ‚úÖ CORRE√á√ÉO: Lista de Figures Plotly (m√∫ltiplos gr√°ficos)
                self.logger.info(f"Resultado: {len(result)} gr√°ficos Plotly.")

                # Aplicar tema escuro a cada Figure
                figures_json = []
                for i, fig in enumerate(result):
                    if 'plotly' in str(type(fig)):
                        # ‚ú® APLICAR TEMA ESCURO CHATGPT
                        fig.update_layout(
                            plot_bgcolor='#2a2b32',
                            paper_bgcolor='#2a2b32',
                            font=dict(color='#ececf1', family='sans-serif'),
                            title=dict(font=dict(color='#ececf1', size=18)),
                            xaxis=dict(
                                gridcolor='#444654',
                                tickfont=dict(color='#ececf1'),
                                title=dict(font=dict(color='#ececf1'))
                            ),
                            yaxis=dict(
                                gridcolor='#444654',
                                tickfont=dict(color='#ececf1'),
                                title=dict(font=dict(color='#ececf1'))
                            ),
                            margin=dict(l=60, r=40, t=40, b=80),
                            hoverlabel=dict(
                                bgcolor='#2a2b32',
                                bordercolor='#10a37f',
                                font=dict(color='#ececf1')
                            ),
                            legend=dict(
                                font=dict(color='#ececf1'),
                                bgcolor='rgba(42, 43, 50, 0.8)'
                            )
                        )
                        figures_json.append(pio.to_json(fig))
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Item {i} na lista n√£o √© uma Figure Plotly: {type(fig)}")

                # üöÄ Registrar query bem-sucedida (m√∫ltiplos gr√°ficos)
                self._log_successful_query(user_query, code_to_execute, len(figures_json))
                return {"type": "multiple_charts", "output": figures_json}
            elif 'plotly' in str(type(result)):
                self.logger.info(f"Resultado: Gr√°fico Plotly.")

                # ‚ú® APLICAR TEMA ESCURO CHATGPT (20/10/2025)
                result.update_layout(
                    plot_bgcolor='#2a2b32',
                    paper_bgcolor='#2a2b32',
                    font=dict(color='#ececf1', family='sans-serif'),
                    title=dict(font=dict(color='#ececf1', size=18)),
                    xaxis=dict(
                        gridcolor='#444654',
                        tickfont=dict(color='#ececf1'),
                        title=dict(font=dict(color='#ececf1'))
                    ),
                    yaxis=dict(
                        gridcolor='#444654',
                        tickfont=dict(color='#ececf1'),
                        title=dict(font=dict(color='#ececf1'))
                    ),
                    margin=dict(l=60, r=40, t=40, b=80),
                    hoverlabel=dict(
                        bgcolor='#2a2b32',
                        bordercolor='#10a37f',
                        font=dict(color='#ececf1')
                    ),
                    legend=dict(
                        font=dict(color='#ececf1'),
                        bgcolor='rgba(42, 43, 50, 0.8)'
                    )
                )

                # üöÄ QUICK WIN 2: Registrar query bem-sucedida (gr√°fico)
                self._log_successful_query(user_query, code_to_execute, 1)
                return {"type": "chart", "output": pio.to_json(result)}
            else:
                self.logger.info(f"Resultado: Texto.")
                return {"type": "text", "output": str(result)}
        
        except TimeoutError as e:
            self.logger.error("A execu√ß√£o do c√≥digo excedeu o tempo limite.")
            # üöÄ QUICK WIN 3: Registrar erro
            self._log_error(user_query, code_to_execute, "timeout", str(e))
            return {"type": "error", "output": "A an√°lise demorou muito e foi interrompida."}
        except Exception as e:
            error_msg = str(e)
            error_type = type(e).__name__

            # üîß SELF-HEALING: Tentar corrigir erro automaticamente
            if self.self_healing and not hasattr(self, '_healing_retry_count'):
                try:
                    self._healing_retry_count = 0

                    self.logger.info(f"üîß Self-Healing: Tentando corrigir {error_type}...")

                    # Obter schema de colunas
                    schema_columns = list(self.column_descriptions.keys())
                    healing_context = {
                        'query': user_query,
                        'schema_columns': schema_columns
                    }

                    # Tentar curar (m√°ximo 2 retries)
                    success, corrected_code, explanation = self.self_healing.heal_after_error(
                        code_to_execute,
                        e,
                        healing_context,
                        max_retries=2
                    )

                    if success and corrected_code != code_to_execute:
                        self.logger.info(f"‚úÖ Self-Healing: {explanation}")
                        self.logger.info(f"üîÑ Tentando novamente com c√≥digo corrigido...")

                        # Limpar cache e atualizar com c√≥digo corrigido
                        if cache_key in self.code_cache:
                            del self.code_cache[cache_key]

                        self.code_cache[cache_key] = corrected_code

                        # Retry com c√≥digo corrigido (m√°ximo 1 vez)
                        if self._healing_retry_count < 1:
                            self._healing_retry_count += 1
                            try:
                                # Re-executar com c√≥digo corrigido
                                local_scope = {
                                    "pd": pd,
                                    "px": px,
                                    "result": None,
                                    "df_raw_data": pd.DataFrame(raw_data) if raw_data else None,
                                }
                                result = self._execute_generated_code(corrected_code, local_scope)

                                # Sucesso! Retornar resultado
                                self.logger.info("‚úÖ Self-Healing: C√≥digo corrigido executado com sucesso!")
                                delattr(self, '_healing_retry_count')

                                # Processar resultado normalmente
                                if isinstance(result, pd.DataFrame):
                                    self._log_successful_query(user_query, corrected_code, len(result))
                                    return {"type": "dataframe", "output": result}
                                elif isinstance(result, pd.Series):
                                    result_df = result.reset_index()
                                    self._log_successful_query(user_query, corrected_code, len(result_df))
                                    return {"type": "dataframe", "output": result_df}
                                elif 'plotly' in str(type(result)):
                                    self._log_successful_query(user_query, corrected_code, 1)
                                    return {"type": "chart", "output": pio.to_json(result)}
                                else:
                                    return {"type": "text", "output": str(result)}

                            except Exception as retry_error:
                                self.logger.warning(f"‚ö†Ô∏è Self-Healing retry falhou: {retry_error}")
                                delattr(self, '_healing_retry_count')
                        else:
                            delattr(self, '_healing_retry_count')

                    else:
                        self.logger.warning(f"‚ö†Ô∏è Self-Healing n√£o conseguiu corrigir automaticamente")

                except Exception as healing_error:
                    self.logger.warning(f"‚ö†Ô∏è Erro no Self-Healing p√≥s-erro: {healing_error}")
                    if hasattr(self, '_healing_retry_count'):
                        delattr(self, '_healing_retry_count')

            # üîÑ AUTO-RECOVERY: Detectar erros comuns e limpar cache (fallback)
            should_retry = False

            if "'DataFrame' object has no attribute 'compute'" in error_msg or \
               "'Series' object has no attribute 'compute'" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo com .compute() inv√°lido")

            elif "boolean value of NA is ambiguous" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo sem tratamento de NA")

            elif "Invalid comparison between dtype=" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo sem convers√£o de tipos")

            elif "'Series' object has no attribute 'sort_values'" in error_msg or \
                 "AttributeError: 'Series'" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo com erro em Series (falta .reset_index()?)")

            elif "UnboundLocalError" in error_type or "cannot access local variable" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado UnboundLocalError - poss√≠vel conflito de escopo")

            if should_retry:

                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo com .compute() inv√°lido em pandas object")
                self.logger.info(f"üîÑ Limpando cache e tentando novamente com prompt atualizado...")

                # Limpar apenas o cache desta query espec√≠fica
                if cache_key in self.code_cache:
                    del self.code_cache[cache_key]
                    self.logger.info(f"‚úÖ Cache da query removido: {cache_key[:50]}...")

                # Tentar novamente (recursivo) - APENAS UMA VEZ
                if not hasattr(self, '_retry_flag'):
                    self._retry_flag = True
                    try:
                        result = self.generate_and_execute_code(user_query, raw_data, **kwargs)
                        return result
                    finally:
                        delattr(self, '_retry_flag')
                else:
                    self.logger.error(f"‚ùå Retry falhou. Erro persistente ap√≥s limpeza de cache.")

            self.logger.error(f"Erro ao executar o c√≥digo gerado: {e}", exc_info=True)
            # üöÄ QUICK WIN 3: Registrar erro
            self._log_error(user_query, code_to_execute, error_type, error_msg)
            return {"type": "error", "output": f"Ocorreu um erro ao executar a an√°lise: {error_msg}"}
    def _extract_python_code(self, text: str) -> str | None:
        """Extrai o bloco de c√≥digo Python da resposta do LLM."""
        match = re.search(r'```python\n(.*)```', text, re.DOTALL)
        return match.group(1).strip() if match else None

    # üöÄ QUICK WIN METHODS
    def _validate_top_n(self, code: str, user_query: str) -> str:
        """
        QUICK WIN 1: Valida se c√≥digo tem .head(N) quando usu√°rio pede 'top N'.
        Corrige automaticamente se necess√°rio.
        """
        query_lower = user_query.lower()

        # Verificar se usu√°rio pediu "top N"
        top_match = re.search(r'top\s+(\d+)', query_lower)

        # ‚úÖ N√ÉO adicionar .head() se o c√≥digo est√° gerando um gr√°fico Plotly
        # Gr√°ficos j√° devem ter o filtro aplicado antes do px.bar/px.pie/etc
        is_plotly_chart = any(func in code for func in ['px.bar(', 'px.pie(', 'px.line(', 'px.scatter(', 'px.histogram('])

        if top_match and '.head(' not in code and not is_plotly_chart:
            n = top_match.group(1)
            self.logger.warning(f"‚ö†Ô∏è Query pede top {n} mas c√≥digo n√£o tem .head(). Corrigindo automaticamente...")

            # Tentar adicionar .head(N) antes de .reset_index()
            if '.reset_index()' in code:
                code = code.replace('.reset_index()', f'.head({n}).reset_index()')
            # Ou antes do resultado final
            elif 'result = ' in code:
                # Encontrar a √∫ltima atribui√ß√£o a result
                lines = code.split('\n')
                for i in range(len(lines) - 1, -1, -1):
                    if lines[i].strip().startswith('result = '):
                        # Adicionar .head(N) se ainda n√£o existir
                        if '.head(' not in lines[i]:
                            lines[i] = lines[i].replace('result = ', f'result = ').rstrip()
                            if not lines[i].endswith(')'):
                                lines[i] = f"{lines[i]}.head({n})"
                        break
                code = '\n'.join(lines)

            self.logger.info(f"‚úÖ C√≥digo corrigido automaticamente com .head({n})")
        elif is_plotly_chart:
            self.logger.info(f"‚ÑπÔ∏è C√≥digo gera gr√°fico Plotly - n√£o adicionando .head() autom√°tico")

        return code

    def _log_successful_query(self, user_query: str, code: str, result_rows: int):
        """
        QUICK WIN 2: Registra queries bem-sucedidas para an√°lise futura.
        + RAG: Coleta autom√°tica para banco de exemplos.
        """
        from datetime import datetime

        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'query': user_query,
            'code': code,
            'rows': result_rows,
            'success': True
        }

        # Salvar em arquivo di√°rio
        date_str = datetime.now().strftime('%Y%m%d')
        log_file = os.path.join(self.logs_dir, f'successful_queries_{date_str}.jsonl')

        try:
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
            self.logger.debug(f"‚úÖ Query registrada em {log_file}")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao registrar query: {e}")

        # üéØ RAG: Coletar query bem-sucedida para treinamento cont√≠nuo
        if self.rag_enabled and self.example_collector:
            try:
                # Detectar inten√ß√£o baseado no c√≥digo gerado
                intent = "python_analysis"
                if 'plotly' in code or 'px.' in code:
                    intent = "visualization"
                elif '.groupby' in code:
                    intent = "aggregation"
                elif '.nlargest' in code or '.nsmallest' in code:
                    intent = "ranking"

                # Coletar exemplo
                self.example_collector.collect_successful_query(
                    user_query=user_query,
                    code_generated=code,
                    result_rows=result_rows,
                    intent=intent
                )
                self.logger.debug(f"üìö RAG: Query coletada para banco de exemplos")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro ao coletar query no RAG: {e}")

    def _log_error(self, user_query: str, code: str, error_type: str, error_message: str):
        """
        QUICK WIN 3: Registra erros por tipo para an√°lise de padr√µes.
        """
        from datetime import datetime

        # Incrementar contador
        self.error_counts[error_type] += 1

        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'query': user_query,
            'code': code,
            'error_type': error_type,
            'error_message': str(error_message),
            'success': False
        }

        # Salvar em arquivo di√°rio
        date_str = datetime.now().strftime('%Y%m%d')
        log_file = os.path.join(self.logs_dir, f'error_log_{date_str}.jsonl')

        try:
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')

            # Tamb√©m salvar contador consolidado
            counter_file = os.path.join(self.logs_dir, f'error_counts_{date_str}.json')
            with open(counter_file, 'w', encoding='utf-8') as f:
                json.dump(dict(self.error_counts), f, indent=2, ensure_ascii=False)

            self.logger.debug(f"‚ö†Ô∏è Erro registrado: {error_type} (total: {self.error_counts[error_type]})")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao registrar erro: {e}")

    def _clean_old_cache(self, max_age_hours=2):
        """Limpa cache antigo automaticamente (padr√£o: 2 horas)"""
        import os
        import time
        from pathlib import Path

        try:
            cache_dirs = [
                Path('data/cache'),
                Path('data/cache_agent_graph')
            ]

            now = time.time()
            max_age = max_age_hours * 60 * 60  # Converte horas para segundos
            removed_count = 0

            for cache_dir in cache_dirs:
                if not cache_dir.exists():
                    continue

                for cache_file in cache_dir.glob('*'):
                    if cache_file.is_file():
                        file_age = now - cache_file.stat().st_mtime
                        if file_age > max_age:
                            cache_file.unlink()
                            removed_count += 1

            if removed_count > 0:
                self.logger.info(f"üßπ Cache limpo: {removed_count} arquivos removidos (> 24h)")

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao limpar cache: {e}")

    def _check_and_invalidate_cache_if_prompt_changed(self):
        """
        üîÑ VERSIONING DE CACHE: Invalida cache se o prompt mudou

        Calcula hash do prompt atual e compara com o hash salvo.
        Se diferente, limpa o cache para for√ßar regenera√ß√£o com novo prompt.
        """
        import hashlib
        from pathlib import Path
        import json

        try:
            # ‚úÖ INCREMENTAR VERS√ÉO para for√ßar regenera√ß√£o ap√≥s corre√ß√£o de schema
            prompt_components = {
                'columns': list(self.column_descriptions.keys()),
                'descriptions': list(self.column_descriptions.values()),
                # Adicionar outros componentes que afetam o prompt
                'version': '5.0_context7_prompt_engineering_few_shot_learning_20251027'  # ‚úÖ NOVA VERS√ÉO
            }

            prompt_str = json.dumps(prompt_components, sort_keys=True)
            current_hash = hashlib.md5(prompt_str.encode()).hexdigest()

            # Arquivo para armazenar hash do prompt
            version_file = Path('data/cache/.prompt_version')

            # Verificar se h√° vers√£o anterior
            if version_file.exists():
                try:
                    with open(version_file, 'r') as f:
                        saved_hash = f.read().strip()

                    if saved_hash != current_hash:
                        # PROMPT MUDOU! Limpar cache
                        self.logger.warning(f"‚ö†Ô∏è  PROMPT MUDOU! Limpando cache para for√ßar regenera√ß√£o...")
                        self.logger.info(f"   Hash anterior: {saved_hash}")
                        self.logger.info(f"   Hash novo: {current_hash}")

                        # Limpar todos os caches
                        cache_dirs = [
                            Path('data/cache'),
                            Path('data/cache_agent_graph')
                        ]

                        removed_count = 0
                        for cache_dir in cache_dirs:
                            if cache_dir.exists():
                                for cache_file in cache_dir.glob('*'):
                                    if cache_file.is_file() and cache_file.name != '.prompt_version':
                                        cache_file.unlink()
                                        removed_count += 1

                        self.logger.info(f"‚úÖ Cache invalidado: {removed_count} arquivos removidos")

                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao ler vers√£o do cache: {e}")

            # Salvar hash atual
            version_file.parent.mkdir(parents=True, exist_ok=True)
            with open(version_file, 'w') as f:
                f.write(current_hash)

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao verificar vers√£o do cache: {e}")

    def detect_broad_query(self, query: str) -> Tuple[bool, str]:
        """
        Detecta se uma query √© muito ampla e pode causar timeout.
        """
        query_lower = query.lower()

        # Crit√©rios de queries amplas
        broad_keywords = ["todos", "todas", "geral", "completo", "tudo", "vendas", "estoque"]
        specific_keywords = ["top", "limite", "une", "segmento", "categoria", "grupo", "fabricante", "pre√ßo", "<", ">", "="]

        has_broad = any(kw in query_lower for kw in broad_keywords)
        has_specific = any(kw in query_lower for kw in specific_keywords)
        has_number = any(char.isdigit() for char in query_lower)

        if has_broad and not has_specific and not has_number:
            return True, "Keyword ampla detectada sem filtros espec√≠ficos ou limites num√©ricos"

        if "ranking" in query_lower and not has_specific and not has_number:
            return True, "Ranking sem limite ou filtro espec√≠fico"

        # Check for queries that are implicitly broad
        if not has_specific and not has_number:
            return True, "Query sem filtros espec√≠ficos ou limites num√©ricos"

        return False, "Query espec√≠fica OK"

    def get_educational_message(self, query: str, reason: str) -> str:
        """
        Gera uma mensagem educativa para queries amplas.
        """
        return f"""
üîç Query Muito Ampla Detectada

**Sua Pergunta:** "{query}"

**Motivo:** {reason}

**Por que isso acontece?**
- Processar milh√µes de registros pode levar muito tempo e causar erros.

**‚úÖ Como fazer queries eficientes:**

**Exemplos de queries v√°lidas:**
   1. Top 10 produtos mais vendidos da UNE NIG
   2. Produtos do segmento ARMARINHO com estoque < 10
   3. Vendas da UNE BEL nos √∫ltimos 30 dias

**üí° Dicas:**
1. Especifique uma UNE (loja)
2. Use limites (Top 10, Top 20)
3. Aplique filtros (segmento, pre√ßo, etc.)
4. Defina um per√≠odo de tempo

**üí° Sugest√£o:** Tente 'Top 10 produtos mais vendidos da UNE [c√≥digo]'
"""
