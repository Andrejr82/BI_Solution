import logging
import os
import json
import re
import pandas as pd
import dask.dataframe as dd  # Dask para lazy loading
import time

# ‚úÖ NOVO: Import Polars (condicional)
try:
    import polars as pl
    POLARS_AVAILABLE = True
except ImportError:
    pl = None
    POLARS_AVAILABLE = False
import plotly.express as px
from typing import List, Dict, Any, Tuple # Import necessary types
import threading
from queue import Queue
import pickle
import faiss
import numpy as np

# Lazy import - s√≥ carrega quando necess√°rio
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    logging.getLogger(__name__).warning("sentence-transformers not available - RAG features disabled")
    SentenceTransformer = None
    SENTENCE_TRANSFORMERS_AVAILABLE = False

import io
import sys
import plotly.io as pio
import uuid
from core.utils.json_utils import _clean_json_values # Import the cleaning function

from core.llm_base import BaseLLMAdapter
from core.learning.pattern_matcher import PatternMatcher
from core.validation.code_validator import CodeValidator
from core.learning.dynamic_prompt import DynamicPrompt
from core.learning.self_healing_system import SelfHealingSystem
from core.config.column_mapping import normalize_column_name, validate_columns, get_essential_columns
from core.utils.column_validator import (
    validate_query_code,
    extract_columns_from_query,
    ColumnValidationError
)
from core.rag.query_retriever import QueryRetriever
from core.rag.example_collector import ExampleCollector
from core.agents.polars_load_data import create_optimized_load_data

class CodeGenAgent:
    """
    Agente especializado em gerar e executar c√≥digo Python para an√°lise de dados.
    """
    def __init__(self, llm_adapter: BaseLLMAdapter, data_adapter: any = None):
        """
        Inicializa o agente com o adaptador LLM e opcionalmente o adaptador de dados.

        Args:
            llm_adapter: Adaptador LLM para gera√ß√£o de c√≥digo
            data_adapter: (Opcional) Adaptador de dados para inje√ß√£o de load_data()
                         Se None, load_data() usar√° path padr√£o do Parquet
        """
        self.logger = logging.getLogger(__name__)
        self.llm = llm_adapter
        self.data_adapter = data_adapter  # Pode ser None (fallback para path padr√£o)
        self.code_cache = {}
        self.catalog_data = {}

        # Carregar o cat√°logo de dados para fornecer contexto ao LLM
        try:
            catalog_path = os.path.join(os.getcwd(), "data", "catalog_focused.json")
            if os.path.exists(catalog_path):
                with open(catalog_path, 'r', encoding='utf-8') as f:
                    self.catalog_data = json.load(f)
                self.logger.info("‚úÖ Cat√°logo de dados (catalog_focused.json) carregado com sucesso.")
            else:
                self.logger.warning("‚ö†Ô∏è  Arquivo de cat√°logo 'data/catalog_focused.json' n√£o encontrado. O agente pode ter dificuldade em interpretar entidades.")
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao carregar o arquivo de cat√°logo: {e}")

        # ‚úÖ CORRE√á√ÉO v2.2: Colunas reais confirmadas em 04/11/2024
        self.column_descriptions = {
            # Identifica√ß√£o
            "codigo": "C√≥digo √∫nico do produto",
            "nome_produto": "Nome/descri√ß√£o do produto",
            "ean": "C√≥digo de barras",

            # Hierarquia
            "nomesegmento": "Segmento - Ex: TECIDOS, PAPELARIA, AVIAMENTOS",
            "NOMECATEGORIA": "Categoria do produto",
            "nomegrupo": "Grupo do produto",
            "NOMESUBGRUPO": "Subgrupo do produto",
            "NOMEFABRICANTE": "Fabricante do produto",

            # Vendas
            "venda_30_d": "Vendas √∫ltimos 30 dias (em unidades)",
            "mes_01": "Vendas do m√™s atual/mais recente",
            "mes_02": "Vendas de 2 meses atr√°s",
            "mes_03": "Vendas de 3 meses atr√°s",
            "mes_04": "Vendas de 4 meses atr√°s",
            "mes_05": "Vendas de 5 meses atr√°s",
            "mes_06": "Vendas de 6 meses atr√°s",
            "mes_07": "Vendas de 7 meses atr√°s",
            "mes_08": "Vendas de 8 meses atr√°s",
            "mes_09": "Vendas de 9 meses atr√°s",
            "mes_10": "Vendas de 10 meses atr√°s",
            "mes_11": "Vendas de 11 meses atr√°s",
            "mes_12": "Vendas de 12 meses atr√°s (mais antigo)",

            # Estoque
            "estoque_atual": "Estoque total da UNE",
            "estoque_lv": "Estoque Linha Verde (√°rea venda)",
            "estoque_gondola_lv": "Estoque na g√¥ndola LV",
            "estoque_ilha_lv": "Estoque em ilha LV",
            "estoque_cd": "Estoque no Centro Distribui√ß√£o",

            # UNE
            "une": "ID num√©rico da UNE - Ex: 1, 2586, 2720",
            "une_nome": "Nome da UNE - Ex: SCR, MAD, BAR",

            # An√°lise ABC
            "abc_une_30_dd": "ABC UNE √∫ltimos 30 dias",
            "abc_cacula_90_dd": "ABC Ca√ßula √∫ltimos 90 dias",
            "abc_une_mes_01": "ABC UNE m√™s 1",
            "abc_une_mes_02": "ABC UNE m√™s 2",
            "abc_une_mes_03": "ABC UNE m√™s 3",
            "abc_une_mes_04": "ABC UNE m√™s 4",

            # Reposi√ß√£o/Log√≠stica
            "media_considerada_lv": "M√©dia considerada reposi√ß√£o LV",
            "ponto_pedido_lv": "Ponto de pedido LV",
            "exposicao_minima": "Exposi√ß√£o m√≠nima",
            "exposicao_minima_une": "Exposi√ß√£o m√≠nima UNE",
            "exposicao_maxima_une": "Exposi√ß√£o m√°xima UNE",
            "leadtime_lv": "Lead time para LV",

            # Outros
            "preco_38_percent": "Pre√ßo venda 38% margem",
            "tipo": "Tipo de produto",
            "embalagem": "Tipo embalagem",
            "promocional": "Produto promocional (True/False)",
            "foralinha": "Produto fora de linha (True/False)"
        }

        # ‚úÖ OTIMIZA√á√ÉO v2.2: Lazy loading do RAG system (economiza 1-3s no startup)
        # RAG ser√° carregado apenas quando realmente necess√°rio
        self._query_retriever = None
        self._example_collector = None
        self._rag_enabled = None  # None = n√£o inicializado, True = OK, False = erro
        self.logger.info("‚úÖ RAG system configurado para lazy loading")

        # Inicializar pattern_matcher and code_validator
        from collections import defaultdict
        try:
            self.pattern_matcher = PatternMatcher()
            self.logger.info("‚úÖ PatternMatcher inicializado (Few-Shot Learning ativo)")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è PatternMatcher n√£o dispon√≠vel: {e}")
            self.pattern_matcher = None

        self.code_validator = CodeValidator()
        self.error_counts = defaultdict(int)
        self.logs_dir = os.path.join(os.getcwd(), "data", "learning")
        os.makedirs(self.logs_dir, exist_ok=True)

        # Inicializar DynamicPrompt (Pilar 4)
        try:
            self.dynamic_prompt = DynamicPrompt()
            self.logger.info("‚úÖ DynamicPrompt inicializado (Pilar 4 ativo)")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è DynamicPrompt n√£o dispon√≠vel: {e}")
            self.dynamic_prompt = None

        # Inicializar Self-Healing System (Auto-corre√ß√£o)
        try:
            self.self_healing = SelfHealingSystem(
                llm_adapter=llm_adapter,
                schema_validator=True
            )
            self.logger.info("‚úÖ SelfHealingSystem inicializado (Auto-corre√ß√£o ativa)")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è SelfHealingSystem n√£o dispon√≠vel: {e}")
            self.self_healing = None

        # ‚ö° SOLU√á√ÉO ZERO-CLICK: Cache √© gerenciado 100% automaticamente
        # Usu√°rio N√ÉO precisa clicar em nada, deslogar ou recarregar p√°gina

        # 1. Limpar cache antigo (5 minutos - MUITO curto para for√ßar regenera√ß√£o r√°pida)
        self._clean_old_cache(max_age_hours=0.08)  # ~5 minutos

        # 2. Invalidar cache quando prompt/c√≥digo muda (detecta corre√ß√µes automaticamente)
        self._check_and_invalidate_cache_if_prompt_changed()

        self.logger.info("CodeGenAgent inicializado.")

    def _ensure_rag_loaded(self):
        """
        ‚úÖ OTIMIZA√á√ÉO v2.2: Carrega RAG system sob demanda (lazy loading)
        Apenas inicializa quando realmente necess√°rio, economizando 1-3s no startup
        """
        if self._rag_enabled is None:  # Ainda n√£o foi inicializado
            try:
                self.logger.info("üîÑ Carregando RAG system sob demanda...")
                self._query_retriever = QueryRetriever()
                self._example_collector = ExampleCollector()
                self._rag_enabled = True
                self.logger.info("‚úÖ RAG system carregado com sucesso")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è RAG n√£o dispon√≠vel: {e}. Continuando sem RAG.")
                self._query_retriever = None
                self._example_collector = None
                self._rag_enabled = False

    @property
    def query_retriever(self):
        """Property que carrega RAG sob demanda quando acessado"""
        self._ensure_rag_loaded()
        return self._query_retriever

    @property
    def example_collector(self):
        """Property que carrega RAG sob demanda quando acessado"""
        self._ensure_rag_loaded()
        return self._example_collector

    @property
    def rag_enabled(self):
        """Property que verifica se RAG est√° dispon√≠vel (carrega se necess√°rio)"""
        self._ensure_rag_loaded()
        return self._rag_enabled

    def _execute_generated_code(self, code: str, local_scope: Dict[str, Any]):
        q = Queue()
        output_capture = io.StringIO()
        original_stdout = sys.stdout
        original_stderr = sys.stderr

        # Fun√ß√£o helper para ser injetada no escopo de execu√ß√£o
        def load_data(filters: Dict[str, Any] = None):
            """
            üöÄ OTIMIZADO: Carrega o dataframe usando PolarsDaskAdapter (h√≠brido Polars/Dask).

            Args:
                filters: Dicion√°rio opcional de filtros para aplicar ANTES de carregar dados.
                        Ex: {'UNE': 'MAD'}, {'NOMESEGMENTO': 'TECIDOS'}, {'PRODUTO': 12345}
                        Filtros reduzem drasticamente mem√≥ria e tempo de carregamento.

            Returns:
                pandas DataFrame (j√° filtrado se filters fornecido)

            IMPORTANTE:
            - COM filtros: Usa PolarsDaskAdapter (predicate pushdown, 5-10x mais r√°pido)
            - SEM filtros: Carrega apenas 10k linhas (prote√ß√£o contra OOM)
            """
            import pandas as pd
            import os

            if self.data_adapter and filters:
                # ‚úÖ USAR ADAPTER COM FILTROS (Polars/Dask com predicate pushdown)
                import time
                start_time = time.time()

                self.logger.info("=" * 80)
                self.logger.info("üîç PLANO A - LOAD_DATA() COM FILTROS")
                self.logger.info(f"   Filtros aplicados: {filters}")
                self.logger.info("   Adapter: PolarsDaskAdapter (predicate pushdown)")

                try:
                    # Delegar para adapter (usa Polars ou Dask automaticamente)
                    result_list = self.data_adapter.execute_query(filters)
                    elapsed = time.time() - start_time

                    self.logger.info(f"‚úÖ SUCESSO - {len(result_list):,} registros carregados em {elapsed:.2f}s")
                    self.logger.info(f"   Performance: {len(result_list)/elapsed:.0f} registros/segundo")
                    self.logger.info("=" * 80)

                    return pd.DataFrame(result_list)

                except Exception as e:
                    elapsed = time.time() - start_time
                    self.logger.error("=" * 80)
                    self.logger.error(f"‚ùå ERRO ao carregar com filtros (ap√≥s {elapsed:.2f}s)")
                    self.logger.error(f"   Tipo: {type(e).__name__}")
                    self.logger.error(f"   Mensagem: {str(e)}")
                    self.logger.error("=" * 80)
                    # Fallback para modo sem filtros (limitado)
                    self.logger.warning("‚ö†Ô∏è  Caindo para modo sem filtros (limitado a 10k linhas)")
                    filters = None  # Trigger fallback abaixo

            if not filters:
                # ‚ö†Ô∏è SEM FILTROS - Modo de prote√ß√£o (limitar a 10k linhas)
                self.logger.warning("‚ö†Ô∏è  load_data() SEM filtros - LIMITANDO a 10.000 linhas para evitar OOM")
                self.logger.warning("   RECOMENDA√á√ÉO: Passe filtros para carregar dados completos")
                self.logger.warning("   Exemplo: load_data(filters={'UNE': 'MAD', 'NOMESEGMENTO': 'TECIDOS'})")

                import dask.dataframe as dd

                # Definir parquet_path para uso no fallback
                parquet_path = None

                if self.data_adapter:
                    file_path = getattr(self.data_adapter, 'file_path', None)
                    if file_path:
                        parquet_path = file_path  # Salvar para fallback
                        ddf = dd.read_parquet(file_path, engine='pyarrow')
                    else:
                        raise AttributeError(f"Adapter {type(self.data_adapter).__name__} n√£o tem file_path")
                else:
                    # Fallback: carregar diretamente do Parquet
                    parquet_dir = os.path.join(os.getcwd(), "data", "parquet")
                    parquet_pattern = os.path.join(parquet_dir, "*.parquet")
                    if not os.path.exists(parquet_dir):
                        raise FileNotFoundError(f"Diret√≥rio Parquet n√£o encontrado em {parquet_dir}")
                    parquet_path = parquet_pattern  # Salvar para fallback
                    ddf = dd.read_parquet(parquet_pattern, engine='pyarrow')

                # Normalizar colunas
                column_mapping = {
                    'une': 'UNE_ID',
                    'nomesegmento': 'NOMESEGMENTO',
                    'codigo': 'PRODUTO',
                    'nome_produto': 'NOME',
                    'une_nome': 'UNE',
                    'nomegrupo': 'NOMEGRUPO',
                    'ean': 'EAN',
                    'preco_38_percent': 'LIQUIDO_38',
                    'venda_30_d': 'VENDA_30DD',
                    'estoque_atual': 'ESTOQUE_UNE',
                    'embalagem': 'EMBALAGEM',
                    'tipo': 'TIPO',
                    'NOMECATEGORIA': 'NOMECATEGORIA',
                    'NOMESUBGRUPO': 'NOMESUBGRUPO',
                    'NOMEFABRICANTE': 'NOMEFABRICANTE'
                }

                rename_dict = {k: v for k, v in column_mapping.items() if k in ddf.columns}
                ddf = ddf.rename(columns=rename_dict)

                # Converter tipos
                if 'ESTOQUE_UNE' in ddf.columns:
                    ddf['ESTOQUE_UNE'] = dd.to_numeric(ddf['ESTOQUE_UNE'], errors='coerce').fillna(0)

                for i in range(1, 13):
                    col_name = f'mes_{i:02d}'
                    if col_name in ddf.columns:
                        ddf[col_name] = dd.to_numeric(ddf[col_name], errors='coerce').fillna(0)

                # ‚ö†Ô∏è LIMITAR A 10K LINHAS (prote√ß√£o contra OOM)
                self.logger.info(f"‚ö° load_data(): Limitando a 10.000 linhas (sem filtros)")
                import time as time_module
                start_compute = time_module.time()

                try:
                    # Computar apenas primeiras 10k linhas
                    df_pandas = ddf.head(10000, npartitions=-1)
                except Exception as compute_error:
                    self.logger.error(f"‚ùå Erro ao computar Dask: {compute_error}")
                    self.logger.warning("üîÑ Tentando fallback: carregar direto do Parquet com pandas (modo otimizado)")

                    # Estrat√©gia de fallback melhorada
                    try:
                        if parquet_path:
                            # Estrat√©gia 1: Usar pandas com limite de linhas e colunas essenciais
                            self.logger.info("   Tentando carregar com pandas (apenas colunas essenciais)...")

                            # Resolver wildcard pattern
                            import glob
                            if '*' in parquet_path:
                                parquet_files = glob.glob(parquet_path)
                                if not parquet_files:
                                    raise FileNotFoundError(f"Nenhum arquivo encontrado em: {parquet_path}")
                                parquet_path = parquet_files[0]  # Usar primeiro arquivo
                                self.logger.info(f"üìÅ Usando arquivo: {os.path.basename(parquet_path)}")

                            # Colunas essenciais para an√°lises b√°sicas (NOMES CORRETOS DO PARQUET)
                            essential_cols = get_essential_columns()
                            self.logger.info(f"   Carregando colunas essenciais: {essential_cols}")

                            df_pandas = pd.read_parquet(
                                parquet_path,
                                engine='pyarrow',
                                columns=essential_cols
                            ).head(10000)

                            self.logger.info(f"‚úÖ Fallback bem-sucedido: {len(df_pandas)} registros carregados com {len(df_pandas.columns)} colunas")
                        else:
                            raise FileNotFoundError(f"Parquet path n√£o dispon√≠vel: {parquet_path}")

                    except Exception as fallback_error:
                        self.logger.error(f"‚ùå Fallback otimizado falhou: {fallback_error}")

                        # Estrat√©gia 2: Carregar apenas 1000 linhas como √∫ltimo recurso
                        try:
                            self.logger.warning("   Tentando carregar apenas 1000 linhas como √∫ltimo recurso...")
                            df_pandas = ddf.head(1000, npartitions=-1)
                            self.logger.info(f"‚ö†Ô∏è  Carregado dataset MUITO reduzido: {len(df_pandas)} linhas")
                        except:
                            self.logger.error(f"‚ùå Todas as estrat√©gias de fallback falharam")
                            # Mensagem amig√°vel ao usu√°rio (sem stacktrace t√©cnico)
                            error_msg = (
                                "‚ùå **Erro ao Processar Consulta**\n\n"
                                "O sistema est√° com recursos limitados no momento.\n\n"
                                "**üí° Sugest√µes:**\n"
                                "- Tente uma consulta mais espec√≠fica (ex: filtre por UNE ou segmento)\n"
                                "- Divida sua an√°lise em partes menores\n"
                                "- Aguarde alguns segundos e tente novamente\n\n"
                                "**Exemplo de consulta espec√≠fica:**\n"
                                "`Top 10 produtos da UNE SCR do segmento TECIDOS`"
                            )
                            raise RuntimeError(error_msg)

                end_compute = time_module.time()
                self.logger.info(f"‚úÖ load_data(): {len(df_pandas)} registros carregados (LIMITADO) em {end_compute - start_compute:.2f}s")

                return df_pandas

        # ‚úÖ v2.2: Usar load_data otimizada com Polars
        try:
            # ‚úÖ FIX: Usar arquivo espec√≠fico ao inv√©s de wildcard
            parquet_path = os.path.join("data", "parquet", "admmat.parquet")

            # ‚úÖ CORRE√á√ÉO v2.2: Path expl√≠cito sem glob (Windows incompat√≠vel)
            if not os.path.exists(parquet_path):
                # Fallback 1: admmat_extended.parquet
                parquet_path = os.path.join("data", "parquet", "admmat_extended.parquet")

            if not os.path.exists(parquet_path):
                # Fallback 2: admmat_backup.parquet
                parquet_path = os.path.join("data", "parquet", "admmat_backup.parquet")

            if not os.path.exists(parquet_path):
                raise FileNotFoundError(f"Nenhum arquivo Parquet encontrado em data/parquet/")

            optimized_load_data = create_optimized_load_data(parquet_path, self.data_adapter)
            local_scope['load_data'] = optimized_load_data
            self.logger.info("‚úÖ Using optimized Polars load_data()")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao criar load_data otimizada: {e}. Usando vers√£o antiga.")
            local_scope['load_data'] = load_data  # Fallback para vers√£o antiga

        local_scope['dd'] = dd  # Adicionar Dask ao escopo para c√≥digo gerado (se necess√°rio)
        local_scope['time'] = __import__('time')  # Adicionar m√≥dulo time ao escopo para evitar UnboundLocalError
        local_scope['pl'] = pl  # ‚úÖ NOVO: Adicionar Polars ao escopo
        local_scope['pd'] = pd  # Adicionar Pandas para compatibilidade

        def worker():
            sys.stdout = output_capture
            sys.stderr = output_capture
            try:
                exec(code, local_scope)
                q.put(local_scope.get('result'))
            except KeyError as e:
                # ‚úÖ TRATAMENTO ESPEC√çFICO: Erro de coluna n√£o encontrada
                error_msg = str(e)

                # Detectar se √© erro de coluna
                if "nome_produto" in error_msg or "KeyError" in str(type(e).__name__):
                    self.logger.error(f"‚ùå Erro de coluna n√£o encontrada: {e}")

                    # Tentar extrair nome da coluna do erro
                    import re
                    col_match = re.search(r"['\"]([^'\"']+)['\"]", error_msg)
                    if col_match:
                        missing_col = col_match.group(1)
                        self.logger.error(f"   Coluna faltante: '{missing_col}'")

                    # Criar erro mais informativo
                    enhanced_error = ColumnValidationError(
                        missing_col if col_match else "desconhecida",
                        suggestions=[],
                        available_columns=[]
                    )
                    q.put(enhanced_error)
                else:
                    q.put(e)

            except Exception as e:
                # ‚úÖ TRATAMENTO GEN√âRICO: Capturar outros erros do Polars
                error_type = type(e).__name__

                # Detectar erros comuns do Polars
                if any(err in error_type for err in ["ColumnNotFoundError", "SchemaError", "ComputeError"]):
                    self.logger.error(f"‚ùå Erro do Polars: {error_type} - {e}")

                q.put(e)
            finally:
                sys.stdout = original_stdout
                sys.stderr = original_stderr

        thread = threading.Thread(target=worker)
        thread.start()
        thread.join(timeout=120.0)

        captured_output = output_capture.getvalue()
        if captured_output:
            self.logger.info(f"Sa√≠da do c√≥digo gerado:\n{captured_output}")

        if thread.is_alive():
            raise TimeoutError("A execu√ß√£o do c√≥digo gerado excedeu o tempo limite.")
        else:
            result = q.get()
            if isinstance(result, Exception):
                raise result
            return result

    def _normalize_query(self, query: str) -> str:
        """
        Normaliza query para melhorar cache hit rate.
        Remove stopwords e varia√ß√µes irrelevantes, mantendo sem√¢ntica.
        """
        query = query.lower().strip()

        # Stopwords comuns em portugu√™s que n√£o afetam a sem√¢ntica da query
        stopwords = [
            'qual', 'quais', 'mostre', 'me', 'gere', 'por favor', 'por gentileza',
            'poderia', 'pode', 'consegue', 'voc√™', 'o', 'a', 'os', 'as',
            'um', 'uma', 'uns', 'umas', 'de', 'da', 'do', 'das', 'dos'
        ]

        # Remover stopwords
        words = query.split()
        filtered_words = [w for w in words if w not in stopwords]
        query = ' '.join(filtered_words)

        # Normalizar varia√ß√µes comuns
        replacements = {
            'gr√°fico': 'graf',
            'gr√°ficos': 'graf',
            'grafico': 'graf',
            'graficos': 'graf',
            'ranking': 'rank',
            'rankings': 'rank',
            'top 5': 'top5',
            'top 10': 'top10',
            'top 20': 'top20',
            '√∫ltimos': 'ultimos',
            '√∫ltimo': 'ultimo',
            'an√°lise': 'analise',
            'an√°lises': 'analise',
        }

        for old, new in replacements.items():
            query = query.replace(old, new)

        # Remover espa√ßos extras
        query = ' '.join(query.split())

        return query

    def _detect_complex_query(self, query: str) -> bool:
        """
        Detecta se query requer racioc√≠nio multi-step (chain-of-thought).

        Baseado em: Context7 - OpenAI Prompt Engineering Best Practices
        """
        complex_keywords = [
            'an√°lise abc', 'distribui√ß√£o', 'sazonalidade', 'tend√™ncia',
            'comparar', 'compara√ß√£o', 'correla√ß√£o', 'previs√£o',
            'alertas', 'insights', 'padr√µes', 'anomalias'
        ]
        query_lower = query.lower()
        return any(kw in query_lower for kw in complex_keywords)

    def _build_structured_prompt(self, user_query: str, rag_examples: list = None) -> str:
        """
        Constr√≥i prompt estruturado, agora injetando contexto do cat√°logo.
        """
        # Se√ß√£o de Segmentos e Fabricantes gerada dinamicamente a partir do cat√°logo
        catalog_context = "## Entidades de Neg√≥cio (Segmentos e Fabricantes)\n\n"
        catalog_context += "Para interpretar a query, use o seguinte contexto:\n\n"

        if "nomesegmento" in self.catalog_data:
            catalog_context += f"### Segmentos (coluna 'nomesegmento')\n"
            catalog_context += f"- **Descri√ß√£o**: {self.catalog_data['nomesegmento']['description']}\n"
            catalog_context += f"- **Exemplos**: {', '.join(self.catalog_data['nomesegmento']['examples'][:5])}...\n\n"
        
        if "NOMEFABRICANTE" in self.catalog_data:
            catalog_context += f"### Fabricantes (coluna 'NOMEFABRICANTE')\n"
            catalog_context += f"- **Descri√ß√£o**: {self.catalog_data['NOMEFABRICANTE']['description']}\n"
            catalog_context += f"- **Exemplos**: {', '.join(self.catalog_data['NOMEFABRICANTE']['examples'][:5])}...\n\n"

        catalog_context += "**REGRA DE OURO PARA FILTROS:**\n"
        catalog_context += "1. Se o usu√°rio mencionar algo como 'tecidos', 'papelaria', 'artesanato', use a coluna `nomesegmento`.\n"
        catalog_context += "2. Se o usu√°rio mencionar uma marca ou fornecedor como 'KIT', 'EURO ROMA', 'LINEA', 'C√çRCULO', use a coluna `NOMEFABRICANTE`.\n"
        catalog_context += "3. Na d√∫vida, verifique se o termo da query est√° na lista de exemplos de `NOMEFABRICANTE`.\n"

        # 1Ô∏è‚É£ DEVELOPER MESSAGE - Identidade e Comportamento (Context7 2025)
        developer_context = f"""
# ü§ñ Analista Python Especializado em BI da UNE

Gere c√≥digo Python eficiente para an√°lise de vendas da UNE usando racioc√≠nio estruturado e regras de neg√≥cio.

## Dataset Parquet
- An√°lise Temporal: Dataset possui colunas de vendas mensais (mes_01 a mes_12) e vendas dos √∫ltimos 30 dias (venda_30_d).
- `estoque_atual`: Estoque total da UNE
- `nomesegmento`: Segmento do produto (Ex: TECIDOS, PAPELARIA)
- `NOMEFABRICANTE`: Fabricante/Fornecedor do produto (Ex: KIT, C√çRCULO)
- Colunas: {', '.join(list(self.column_descriptions.keys())[:15])}...

{catalog_context}

## Regras Essenciais de C√≥digo
1. **Nomes EXATOS** de colunas (case-sensitive): `nomesegmento`, `NOMEFABRICANTE`.
2. **Valida√ß√£o flex√≠vel**: `if 'col' in df.columns` antes de usar.
3. **Retorne** em `result`: dict, DataFrame ou Plotly Figure.
4. **Coment√°rios** explicativos no c√≥digo.
5. **Trate casos extremos**: dados vazios, valores nulos.
"""
        # O resto do m√©todo continua como antes...
        few_shot_section = ""
        if rag_examples and len(rag_examples) > 0:
            num_examples = min(3, len(rag_examples))
            few_shot_section = "\n\n# üìö EXEMPLOS DE REFER√äNCIA (Few-Shot Learning)\n\n"
            few_shot_section += "Analise estes exemplos para entender o padr√£o, mas adapte para a query atual.\n\n"
            for i, ex in enumerate(rag_examples[:num_examples], 1):
                similarity = ex.get('similarity_score', 0)
                few_shot_section += f"## Exemplo {i} (Relev√¢ncia: {similarity:.1%})\n\n**Input:** \"{ex.get('query_user', 'N/A')}\"\n\n**Racioc√≠nio:** {self._extract_reasoning_from_example(ex)}\n\n**C√≥digo Python:**\n```python\n{ex.get('code_generated', 'N/A')}\n```\n\n**Output:** {ex.get('result_type', 'success')} | {ex.get('rows_returned', 0)} registros\n\n---\n\n"
        
        user_message = f"""
## Query Atual
{user_query}

## Abordagem (Sketch-of-Thought)
Antes de gerar o c√≥digo, considere:

1. **Objetivo**: O que o usu√°rio quer descobrir?
2. **Dados necess√°rios**: Quais colunas usar? `nomesegmento` ou `NOMEFABRICANTE`?
3. **Transforma√ß√µes**: Filtros, agrega√ß√µes, ordena√ß√£o?
4. **Sa√≠da**: Tabela, gr√°fico ou m√©trica?

Agora gere c√≥digo Python limpo usando `load_data()`.
O script DEVE terminar atribuindo o resultado final √† vari√°vel `result`.
"""
        full_prompt = developer_context + few_shot_section + user_message
        return full_prompt

    def _extract_reasoning_from_example(self, example: Dict[str, Any]) -> str:
        """
        Extrai/gera racioc√≠nio para um exemplo few-shot (Context7 2025).
        Inclui contexto de regras de neg√≥cio UNE.

        Args:
            example: Dicion√°rio com exemplo de query

        Returns:
            String com racioc√≠nio breve estruturado
        """
        query = example.get('query_user', '').lower()
        code = example.get('code_generated', '')

        # Inferir racioc√≠nio baseado no padr√£o da query (Context7 2025)
        reasoning_parts = []

        # 1. Detectar tipo de an√°lise (objetivo)
        if 'ranking' in query or 'top' in query:
            reasoning_parts.append("Objetivo: Ranking (ordena√ß√£o desc + limita√ß√£o)")
        elif 'gr√°fico' in query or 'grafico' in query:
            reasoning_parts.append("Objetivo: Visualiza√ß√£o (agrega√ß√£o + Plotly)")
        elif 'total' in query or 'soma' in query:
            reasoning_parts.append("Objetivo: Totaliza√ß√£o (sum/agrega√ß√£o)")
        elif 'comparar' in query or 'versus' in query:
            reasoning_parts.append("Objetivo: Compara√ß√£o (groupby m√∫ltiplo)")
        elif 'tend√™ncia' in query or 'evolu√ß√£o' in query:
            reasoning_parts.append("Objetivo: S√©rie temporal (mes_01 a mes_12)")
        else:
            reasoning_parts.append("Objetivo: Consulta anal√≠tica")

        # 2. Detectar dados necess√°rios (colunas)
        data_needed = []
        if 'estoque' in query:
            if 'linha verde' in query or 'lv' in query:
                data_needed.append("estoque_lv")
            elif 'cd' in query or 'centro' in query:
                data_needed.append("estoque_cd")
            else:
                data_needed.append("estoque_atual")

        if 'venda' in query or 'vendas' in query:
            if any(x in query for x in ['30 dias', '30d', 'm√™s', 'mes']):
                data_needed.append("venda_30_d ou mes_XX")
            else:
                data_needed.append("venda_30_d")

        if 'loja' in query or 'une' in query:
            data_needed.append("une_nome")

        if 'segmento' in query:
            data_needed.append("nomesegmento")

        if 'pre√ßo' in query or 'preco' in query:
            data_needed.append("preco_38_percent")

        if data_needed:
            reasoning_parts.append(f"Dados: {', '.join(data_needed)}")

        # 3. Detectar transforma√ß√µes (opera√ß√µes)
        transformations = []
        if 'groupby' in code:
            transformations.append("groupby")
        if 'sort_values' in code:
            transformations.append("sort desc")
        if 'head(' in code:
            transformations.append("limit N")
        if 'px.' in code or 'plotly' in code:
            transformations.append("plotar")
        if 'filter' in code or 'query' in code or '[' in code:
            transformations.append("filtrar")

        if transformations:
            reasoning_parts.append(f"A√ß√µes: {' + '.join(transformations)}")

        # 4. Detectar tipo de sa√≠da
        if 'px.' in code:
            reasoning_parts.append("Sa√≠da: Gr√°fico Plotly")
        elif 'DataFrame' in code or 'df' in code:
            reasoning_parts.append("Sa√≠da: Tabela")
        else:
            reasoning_parts.append("Sa√≠da: M√©trica/Dict")

        # Montar racioc√≠nio estruturado (SoT - Sketch of Thought)
        if reasoning_parts:
            return " ‚Üí ".join(reasoning_parts)
        else:
            return "Consulta direta filtrada"

    def generate_and_execute_code(self, input_data: Dict[str, Any]) -> dict:
        """
        Gera, executa e retorna o resultado do c√≥digo Python para uma dada consulta.
        Esta vers√£o foi refatorada para usar diretamente o prompt fornecido e injetar uma fun√ß√£o `load_data`.
        """
        prompt = input_data.get("query", "")
        raw_data = input_data.get("raw_data", [])
        user_query = input_data.get("query", "")  # Definir no in√≠cio para evitar UnboundLocalError

        # üéØ Cache inteligente V2: Normalizar query para maior hit rate
        # Isso permite que "Mostre o ranking de papelaria" = "ranking papelaria" = "top 10 papelaria"
        normalized_query = self._normalize_query(user_query)
        query_lower = user_query.lower()
        intent_markers = []

        # Detectar tipo de an√°lise
        if any(word in query_lower for word in ['gr√°fico', 'chart', 'visualiza√ß√£o', 'plot', 'graf']):
            intent_markers.append('viz')
        if any(word in query_lower for word in ['ranking', 'top', 'rank']):
            intent_markers.append('rank')

        # Detectar segmento espec√≠fico (extrair para evitar cache cruzado)
        segment_match = re.search(r'(tecido|papelaria|armarinho|festas|artes|casa|decora√ß√£o|higiene|beleza|esporte|lazer|bazar|el√©trica|limpeza|sazonais|inform√°tica|embalagens)', query_lower)
        if segment_match:
            intent_markers.append(f'seg_{segment_match.group(1)}')

        # Gerar chave de cache √∫nica baseada em query NORMALIZADA + inten√ß√£o
        # Usar query normalizada aumenta hit rate em ~30-50%
        cache_key = hash(normalized_query + '_'.join(intent_markers) + (json.dumps(raw_data, sort_keys=True) if raw_data else ""))

        self.logger.debug(f"Cache: query_original='{user_query}' ‚Üí normalized='{normalized_query}' ‚Üí key={cache_key}")

        if cache_key in self.code_cache:
            code_to_execute = self.code_cache[cache_key]
            self.logger.info(f"C√≥digo recuperado do cache.")
        else:
            # ‚úÖ CORRE√á√ÉO: Usar nomes REAIS do Parquet (confirmados via read_parquet_schema)
            important_columns = [
                "codigo", "nome_produto", "nomesegmento", "NOMECATEGORIA", "nomegrupo", "NOMESUBGRUPO",
                "NOMEFABRICANTE", "venda_30_d", "estoque_atual", "preco_38_percent",
                "une", "une_nome", "tipo", "embalagem", "ean",
                # Colunas temporais para gr√°ficos de evolu√ß√£o
                "mes_01", "mes_02", "mes_03", "mes_04", "mes_05", "mes_06",
                "mes_07", "mes_08", "mes_09", "mes_10", "mes_11", "mes_12",
                # Colunas de an√°lise adicional
                "media_considerada_lv", "estoque_lv", "estoque_cd", "abc_une_30_dd"
            ]

            column_context = "üìä COLUNAS DISPON√çVEIS:\n"
            for col in important_columns:
                if col in self.column_descriptions:
                    column_context += f"- {col}: {self.column_descriptions[col]}\n"

            # Adicionar valores v√°lidos de segmentos com mapeamento inteligente
            valid_segments = """
**VALORES V√ÅLIDOS DE SEGMENTOS (NOMESEGMENTO):**
Use EXATAMENTE estes valores no c√≥digo Python (incluindo acentos e plural/singular):

1. 'TECIDOS' ‚Üí se usu√°rio mencionar: tecido, tecidos, segmento tecido, tecidos e armarinhos
2. 'ARMARINHO E CONFEC√á√ÉO' ‚Üí se usu√°rio mencionar: armarinho, confec√ß√£o, aviamentos
3. 'PAPELARIA' ‚Üí se usu√°rio mencionar: papelaria, papel, cadernos
4. 'CASA E DECORA√á√ÉO' ‚Üí se usu√°rio mencionar: casa, decora√ß√£o, utilidades dom√©sticas
5. 'ARTES' ‚Üí se usu√°rio mencionar: artes, artesanato, pintura
6. 'SAZONAIS' ‚Üí se usu√°rio mencionar: sazonais, p√°scoa, natal, datas comemorativas
7. 'FESTAS' ‚Üí se usu√°rio mencionar: festas, anivers√°rio, bal√µes
8. 'INFORM√ÅTICA' ‚Üí se usu√°rio mencionar: inform√°tica, eletr√¥nica, computadores
9. 'HIGIENE E BELEZA' ‚Üí se usu√°rio mencionar: higiene, beleza, cosm√©ticos
10. 'ESPORTE E LAZER' ‚Üí se usu√°rio mencionar: esporte, lazer, brinquedos
11. 'EMBALAGENS E DESCART√ÅVEIS' ‚Üí se usu√°rio mencionar: embalagens, descart√°veis
12. 'BAZAR' ‚Üí se usu√°rio mencionar: bazar, utilidades
13. 'EL√âTRICA E MANUTEN√á√ÉO' ‚Üí se usu√°rio mencionar: el√©trica, manuten√ß√£o, ferramentas
14. 'MATERIAL DE LIMPEZA' ‚Üí se usu√°rio mencionar: limpeza, produtos de limpeza

**REGRA DE OURO:** Interprete a inten√ß√£o do usu√°rio e mapeie para o valor EXATO da lista acima!
"""

            # ‚úÖ CORRE√á√ÉO CR√çTICA: Atualizar instru√ß√µes para usar 'une_nome' (n√£o 'UNE')
            valid_unes = """
**üö® VALORES V√ÅLIDOS DE LOJAS/UNIDADES (SCHEMA CORRETO DO PARQUET):**

O Parquet possui DUAS colunas relacionadas a UNE:
1. **une** (int) - ID num√©rico da loja (ex: 1, 2586, 2720)
2. **une_nome** (str) - Nome da loja (ex: 'SCR', 'MAD', '261')

**NOMES V√ÅLIDOS (coluna une_nome):**
'SCR', 'ALC', 'DC', 'CFR', 'PET', 'VVL', 'VIL', 'REP', 'JFA', 'NIT',
'CGR', 'OBE', 'CXA', '261', 'BGU', 'ALP', 'BAR', 'CP2', 'JRD', 'NIG',
'ITA', 'MAD', 'JFJ', 'CAM', 'VRD', 'SGO', 'NFR', 'TIJ', 'ANG', 'BON',
'IPA', 'BOT', 'NIL', 'TAQ', 'RDO', '3RS', 'STS', 'NAM'

**‚úÖ EXEMPLOS CORRETOS (usar une_nome, N√ÉO 'UNE'):**
```python
# Filtrar por nome de loja (use une_nome!)
df_mad = df[df['une_nome'] == 'MAD']
df_scr = df[df['une_nome'] == 'SCR']
df_261 = df[df['une_nome'] == '261']
df_nil = df[df['une_nome'] == 'NIL']
```

**‚ùå ERRADO (N√ÉO use 'UNE', essa coluna N√ÉO EXISTE no Parquet!):**
```python
df_mad = df[df['UNE'] == 'MAD']  # ‚ùå KeyError: 'UNE'
```

**REGRA DE OURO:** SEMPRE use 'une_nome' para filtrar por loja!
Se precisar do ID num√©rico, use a coluna 'une' (min√∫sculo).
"""

            # üéØ PILAR 2: Injetar exemplos contextuais baseados em padr√µes (Few-Shot Learning)
            examples_context = ""
            if self.pattern_matcher:
                try:
                    # Buscar padr√£o similar √† query do usu√°rio
                    match_result = self.pattern_matcher.match_pattern(user_query)
                    if match_result:
                        pattern_name, pattern_data = match_result
                        # Formatar exemplos para inje√ß√£o no prompt
                        examples_context = self.pattern_matcher.format_examples_for_prompt(pattern_data, max_examples=2)
                        self.logger.info(f"üéØ Few-Shot Learning: Padr√£o '{pattern_name}' identificado com {len(pattern_data.get('examples', []))} exemplos")
                    else:
                        self.logger.debug("‚ÑπÔ∏è Nenhum padr√£o espec√≠fico identificado para esta query")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao buscar padr√µes: {e}")

            # üéØ PILAR 2.5: RAG - Busca sem√¢ntica de queries similares (expandir Few-Shot Learning)
            rag_examples = []
            if self.rag_enabled and self.query_retriever:
                try:
                    similar_queries = self.query_retriever.find_similar_queries(user_query, top_k=3)
                    if similar_queries:
                        # Filtrar apenas exemplos com alta similaridade (> 0.7)
                        rag_examples = [ex for ex in similar_queries if ex.get('similarity_score', 0) > 0.7]

                        if rag_examples:
                            self.logger.info(f"üîç RAG: {len(rag_examples)} queries similares de alta qualidade encontradas (melhor match: {similar_queries[0].get('similarity_score', 0):.2%})")
                        else:
                            self.logger.debug("‚ÑπÔ∏è RAG: Nenhuma query com similaridade > 0.7 encontrada")
                    else:
                        self.logger.debug("‚ÑπÔ∏è RAG: Nenhuma query similar encontrada no banco")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao buscar queries similares (RAG): {e}")

            # ‚úÖ NOVO: Construir prompt estruturado usando Context7 best practices
            # Developer message + Few-shot learning + Chain-of-thought
            system_prompt = self._build_structured_prompt(user_query, rag_examples=rag_examples)

            # Adicionar contexto de exemplos do PatternMatcher (se houver)
            if examples_context:
                system_prompt += f"\n\n{examples_context}"

            self.logger.info(f"‚úÖ Prompt estruturado gerado: {len(system_prompt)} caracteres, {len(rag_examples)} exemplos RAG")

            # üöÄ PILAR 4: Adicionar avisos din√¢micos baseados em erros recentes
            if self.dynamic_prompt:
                try:
                    enhanced_prompt = self.dynamic_prompt.get_enhanced_prompt()
                    # Adicionar avisos ao system_prompt
                    system_prompt = system_prompt + "\n\n" + enhanced_prompt
                    self.logger.info("‚úÖ Prompt enriquecido com DynamicPrompt (Pilar 4)")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao enriquecer prompt: {e}")

            # O agente agora usa o prompt diretamente, sem construir um novo.
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]

            # üî• NOVO: Passar contexto ao cache para separar gera√ß√£o de c√≥digo/gr√°ficos
            cache_context = {
                "operation": "generate_code",
                "query_type": "python_generation",
                "user_query": user_query[:100]  # Primeiros 100 chars para diferenciar queries similares
            }

            start_llm_query = time.time()
            llm_response = self.llm.get_completion(messages=messages, cache_context=cache_context)
            end_llm_query = time.time()
            self.logger.info(f"Tempo de consulta LLM: {end_llm_query - start_llm_query:.4f} segundos")

            if "error" in llm_response:
                self.logger.error(f"Erro ao obter resposta do LLM: {llm_response['error']}")
                return {"type": "error", "output": "N√£o foi poss√≠vel gerar o c√≥digo de an√°lise."}

            code_to_execute = self._extract_python_code(llm_response.get("content", ""))

            if not code_to_execute:
                self.logger.warning("Nenhum c√≥digo Python foi gerado pelo LLM.")
                return {"type": "text", "output": "N√£o consegui gerar um script para responder √† sua pergunta."}

            # üöÄ QUICK WIN 1: Validar e corrigir Top N automaticamente
            # user_query j√° foi definido no in√≠cio da fun√ß√£o
            code_to_execute = self._validate_top_n(code_to_execute, user_query)

            # üîß SELF-HEALING: Valida√ß√£o e auto-corre√ß√£o PR√â-execu√ß√£o
            if self.self_healing:
                try:
                    # Obter schema de colunas dispon√≠veis
                    schema_columns = list(self.column_descriptions.keys())

                    healing_context = {
                        'query': user_query,
                        'schema_columns': schema_columns
                    }

                    # Validar e tentar curar
                    is_valid, healed_code, feedback = self.self_healing.validate_and_heal(
                        code_to_execute,
                        healing_context
                    )

                    if feedback:
                        for msg in feedback:
                            self.logger.info(f"üîß Self-Healing: {msg}")

                    if healed_code != code_to_execute:
                        self.logger.info("‚úÖ C√≥digo auto-corrigido pelo SelfHealingSystem")
                        code_to_execute = healed_code

                    if not is_valid:
                        self.logger.warning("‚ö†Ô∏è Self-Healing detectou problemas que podem causar erro")

                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro no Self-Healing pr√©-execu√ß√£o: {e}")

            # ‚úÖ FASE 1: Validar c√≥digo antes de executar
            validation_result = self.code_validator.validate(code_to_execute, user_query)

            if not validation_result['valid']:
                self.logger.warning(f"‚ö†Ô∏è C√≥digo com problemas: {validation_result['errors']}")

                # Tentar corre√ß√£o autom√°tica (FUNCIONALIDADE PENDENTE)
                # fix_result = self.code_validator.auto_fix(validation_result, user_query)

                # if fix_result['fixed']:
                #     self.logger.info(f"‚úÖ C√≥digo corrigido automaticamente: {fix_result['fixes_applied']}")
                #     code_to_execute = fix_result['code']
                # else:
                #     self.logger.warning(f"‚ö†Ô∏è Corre√ß√£o autom√°tica falhou. Erros restantes: {fix_result.get('remaining_errors', [])}")
                #     # Continuar mesmo assim, mas com log

            # Valida√ß√µes adicionais com warnings (n√£o bloqueiam execu√ß√£o)
            if validation_result.get('warnings'):
                self.logger.info(f"‚ÑπÔ∏è Avisos: {validation_result['warnings']}")

            if validation_result.get('suggestions'):
                self.logger.debug(f"üí° Sugest√µes: {validation_result['suggestions']}")

            self.code_cache[cache_key] = code_to_execute

        self.logger.info(f"\nC√≥digo a ser executado:\n---\n{code_to_execute}\n---")
        print(f"Generated code:\n{code_to_execute}")

        try:
            # ‚ö†Ô∏è IMPORTANTE: Reutilizar a fun√ß√£o load_data() definida em _execute_generated_code
            # que j√° usa Dask e l√™ TODOS os arquivos Parquet (*.parquet)

            local_scope = {
                "pd": pd,
                "px": px,
                "result": None,
                "df_raw_data": pd.DataFrame(raw_data) if raw_data else None,
                # load_data ser√° injetado em _execute_generated_code
            }
            
            px.defaults.template = "plotly_white"

            start_code_execution = time.time()
            result = self._execute_generated_code(code_to_execute, local_scope)
            end_code_execution = time.time()
            self.logger.info(f"Tempo de execu√ß√£o do c√≥digo: {end_code_execution - start_code_execution:.4f} segundos")

            # ‚ö†Ô∏è VALIDA√á√ÉO CR√çTICA: Verificar se resultado √© Dask n√£o computado
            if hasattr(result, '_name') and 'dask' in str(type(result)).lower():
                self.logger.error(f"‚ùå ERRO: C√≥digo retornou Dask object n√£o computado: {type(result)}")
                self.logger.error(f"   O c√≥digo gerado deve chamar .compute() antes de retornar o resultado!")
                return {
                    "type": "error",
                    "output": "Erro interno: O c√≥digo gerou um resultado Dask n√£o computado. Tentando novamente..."
                }

            # An√°lise do tipo de resultado
            if isinstance(result, pd.DataFrame):
                self.logger.info(f"Resultado: DataFrame com {len(result)} linhas.")
                # üöÄ QUICK WIN 2: Registrar query bem-sucedida
                self._log_successful_query(user_query, code_to_execute, len(result))
                return {"type": "dataframe", "output": result}
            elif isinstance(result, pd.Series):
                self.logger.info(f"Resultado: Series com {len(result)} elementos.")
                # Converter Series para DataFrame para consist√™ncia
                result_df = result.reset_index()
                self._log_successful_query(user_query, code_to_execute, len(result_df))
                return {"type": "dataframe", "output": result_df}
            elif isinstance(result, list) and len(result) > 0 and 'plotly' in str(type(result[0])):
                # ‚úÖ CORRE√á√ÉO: Lista de Figures Plotly (m√∫ltiplos gr√°ficos)
                self.logger.info(f"Resultado: {len(result)} gr√°ficos Plotly.")

                # Aplicar tema escuro a cada Figure
                figures_json = []
                for i, fig in enumerate(result):
                    if 'plotly' in str(type(fig)):
                        # ‚ú® APLICAR TEMA ESCURO CHATGPT
                        fig.update_layout(
                            plot_bgcolor='#2a2b32',
                            paper_bgcolor='#2a2b32',
                            font=dict(color='#ececf1', family='sans-serif'),
                            title=dict(font=dict(color='#ececf1', size=18)),
                            xaxis=dict(
                                gridcolor='#444654',
                                tickfont=dict(color='#ececf1'),
                                title=dict(font=dict(color='#ececf1'))
                            ),
                            yaxis=dict(
                                gridcolor='#444654',
                                tickfont=dict(color='#ececf1'),
                                title=dict(font=dict(color='#ececf1'))
                            ),
                            margin=dict(l=60, r=40, t=40, b=80),
                            hoverlabel=dict(
                                bgcolor='#2a2b32',
                                bordercolor='#10a37f',
                                font=dict(color='#ececf1')
                            ),
                            legend=dict(
                                font=dict(color='#ececf1'),
                                bgcolor='rgba(42, 43, 50, 0.8)'
                            )
                        )
                        figures_json.append(pio.to_json(fig))
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Item {i} na lista n√£o √© uma Figure Plotly: {type(fig)}")

                # üöÄ Registrar query bem-sucedida (m√∫ltiplos gr√°ficos)
                self._log_successful_query(user_query, code_to_execute, len(figures_json))
                return {"type": "multiple_charts", "output": figures_json}
            elif 'plotly' in str(type(result)):
                self.logger.info(f"Resultado: Gr√°fico Plotly.")

                # ‚ú® APLICAR TEMA ESCURO CHATGPT (20/10/2025)
                result.update_layout(
                    plot_bgcolor='#2a2b32',
                    paper_bgcolor='#2a2b32',
                    font=dict(color='#ececf1', family='sans-serif'),
                    title=dict(font=dict(color='#ececf1', size=18)),
                    xaxis=dict(
                        gridcolor='#444654',
                        tickfont=dict(color='#ececf1'),
                        title=dict(font=dict(color='#ececf1'))
                    ),
                    yaxis=dict(
                        gridcolor='#444654',
                        tickfont=dict(color='#ececf1'),
                        title=dict(font=dict(color='#ececf1'))
                    ),
                    margin=dict(l=60, r=40, t=40, b=80),
                    hoverlabel=dict(
                        bgcolor='#2a2b32',
                        bordercolor='#10a37f',
                        font=dict(color='#ececf1')
                    ),
                    legend=dict(
                        font=dict(color='#ececf1'),
                        bgcolor='rgba(42, 43, 50, 0.8)'
                    )
                )

                # üöÄ QUICK WIN 2: Registrar query bem-sucedida (gr√°fico)
                self._log_successful_query(user_query, code_to_execute, 1)
                return {"type": "chart", "output": pio.to_json(result)}
            else:
                self.logger.info(f"Resultado: Texto.")
                return {"type": "text", "output": str(result)}
        
        except TimeoutError as e:
            self.logger.error("A execu√ß√£o do c√≥digo excedeu o tempo limite.")
            # üöÄ QUICK WIN 3: Registrar erro
            self._log_error(user_query, code_to_execute, "timeout", str(e))
            return {"type": "error", "output": "A an√°lise demorou muito e foi interrompida."}
        except Exception as e:
            error_msg = str(e)
            error_type = type(e).__name__

            # üîß SELF-HEALING: Tentar corrigir erro automaticamente
            if self.self_healing and not hasattr(self, '_healing_retry_count'):
                try:
                    self._healing_retry_count = 0

                    self.logger.info(f"üîß Self-Healing: Tentando corrigir {error_type}...")

                    # Obter schema de colunas
                    schema_columns = list(self.column_descriptions.keys())
                    healing_context = {
                        'query': user_query,
                        'schema_columns': schema_columns
                    }

                    # Validar e tentar curar
                    is_valid, healed_code, feedback = self.self_healing.validate_and_heal(
                        code_to_execute,
                        healing_context
                    )

                    if feedback:
                        for msg in feedback:
                            self.logger.info(f"üîß Self-Healing: {msg}")

                    if healed_code != code_to_execute:
                        self.logger.info("‚úÖ C√≥digo auto-corrigido pelo SelfHealingSystem")
                        code_to_execute = healed_code

                    if not is_valid:
                        self.logger.warning("‚ö†Ô∏è Self-Healing detectou problemas que podem causar erro")

                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro no Self-Healing pr√©-execu√ß√£o: {e}")

            # ‚ö†Ô∏è AUTO-RECOVERY: Detectar erros comuns e limpar cache (fallback)
            should_retry = False

            if "'DataFrame' object has no attribute 'compute'" in error_msg or \
               "'Series' object has no attribute 'compute'" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo com .compute() inv√°lido")

            elif "boolean value of NA is ambiguous" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo sem tratamento de NA")

            elif "Invalid comparison between dtype=" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo sem convers√£o de tipos")

            elif "'Series' object has no attribute 'sort_values'" in error_msg or \
                 "AttributeError: 'Series'" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo com erro em Series (falta .reset_index()?)")

            elif "UnboundLocalError" in error_type or "cannot access local variable" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado UnboundLocalError - poss√≠vel conflito de escopo")

            if should_retry:

                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo com .compute() inv√°lido em pandas object")
                self.logger.info(f"üîÑ Limpando cache e tentando novamente com prompt atualizado...")

                # Limpar apenas o cache desta query espec√≠fica
                if cache_key in self.code_cache:
                    del self.code_cache[cache_key]
                    self.logger.info(f"‚úÖ Cache da query removido: {str(cache_key)[:50]}...")

                # Tentar novamente (recursivo) - APENAS UMA VEZ
                if not hasattr(self, '_retry_flag'):
                    self._retry_flag = True
                    try:
                        result = self.generate_and_execute_code(input_data)
                        return result
                    finally:
                        delattr(self, '_retry_flag')
                else:
                    self.logger.error(f"‚ùå Retry falhou. Erro persistente ap√≥s limpeza de cache.")

            self.logger.error(f"Erro ao executar o c√≥digo gerado: {e}", exc_info=True)
            # üöÄ QUICK WIN 3: Registrar erro
            self._log_error(user_query, code_to_execute, error_type, error_msg)
            return {"type": "error", "output": f"Ocorreu um erro ao executar a an√°lise: {error_msg}"}
    def _extract_python_code(self, text: str) -> str | None:
        """Extrai o bloco de c√≥digo Python da resposta do LLM."""
        match = re.search(r'```python\n(.*)```', text, re.DOTALL)
        return match.group(1).strip() if match else None

    # üöÄ QUICK WIN METHODS
    def _validate_top_n(self, code: str, user_query: str) -> str:
        """
        QUICK WIN 1: Valida se c√≥digo tem .head(N) quando usu√°rio pede 'top N'.
        Corrige automaticamente se necess√°rio.
        """
        query_lower = user_query.lower()

        # Verificar se usu√°rio pediu "top N"
        top_match = re.search(r'top\s+(\d+)', query_lower)

        # ‚úÖ N√ÉO adicionar .head() se o c√≥digo est√° gerando um gr√°fico Plotly
        # Gr√°ficos j√° devem ter o filtro aplicado antes do px.bar/px.pie/etc
        is_plotly_chart = any(func in code for func in ['px.bar(', 'px.pie(', 'px.line(', 'px.scatter(', 'px.histogram('])

        if top_match and '.head(' not in code and not is_plotly_chart:
            n = top_match.group(1)
            self.logger.warning(f"‚ö†Ô∏è Query pede top {n} mas c√≥digo n√£o tem .head(). Corrigindo automaticamente...")

            # Tentar adicionar .head(N) antes de .reset_index()
            if '.reset_index()' in code:
                code = code.replace('.reset_index()', f'.head({n}).reset_index()')
            # Ou antes do resultado final
            elif 'result = ' in code:
                # Encontrar a √∫ltima atribui√ß√£o a result
                lines = code.split('\n')
                for i in range(len(lines) - 1, -1, -1):
                    if lines[i].strip().startswith('result = '):
                        # Adicionar .head(N) se ainda n√£o existir
                        if '.head(' not in lines[i]:
                            lines[i] = lines[i].replace('result = ', f'result = ').rstrip()
                            if not lines[i].endswith(')'):
                                lines[i] = f"{lines[i]}.head({n})"
                        break
                code = '\n'.join(lines)

            self.logger.info(f"‚úÖ C√≥digo corrigido automaticamente com .head({n})")
        elif is_plotly_chart:
            self.logger.info(f"‚ÑπÔ∏è C√≥digo gera gr√°fico Plotly - n√£o adicionando .head() autom√°tico")

        return code

    def _log_successful_query(self, user_query: str, code: str, result_rows: int):
        """
        QUICK WIN 2: Registra queries bem-sucedidas para an√°lise futura.
        + RAG: Coleta autom√°tica para banco de exemplos.
        """
        from datetime import datetime

        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'query': user_query,
            'code': code,
            'rows': result_rows,
            'success': True
        }

        # Salvar em arquivo di√°rio
        date_str = datetime.now().strftime('%Y%m%d')
        log_file = os.path.join(self.logs_dir, f'successful_queries_{date_str}.jsonl')

        try:
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
            self.logger.debug(f"‚úÖ Query registrada em {log_file}")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao registrar query: {e}")

        # üéØ RAG: Coletar query bem-sucedida para treinamento cont√≠nuo
        if self.rag_enabled and self.example_collector:
            try:
                # Detectar inten√ß√£o baseado no c√≥digo gerado
                intent = "python_analysis"
                if 'plotly' in code or 'px.' in code:
                    intent = "visualization"
                elif '.groupby' in code:
                    intent = "aggregation"
                elif '.nlargest' in code or '.nsmallest' in code:
                    intent = "ranking"

                # Coletar exemplo
                self.example_collector.collect_successful_query(
                    user_query=user_query,
                    code_generated=code,
                    result_rows=result_rows,
                    intent=intent
                )
                self.logger.debug(f"üìö RAG: Query coletada para banco de exemplos")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro ao coletar query no RAG: {e}")

    def _log_error(self, user_query: str, code: str, error_type: str, error_message: str):
        """
        QUICK WIN 3: Registra erros por tipo para an√°lise de padr√µes.
        """
        from datetime import datetime

        # Incrementar contador
        self.error_counts[error_type] += 1

        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'query': user_query,
            'code': code,
            'error_type': error_type,
            'error_message': str(error_message),
            'success': False
        }

        # Salvar em arquivo di√°rio
        date_str = datetime.now().strftime('%Y%m%d')
        log_file = os.path.join(self.logs_dir, f'error_log_{date_str}.jsonl')

        try:
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')

            # Tamb√©m salvar contador consolidado
            counter_file = os.path.join(self.logs_dir, f'error_counts_{date_str}.json')
            with open(counter_file, 'w', encoding='utf-8') as f:
                json.dump(dict(self.error_counts), f, indent=2, ensure_ascii=False)

            self.logger.debug(f"‚ö†Ô∏è Erro registrado: {error_type} (total: {self.error_counts[error_type]})")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao registrar erro: {e}")

    def _clean_old_cache(self, max_age_hours=2):
        """Limpa cache antigo automaticamente (padr√£o: 2 horas)"""
        import os
        import time
        from pathlib import Path

        try:
            cache_dirs = [
                Path('data/cache'),
                Path('data/cache_agent_graph')
            ]

            now = time.time()
            max_age = max_age_hours * 60 * 60  # Converte horas para segundos
            removed_count = 0

            for cache_dir in cache_dirs:
                if not cache_dir.exists():
                    continue

                for cache_file in cache_dir.glob('*'):
                    if cache_file.is_file():
                        file_age = now - cache_file.stat().st_mtime
                        if file_age > max_age:
                            cache_file.unlink()
                            removed_count += 1

            if removed_count > 0:
                self.logger.info(f"üßπ Cache limpo: {removed_count} arquivos removidos (> 24h)")

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao limpar cache: {e}")

    def _check_and_invalidate_cache_if_prompt_changed(self):
        """
        üîÑ VERSIONING DE CACHE: Invalida cache se o prompt mudou

        Calcula hash do prompt atual e compara com o hash salvo.
        Se diferente, limpa o cache para for√ßar regenera√ß√£o com novo prompt.
        """
        import hashlib
        from pathlib import Path
        import json

        try:
            # ‚úÖ INCREMENTAR VERS√ÉO para for√ßar regenera√ß√£o ap√≥s corre√ß√£o de schema
            prompt_components = {
                'columns': list(self.column_descriptions.keys()),
                'descriptions': list(self.column_descriptions.values()),
                # Adicionar outros componentes que afetam o prompt
                'version': '6.1_fix_temporal_dataframe_scalar_error_20251102'  # ‚úÖ FIX: Erro de DataFrame escalar em gr√°ficos de evolu√ß√£o
            }

            prompt_str = json.dumps(prompt_components, sort_keys=True)
            current_hash = hashlib.md5(prompt_str.encode()).hexdigest()

            # Arquivo para armazenar hash do prompt
            version_file = Path('data/cache/.prompt_version')

            # Verificar se h√° vers√£o anterior
            if version_file.exists():
                try:
                    with open(version_file, 'r') as f:
                        saved_hash = f.read().strip()

                    if saved_hash != current_hash:
                        # PROMPT MUDOU! Limpar cache
                        self.logger.warning(f"‚ö†Ô∏è  PROMPT MUDOU! Limpando cache para for√ßar regenera√ß√£o...")
                        self.logger.info(f"   Hash anterior: {saved_hash}")
                        self.logger.info(f"   Hash novo: {current_hash}")

                        # Limpar todos os caches
                        cache_dirs = [
                            Path('data/cache'),
                            Path('data/cache_agent_graph')
                        ]

                        removed_count = 0
                        for cache_dir in cache_dirs:
                            if cache_dir.exists():
                                for cache_file in cache_dir.glob('*'):
                                    if cache_file.is_file() and cache_file.name != '.prompt_version':
                                        cache_file.unlink()
                                        removed_count += 1

                        self.logger.info(f"‚úÖ Cache invalidado: {removed_count} arquivos removidos")

                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao ler vers√£o do cache: {e}")

            # Salvar hash atual
            version_file.parent.mkdir(parents=True, exist_ok=True)
            with open(version_file, 'w') as f:
                f.write(current_hash)

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao verificar vers√£o do cache: {e}")

    def detect_broad_query(self, query: str) -> Tuple[bool, str]:
        """
        Detecta se uma query √© muito ampla e pode causar timeout.
        """
        query_lower = query.lower()

        # Crit√©rios de queries amplas
        broad_keywords = ["todos", "todas", "geral", "completo", "tudo", "vendas", "estoque"]
        specific_keywords = ["top", "limite", "une", "segmento", "categoria", "grupo", "fabricante", "pre√ßo", "<", ">", "="]

        has_broad = any(kw in query_lower for kw in broad_keywords)
        has_specific = any(kw in query_lower for kw in specific_keywords)
        has_number = any(char.isdigit() for char in query_lower)

        if has_broad and not has_specific and not has_number:
            return True, "Keyword ampla detectada sem filtros espec√≠ficos ou limites num√©ricos"

        if "ranking" in query_lower and not has_specific and not has_number:
            return True, "Ranking sem limite ou filtro espec√≠fico"

        # Check for queries that are implicitly broad
        if not has_specific and not has_number:
            return True, "Query sem filtros espec√≠ficos ou limites num√©ricos"

        return False, "Query espec√≠fica OK"

    def get_educational_message(self, query: str, reason: str) -> str:
        """
        Gera uma mensagem educativa para queries amplas.
        """
        return f"""
üîç Query Muito Ampla Detectada

**Sua Pergunta:** "{query}"

**Motivo:** {reason}

**Por que isso acontece?**
- Processar milh√µes de registros pode levar muito tempo e causar erros.

**‚úÖ Como fazer queries eficientes:**

**Exemplos de queries v√°lidas:**
   1. Top 10 produtos mais vendidos da UNE NIG
   2. Produtos do segmento ARMARINHO com estoque < 10
   3. Vendas da UNE BEL nos √∫ltimos 30 dias

**üí° Dicas:**
1. Especifique uma UNE (loja)
2. Use limites (Top 10, Top 20)
3. Aplique filtros (segmento, pre√ßo, etc.)
4. Defina um per√≠odo de tempo

**üí° Sugest√£o:** Tente 'Top 10 produtos mais vendidos da UNE [c√≥digo]'"""