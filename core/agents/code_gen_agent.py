"""
M√≥dulo para core/agents/code_gen_agent.py. Define a classe principal 'CodeGenAgent'. Fornece as fun√ß√µes: generate_and_execute_code, worker.
"""

# core/agents/code_gen_agent.py
import logging
import os
import json
import re
import pandas as pd
import dask.dataframe as dd  # Dask para lazy loading
import time
import plotly.express as px
from typing import List, Dict, Any # Import necessary types
import threading
from queue import Queue
import pickle
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import io
import sys
import plotly.io as pio
import uuid
from core.utils.json_utils import _clean_json_values # Import the cleaning function

from core.llm_base import BaseLLMAdapter
from core.learning.pattern_matcher import PatternMatcher
from core.validation.code_validator import CodeValidator
from core.learning.dynamic_prompt import DynamicPrompt

class CodeGenAgent:
    """
    Agente especializado em gerar e executar c√≥digo Python para an√°lise de dados.
    """
    def __init__(self, llm_adapter: BaseLLMAdapter, data_adapter: any = None):
        """
        Inicializa o agente com o adaptador LLM e opcionalmente o adaptador de dados.

        Args:
            llm_adapter: Adaptador LLM para gera√ß√£o de c√≥digo
            data_adapter: (Opcional) Adaptador de dados para inje√ß√£o de load_data()
                         Se None, load_data() usar√° path padr√£o do Parquet
        """
        self.logger = logging.getLogger(__name__)
        self.llm = llm_adapter
        self.data_adapter = data_adapter  # Pode ser None (fallback para path padr√£o)
        self.code_cache = {}

        # Inicializar dicion√°rio de descri√ß√µes de colunas ANTES de verificar cache
        self.column_descriptions = {
            "PRODUTO": "C√≥digo √∫nico do produto",
            "NOME": "Nome/descri√ß√£o do produto",
            "NOMESEGMENTO": "Segmento do produto (TECIDOS, PAPELARIA, etc.)",
            "NOMECATEGORIA": "Categoria do produto",
            "NOMEGRUPO": "Grupo do produto",
            "NOMESUBGRUPO": "Subgrupo do produto",
            "NOMEFABRICANTE": "Fabricante do produto",
            "VENDA_30DD": "Total de vendas nos √∫ltimos 30 dias",
            "ESTOQUE_UNE": "Quantidade em estoque",
            "LIQUIDO_38": "Pre√ßo de venda",
            "UNE": "Nome da loja/unidade (ex: SCR, MAD, 261, ALC, NIL, etc.)",
            "UNE_ID": "ID num√©rico da loja (ex: 1=SCR, 2720=MAD, 1685=261)",
            "TIPO": "Tipo de produto",
            "EMBALAGEM": "Embalagem do produto",
            "EAN": "C√≥digo de barras",
            # üìä COLUNAS TEMPORAIS - Vendas mensais (mes_01 = m√™s mais recente)
            "mes_01": "Vendas do m√™s mais recente (m√™s 1)",
            "mes_02": "Vendas de 2 meses atr√°s",
            "mes_03": "Vendas de 3 meses atr√°s",
            "mes_04": "Vendas de 4 meses atr√°s",
            "mes_05": "Vendas de 5 meses atr√°s",
            "mes_06": "Vendas de 6 meses atr√°s",
            "mes_07": "Vendas de 7 meses atr√°s",
            "mes_08": "Vendas de 8 meses atr√°s",
            "mes_09": "Vendas de 9 meses atr√°s",
            "mes_10": "Vendas de 10 meses atr√°s",
            "mes_11": "Vendas de 11 meses atr√°s",
            "mes_12": "Vendas de 12 meses atr√°s (m√™s mais antigo)"
        }

        # Inicializar pattern_matcher and code_validator
        from collections import defaultdict
        try:
            self.pattern_matcher = PatternMatcher()
            self.logger.info("‚úÖ PatternMatcher inicializado (Few-Shot Learning ativo)")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è PatternMatcher n√£o dispon√≠vel: {e}")
            self.pattern_matcher = None

        self.code_validator = CodeValidator()
        self.error_counts = defaultdict(int)
        self.logs_dir = os.path.join(os.getcwd(), "data", "learning")
        os.makedirs(self.logs_dir, exist_ok=True)

        # Inicializar DynamicPrompt (Pilar 4)
        try:
            self.dynamic_prompt = DynamicPrompt()
            self.logger.info("‚úÖ DynamicPrompt inicializado (Pilar 4 ativo)")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è DynamicPrompt n√£o dispon√≠vel: {e}")
            self.dynamic_prompt = None

        # Limpar cache antigo automaticamente (> 2h - reduzido para evitar c√≥digo obsoleto)
        self._clean_old_cache(max_age_hours=2)

        # üîÑ VERSIONING DE CACHE: Invalidar cache quando prompt muda
        self._check_and_invalidate_cache_if_prompt_changed()

        self.logger.info("CodeGenAgent inicializado.")

    def _execute_generated_code(self, code: str, local_scope: Dict[str, Any]):
        q = Queue()
        output_capture = io.StringIO()
        original_stdout = sys.stdout
        original_stderr = sys.stderr

        # Fun√ß√£o helper para ser injetada no escopo de execu√ß√£o
        def load_data():
            """
            Carrega o dataframe usando Dask (lazy loading).
            IMPORTANTE: Retorna um Dask DataFrame - aplique filtros ANTES de .compute()!
            """
            import dask.dataframe as dd

            if self.data_adapter:
                # ParquetAdapter tem file_path
                file_path = getattr(self.data_adapter, 'file_path', None)
                if file_path:
                    # üöÄ CARREGAR COMO DASK DATAFRAME (lazy)
                    ddf = dd.read_parquet(file_path, engine='pyarrow')
                else:
                    raise AttributeError(f"Adapter {type(self.data_adapter).__name__} n√£o tem file_path")
            else:
                # Fallback: carregar diretamente do Parquet (legacy/compatibilidade)
                import os
                parquet_dir = os.path.join(os.getcwd(), "data", "parquet")
                parquet_pattern = os.path.join(parquet_dir, "*.parquet")
                if not os.path.exists(parquet_dir):
                    raise FileNotFoundError(f"Diret√≥rio Parquet n√£o encontrado em {parquet_dir}")
                # üöÄ CARREGAR COMO DASK DATAFRAME (lazy) - LER TODOS OS ARQUIVOS!
                ddf = dd.read_parquet(parquet_pattern, engine='pyarrow')

            # ‚úÖ NORMALIZAR COLUNAS: Mapear para os nomes esperados pelo LLM (em Dask)
            column_mapping = {
                'une': 'UNE_ID',  # Renomear 'une' para evitar conflito com 'UNE' (de une_nome)
                'nomesegmento': 'NOMESEGMENTO',
                'codigo': 'PRODUTO',
                'nome_produto': 'NOME',
                'une_nome': 'UNE',  # une_nome vira UNE (nome da loja)
                'nomegrupo': 'NOMEGRUPO',
                'ean': 'EAN',
                'preco_38_percent': 'LIQUIDO_38',
                'venda_30_d': 'VENDA_30DD',
                'estoque_atual': 'ESTOQUE_UNE',
                'embalagem': 'EMBALAGEM',
                'tipo': 'TIPO'
            }

            # Aplicar mapeamento apenas para colunas que existem (Dask suporta .rename)
            rename_dict = {k: v for k, v in column_mapping.items() if k in ddf.columns}
            ddf = ddf.rename(columns=rename_dict)

            # ‚úÖ CONVERTER ESTOQUE_UNE PARA NUM√âRICO (Dask suporta map_partitions)
            if 'ESTOQUE_UNE' in ddf.columns:
                ddf['ESTOQUE_UNE'] = dd.to_numeric(ddf['ESTOQUE_UNE'], errors='coerce').fillna(0)

            # RETORNAR DASK DATAFRAME - O c√≥digo gerado deve chamar .compute() ap√≥s filtros!
            return ddf

        local_scope['load_data'] = load_data
        local_scope['dd'] = dd  # Adicionar Dask ao escopo para c√≥digo gerado

        def worker():
            sys.stdout = output_capture
            sys.stderr = output_capture
            try:
                exec(code, local_scope)
                q.put(local_scope.get('result'))
            except Exception as e:
                q.put(e)
            finally:
                sys.stdout = original_stdout
                sys.stderr = original_stderr

        thread = threading.Thread(target=worker)
        thread.start()
        thread.join(timeout=120.0)

        captured_output = output_capture.getvalue()
        if captured_output:
            self.logger.info(f"Sa√≠da do c√≥digo gerado:\n{captured_output}")

        if thread.is_alive():
            raise TimeoutError("A execu√ß√£o do c√≥digo gerado excedeu o tempo limite.")
        else:
            result = q.get()
            if isinstance(result, Exception):
                raise result
            return result

    def _normalize_query(self, query: str) -> str:
        """
        Normaliza query para melhorar cache hit rate.
        Remove stopwords e varia√ß√µes irrelevantes, mantendo sem√¢ntica.
        """
        query = query.lower().strip()

        # Stopwords comuns em portugu√™s que n√£o afetam a sem√¢ntica da query
        stopwords = [
            'qual', 'quais', 'mostre', 'me', 'gere', 'por favor', 'por gentileza',
            'poderia', 'pode', 'consegue', 'voc√™', 'o', 'a', 'os', 'as',
            'um', 'uma', 'uns', 'umas', 'de', 'da', 'do', 'das', 'dos'
        ]

        # Remover stopwords
        words = query.split()
        filtered_words = [w for w in words if w not in stopwords]
        query = ' '.join(filtered_words)

        # Normalizar varia√ß√µes comuns
        replacements = {
            'gr√°fico': 'graf',
            'gr√°ficos': 'graf',
            'grafico': 'graf',
            'graficos': 'graf',
            'ranking': 'rank',
            'rankings': 'rank',
            'top 5': 'top5',
            'top 10': 'top10',
            'top 20': 'top20',
            '√∫ltimos': 'ultimos',
            '√∫ltimo': 'ultimo',
            'an√°lise': 'analise',
            'an√°lises': 'analise',
        }

        for old, new in replacements.items():
            query = query.replace(old, new)

        # Remover espa√ßos extras
        query = ' '.join(query.split())

        return query

    def generate_and_execute_code(self, input_data: Dict[str, Any]) -> dict:
        """
        Gera, executa e retorna o resultado do c√≥digo Python para uma dada consulta.
        Esta vers√£o foi refatorada para usar diretamente o prompt fornecido e injetar uma fun√ß√£o `load_data`.
        """
        prompt = input_data.get("query", "")
        raw_data = input_data.get("raw_data", [])
        user_query = input_data.get("query", "")  # Definir no in√≠cio para evitar UnboundLocalError

        # üéØ Cache inteligente V2: Normalizar query para maior hit rate
        # Isso permite que "Mostre o ranking de papelaria" = "ranking papelaria" = "top 10 papelaria"
        normalized_query = self._normalize_query(user_query)
        query_lower = user_query.lower()
        intent_markers = []

        # Detectar tipo de an√°lise
        if any(word in query_lower for word in ['gr√°fico', 'chart', 'visualiza√ß√£o', 'plot', 'graf']):
            intent_markers.append('viz')
        if any(word in query_lower for word in ['ranking', 'top', 'rank']):
            intent_markers.append('rank')

        # Detectar segmento espec√≠fico (extrair para evitar cache cruzado)
        import re as regex_module
        segment_match = regex_module.search(r'(tecido|papelaria|armarinho|festas|artes|casa|decora√ß√£o|higiene|beleza|esporte|lazer|bazar|el√©trica|limpeza|sazonais|inform√°tica|embalagens)', query_lower)
        if segment_match:
            intent_markers.append(f'seg_{segment_match.group(1)}')

        # Gerar chave de cache √∫nica baseada em query NORMALIZADA + inten√ß√£o
        # Usar query normalizada aumenta hit rate em ~30-50%
        cache_key = hash(normalized_query + '_'.join(intent_markers) + (json.dumps(raw_data, sort_keys=True) if raw_data else ""))

        self.logger.debug(f"Cache: query_original='{user_query}' ‚Üí normalized='{normalized_query}' ‚Üí key={cache_key}")

        if cache_key in self.code_cache:
            code_to_execute = self.code_cache[cache_key]
            self.logger.info(f"C√≥digo recuperado do cache.")
        else:
            # Construir contexto com descri√ß√µes das colunas mais importantes
            important_columns = [
                "PRODUTO", "NOME", "NOMESEGMENTO", "NOMECATEGORIA", "NOMEGRUPO", "NOMESUBGRUPO",
                "NOMEFABRICANTE", "VENDA_30DD", "ESTOQUE_UNE", "LIQUIDO_38",
                "UNE", "UNE_ID", "TIPO", "EMBALAGEM", "EAN",
                # Colunas temporais para gr√°ficos de evolu√ß√£o
                "mes_01", "mes_02", "mes_03", "mes_04", "mes_05", "mes_06",
                "mes_07", "mes_08", "mes_09", "mes_10", "mes_11", "mes_12"
            ]

            column_context = "üìä COLUNAS DISPON√çVEIS:\n"
            for col in important_columns:
                if col in self.column_descriptions:
                    column_context += f"- {col}: {self.column_descriptions[col]}\n"

            # Adicionar valores v√°lidos de segmentos com mapeamento inteligente
            valid_segments = """
**VALORES V√ÅLIDOS DE SEGMENTOS (NOMESEGMENTO):**
Use EXATAMENTE estes valores no c√≥digo Python (incluindo acentos e plural/singular):

1. 'TECIDOS' ‚Üí se usu√°rio mencionar: tecido, tecidos, segmento tecido, tecidos e armarinhos
2. 'ARMARINHO E CONFEC√á√ÉO' ‚Üí se usu√°rio mencionar: armarinho, confec√ß√£o, aviamentos
3. 'PAPELARIA' ‚Üí se usu√°rio mencionar: papelaria, papel, cadernos
4. 'CASA E DECORA√á√ÉO' ‚Üí se usu√°rio mencionar: casa, decora√ß√£o, utilidades dom√©sticas
5. 'ARTES' ‚Üí se usu√°rio mencionar: artes, artesanato, pintura
6. 'SAZONAIS' ‚Üí se usu√°rio mencionar: sazonais, p√°scoa, natal, datas comemorativas
7. 'FESTAS' ‚Üí se usu√°rio mencionar: festas, anivers√°rio, bal√µes
8. 'INFORM√ÅTICA' ‚Üí se usu√°rio mencionar: inform√°tica, eletr√¥nica, computadores
9. 'HIGIENE E BELEZA' ‚Üí se usu√°rio mencionar: higiene, beleza, cosm√©ticos
10. 'ESPORTE E LAZER' ‚Üí se usu√°rio mencionar: esporte, lazer, brinquedos
11. 'EMBALAGENS E DESCART√ÅVEIS' ‚Üí se usu√°rio mencionar: embalagens, descart√°veis
12. 'BAZAR' ‚Üí se usu√°rio mencionar: bazar, utilidades
13. 'EL√âTRICA E MANUTEN√á√ÉO' ‚Üí se usu√°rio mencionar: el√©trica, manuten√ß√£o, ferramentas
14. 'MATERIAL DE LIMPEZA' ‚Üí se usu√°rio mencionar: limpeza, produtos de limpeza

**REGRA DE OURO:** Interprete a inten√ß√£o do usu√°rio e mapeie para o valor EXATO da lista acima!
"""

            # Lista de UNEs v√°lidas
            valid_unes = """
**VALORES V√ÅLIDOS DE LOJAS/UNIDADES (coluna UNE - nomes):**
Quando o usu√°rio mencionar uma loja, use EXATAMENTE estes nomes:

'SCR', 'ALC', 'DC', 'CFR', 'PET', 'VVL', 'VIL', 'REP', 'JFA', 'NIT',
'CGR', 'OBE', 'CXA', '261', 'BGU', 'ALP', 'BAR', 'CP2', 'JRD', 'NIG',
'ITA', 'MAD', 'JFJ', 'CAM', 'VRD', 'SGO', 'NFR', 'TIJ', 'ANG', 'BON',
'IPA', 'BOT', 'NIL', 'TAQ', 'RDO', '3RS', 'STS', 'NAM'

**EXEMPLOS DE MAPEAMENTO:**
- Usu√°rio diz "une mad" ou "une MAD" ‚Üí Filtrar: df[df['UNE'] == 'MAD']
- Usu√°rio diz "une 261" ‚Üí Filtrar: df[df['UNE'] == '261']
- Usu√°rio diz "une scr" ‚Üí Filtrar: df[df['UNE'] == 'SCR']
- Usu√°rio diz "une nil" ‚Üí Filtrar: df[df['UNE'] == 'NIL']

**IMPORTANTE:** A coluna 'UNE' cont√©m o NOME da loja (texto), n√£o o ID num√©rico!
Se precisar do ID num√©rico, use a coluna 'UNE_ID'.
"""

            # üéØ PILAR 2: Injetar exemplos contextuais baseados em padr√µes (Few-Shot Learning)
            examples_context = ""
            if self.pattern_matcher:
                try:
                    # Buscar padr√£o similar √† query do usu√°rio
                    matched_pattern = self.pattern_matcher.match_pattern(user_query)
                    if matched_pattern:
                        # Formatar exemplos para inje√ß√£o no prompt
                        examples_context = self.pattern_matcher.format_examples_for_prompt(matched_pattern, max_examples=2)
                        self.logger.info(f"üéØ Few-Shot Learning: Padr√£o '{matched_pattern.pattern_name}' identificado com {len(matched_pattern.examples)} exemplos")
                    else:
                        self.logger.debug("‚ÑπÔ∏è Nenhum padr√£o espec√≠fico identificado para esta query")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao buscar padr√µes: {e}")

            system_prompt = f"""Voc√™ √© um especialista em an√°lise de dados Python com pandas e interpreta√ß√£o de linguagem natural.

{column_context}

{valid_segments}

{valid_unes}

{examples_context}

**üöÄ INSTRU√á√ÉO CR√çTICA #0 - DASK DATAFRAME:**
‚ö†Ô∏è **ATEN√á√ÉO:** load_data() retorna um **Dask DataFrame** (lazy loading), N√ÉO um pandas DataFrame!

**VOC√ä DEVE:**
1. Aplicar todos os filtros no Dask DataFrame primeiro
2. Chamar `.compute()` APENAS UMA VEZ, logo ap√≥s filtros/groupby
3. Depois de `.compute()`, voc√™ ter√° um pandas DataFrame normal
4. NUNCA chamar `.compute()` m√∫ltiplas vezes ou em pandas DataFrame!

‚úÖ **CORRETO - Exemplo 1 (com filtro):**
```python
ddf = load_data()  # Dask DataFrame (lazy)
ddf_filtered = ddf[(ddf['PRODUTO'].astype(str) == '369947') & (ddf['UNE'] == 'SCR')]  # Filtro no Dask
df = ddf_filtered.compute()  # ‚úÖ Computar UMA VEZ
result = px.bar(df, x='NOME', y='VENDA_30DD')  # df √© pandas agora
```

‚úÖ **CORRETO - Exemplo 2 (com groupby):**
```python
ddf = load_data()  # Dask DataFrame (lazy)
ddf_papelaria = ddf[ddf['NOMESEGMENTO'] == 'PAPELARIA']  # Filtro no Dask
vendas_por_une = ddf_papelaria.groupby('UNE')['VENDA_30DD'].sum()  # Ainda Dask
df_result = vendas_por_une.compute().reset_index()  # ‚úÖ Computar UMA VEZ
une_mais_vendedora = df_result.sort_values(by='VENDA_30DD', ascending=False).head(1)  # pandas ops
result = une_mais_vendedora  # ‚úÖ df_result √© pandas, N√ÉO chamar .compute() de novo!
```

‚ùå **ERRADO - M√∫ltiplos .compute():**
```python
ddf = load_data()
df = ddf[ddf['NOMESEGMENTO'] == 'PAPELARIA'].compute()  # compute #1
result = df.groupby('UNE')['VENDA_30DD'].sum().compute()  # ‚ùå ERRO! df j√° √© pandas!
```

‚ùå **ERRADO - .compute() no DataFrame completo:**
```python
df = load_data().compute()  # ‚ùå ERRO: carrega 2.2M linhas na mem√≥ria!
```

**REGRA ABSOLUTA:**
- Chame `.compute()` APENAS UMA VEZ, ap√≥s todos os filtros Dask
- Depois de `.compute()`, trabalhe com pandas normalmente (SEM .compute()!)

---

**üö® INSTRU√á√ÉO CR√çTICA #1 - TRATAMENTO DE VALORES NA/NULL:**
‚ö†Ô∏è **ATEN√á√ÉO:** Colunas do Parquet podem conter valores NA (null/NaN) que causam erros!

**VOC√ä DEVE:**
1. SEMPRE preencher ou remover NA ANTES de compara√ß√µes
2. NUNCA usar `.apply()` com lambdas que comparam valores (use opera√ß√µes vetorizadas!)
3. Se precisar de `.apply()`, forne√ßa `meta=` e trate NA na fun√ß√£o

‚ùå **ERRADO - Causa erro 'boolean value of NA is ambiguous':**
```python
ddf = load_data()
# Compara√ß√£o direta com NA causa erro
ddf['flag'] = ddf.apply(lambda row: row['exposicao_minima'] < row['VENDA_30DD'], axis=1)
```

‚úÖ **CORRETO - Op√ß√£o 1 (PREFERIDA - mais r√°pida):**
```python
ddf = load_data()
# Preencher NA com 0 ANTES de comparar
ddf['exposicao_minima'] = ddf['exposicao_minima'].fillna(0)
ddf['VENDA_30DD'] = ddf['VENDA_30DD'].fillna(0)
# Opera√ß√£o vetorizada (SEM apply!)
ddf['flag'] = ddf['exposicao_minima'] < ddf['VENDA_30DD']
df = ddf.compute()
result = df
```

‚úÖ **CORRETO - Op√ß√£o 2 (remover NA):**
```python
ddf = load_data()
# Remover linhas com NA nas colunas relevantes
ddf = ddf.dropna(subset=['exposicao_minima', 'VENDA_30DD'])
ddf['flag'] = ddf['exposicao_minima'] < ddf['VENDA_30DD']
df = ddf.compute()
result = df
```

‚úÖ **CORRETO - Op√ß√£o 3 (apenas se apply for REALMENTE necess√°rio):**
```python
ddf = load_data()
# Usar apply com meta= e tratamento de NA
ddf['flag'] = ddf.apply(
    lambda row: (
        row['exposicao_minima'] < row['VENDA_30DD']
        if pd.notna(row['exposicao_minima']) and pd.notna(row['VENDA_30DD'])
        else False
    ),
    axis=1,
    meta=('flag', 'bool')  # OBRIGAT√ìRIO!
)
df = ddf.compute()
result = df
```

**REGRA DE OURO:** Sempre use opera√ß√µes vetorizadas (op√ß√£o 1). Evite `.apply()` sempre que poss√≠vel!

**COLUNAS COMUNS COM NA:**
- `exposicao_minima` - pode ter NA
- `ESTOQUE_UNE` - pode ter NA
- Colunas de vendas mensais (`mes_01` a `mes_12`) - podem ter NA

**ANTES DE QUALQUER COMPARA√á√ÉO:**
```python
# Sempre preencher NA nas colunas que vai usar
ddf['coluna1'] = ddf['coluna1'].fillna(0)
ddf['coluna2'] = ddf['coluna2'].fillna(0)
# Agora pode comparar com seguran√ßa
ddf['resultado'] = ddf['coluna1'] > ddf['coluna2']
```

---

**INSTRU√á√ïES CR√çTICAS:**
1. **INTERPRETA√á√ÉO INTELIGENTE**: Se o usu√°rio mencionar "tecido" (singular), voc√™ DEVE usar 'TECIDOS' (plural) no c√≥digo!
2. **MAPEAMENTO AUTOM√ÅTICO**: Use a lista de valores v√°lidos acima para mapear termos do usu√°rio ‚Üí valores exatos do banco
3. **NOMES DE COLUNAS**: Use sempre MAI√öSCULAS conforme listado
4. **ACENTOS**: Mantenha acentua√ß√£o exata (CONFEC√á√ÉO, DECORA√á√ÉO, INFORM√ÅTICA, etc.)
5. **VENDAS**: Sempre use VENDA_30DD para m√©tricas de vendas
6. **ESTOQUE**: Use ESTOQUE_UNE para estoque
7. **USE OS EXEMPLOS ACIMA** como refer√™ncia se foram fornecidos!

**‚ö†Ô∏è DETEC√á√ÉO DE RUPTURA:**
Se o usu√°rio perguntar sobre "ruptura", "produtos em falta", "estoque zero":
- Ruptura significa ESTOQUE_UNE == 0 OU ESTOQUE_UNE < exposicao_minima
- Para identificar segmentos com ruptura: agrupe por NOMESEGMENTO onde ESTOQUE_UNE <= 0
- Exemplo: `df[df['ESTOQUE_UNE'] <= 0].groupby('NOMESEGMENTO')['PRODUTO'].count()`

**REGRAS PARA RANKINGS/TOP N:**
- Se a pergunta mencionar "ranking", "top", "maior", "mais vendido" ‚Üí voc√™ DEVE fazer groupby + sum + sort_values
- Se mencionar "top 10", "top 5" ‚Üí adicione .head(N) ou .nlargest(N) ANTES de criar gr√°fico
- SEMPRE agrupe por NOME (nome do produto) para rankings de produtos
- SEMPRE ordene por VENDA_30DD (vendas em 30 dias) de forma DECRESCENTE (ascending=False)
- **üö® CR√çTICO:** SEMPRE use `.reset_index()` ap√≥s `.groupby().sum()` ou `.groupby().agg()` ANTES de chamar `.sort_values()`
- **IMPORTANTE:** N√ÉO retorne apenas o filtro! Sempre fa√ßa o groupby quando houver ranking/top!

**‚ö†Ô∏è REGRA ANTI-ERRO SERIES:**
Ao fazer agrega√ß√µes (groupby + sum/mean/count), SEMPRE use `.reset_index()` ANTES de `.sort_values()`:
```python
# ‚ùå ERRADO: Series n√£o tem .sort_values() confi√°vel
result = df.groupby('NOME')['VENDA_30DD'].sum().sort_values()

# ‚úÖ CORRETO: Converter para DataFrame primeiro
result = df.groupby('NOME')['VENDA_30DD'].sum().reset_index().sort_values(by='VENDA_30DD', ascending=False)
```

**üéØ DETEC√á√ÉO DE GR√ÅFICOS - REGRA ABSOLUTA:**
Se o usu√°rio mencionar qualquer uma destas palavras-chave, voc√™ DEVE gerar um gr√°fico Plotly:
- Palavras-chave visuais: "gr√°fico", "chart", "visualiza√ß√£o", "plotar", "plot", "barras", "pizza", "linhas", "scatter"
- Palavras-chave anal√≠ticas: "ranking", "top N", "top 10", "maiores", "menores", "compara√ß√£o"

**‚ö†Ô∏è REGRA CR√çTICA - GR√ÅFICOS PLOTLY:**
Quando gerar gr√°ficos Plotly (px.bar, px.pie, px.line):
1. Filtre e limite os dados (.nlargest, .head, filtros) ANTES de criar o gr√°fico
2. NUNCA use .head() ou .nlargest() DEPOIS de px.bar/px.pie/px.line
3. A vari√°vel result deve conter o objeto Figure diretamente

‚ùå ERRADO (causa erro 'Figure' object has no attribute 'head'):
```python
df_top = df.nlargest(10, 'VENDA_30DD')
result = px.bar(df_top, x='NOME', y='VENDA_30DD')
result = result.head(10)  # ‚ùå Figure n√£o tem .head()!
```

‚úÖ CORRETO:
```python
df_top = df.nlargest(10, 'VENDA_30DD')  # Limite ANTES
result = px.bar(df_top, x='NOME', y='VENDA_30DD')  # result √© Figure
```

**TIPOS DE GR√ÅFICOS DISPON√çVEIS:**
- px.bar() - Gr√°fico de barras (use para rankings, compara√ß√µes)
- px.pie() - Gr√°fico de pizza (use para propor√ß√µes)
- px.line() - Gr√°fico de linhas (use para tend√™ncias temporais)
- px.scatter() - Gr√°fico de dispers√£o (use para correla√ß√µes)

**EXEMPLOS COMPLETOS DE GR√ÅFICOS:**

1. **Gr√°fico de Barras - Top 10:**
```python
df = load_data()
df_filtered = df[df['NOMESEGMENTO'] == 'TECIDOS']
df_top10 = df_filtered.nlargest(10, 'VENDA_30DD')
result = px.bar(df_top10, x='NOME', y='VENDA_30DD', title='Top 10 Produtos - Tecidos')
```

2. **Gr√°fico de Pizza - Distribui√ß√£o por Segmento:**
```python
df = load_data()
vendas_por_segmento = df.groupby('NOMESEGMENTO')['VENDA_30DD'].sum().reset_index()
result = px.pie(vendas_por_segmento, names='NOMESEGMENTO', values='VENDA_30DD', title='Vendas por Segmento')
```

3. **Gr√°fico de Barras - Compara√ß√£o de Grupos:**
```python
df = load_data()
papelaria = df[df['NOMESEGMENTO'] == 'PAPELARIA']
vendas_por_grupo = papelaria.groupby('NOMEGRUPO')['VENDA_30DD'].sum().sort_values(ascending=False).head(5).reset_index()
result = px.bar(vendas_por_grupo, x='NOMEGRUPO', y='VENDA_30DD', title='Top 5 Grupos - Papelaria')
```

**üìä GR√ÅFICOS DE EVOLU√á√ÉO TEMPORAL (MUITO IMPORTANTE!):**

Quando o usu√°rio pedir "evolu√ß√£o", "tend√™ncia", "ao longo do tempo", "nos √∫ltimos N meses", "mensais":

‚úÖ **USE AS COLUNAS mes_01 a mes_12** para criar gr√°ficos de linha mostrando evolu√ß√£o temporal!

**IMPORTANTE:**
- mes_01 = m√™s mais recente
- mes_12 = m√™s mais antigo (12 meses atr√°s)
- Os valores s√£o NUM√âRICOS (vendas do m√™s)

**EXEMPLO COMPLETO - Evolu√ß√£o de Vendas (6 meses):**
```python
ddf = load_data()
# Filtrar produto espec√≠fico
ddf_filtered = ddf[ddf['PRODUTO'].astype(str) == '369947']
df = ddf_filtered.compute()

# Preparar dados temporais (6 meses mais recentes)
import pandas as pd
temporal_data = pd.DataFrame({{
    'M√™s': ['M√™s 6', 'M√™s 5', 'M√™s 4', 'M√™s 3', 'M√™s 2', 'M√™s 1'],
    'Vendas': [
        df['mes_06'].sum(),
        df['mes_05'].sum(),
        df['mes_04'].sum(),
        df['mes_03'].sum(),
        df['mes_02'].sum(),
        df['mes_01'].sum()
    ]
}})

result = px.line(temporal_data, x='M√™s', y='Vendas',
                 title='Evolu√ß√£o de Vendas - √öltimos 6 Meses',
                 markers=True)
```

**EXEMPLO - Evolu√ß√£o de Vendas por Segmento (12 meses):**
```python
ddf = load_data()
ddf_filtered = ddf[ddf['NOMESEGMENTO'] == 'TECIDOS']
df = ddf_filtered.compute()

import pandas as pd
meses = ['M√™s 12', 'M√™s 11', 'M√™s 10', 'M√™s 9', 'M√™s 8', 'M√™s 7',
         'M√™s 6', 'M√™s 5', 'M√™s 4', 'M√™s 3', 'M√™s 2', 'M√™s 1']
vendas = [
    df['mes_12'].sum(), df['mes_11'].sum(), df['mes_10'].sum(),
    df['mes_09'].sum(), df['mes_08'].sum(), df['mes_07'].sum(),
    df['mes_06'].sum(), df['mes_05'].sum(), df['mes_04'].sum(),
    df['mes_03'].sum(), df['mes_02'].sum(), df['mes_01'].sum()
]

temporal_data = pd.DataFrame({{'M√™s': meses, 'Vendas': vendas}})
result = px.line(temporal_data, x='M√™s', y='Vendas',
                 title='Evolu√ß√£o Mensal - Tecidos',
                 markers=True)
```

**REGRA:** Se usu√°rio pedir "√∫ltimos N meses", use mes_01 at√© mes_N (do mais recente ao mais antigo).

**MAPEAMENTO OBRIGAT√ìRIO DE SEGMENTOS:**
IMPORTANTE: O usu√°rio pode usar termos no singular ou simplificados. Voc√™ DEVE usar os valores EXATOS da base de dados:

- Usu√°rio diz: "tecido" ou "tecidos" ‚Üí Voc√™ usa: df[df['NOMESEGMENTO'] == 'TECIDOS']
- Usu√°rio diz: "papelaria" ‚Üí Voc√™ usa: df[df['NOMESEGMENTO'] == 'PAPELARIA']
- Usu√°rio diz: "armarinho" ou "confec√ß√£o" ‚Üí Voc√™ usa: df[df['NOMESEGMENTO'] == 'ARMARINHO E CONFEC√á√ÉO']
- Usu√°rio diz: "limpeza" ‚Üí Voc√™ usa: df[df['NOMESEGMENTO'] == 'MATERIAL DE LIMPEZA']
- Usu√°rio diz: "casa" ou "decora√ß√£o" ‚Üí Voc√™ usa: df[df['NOMESEGMENTO'] == 'CASA E DECORA√á√ÉO']
- Usu√°rio diz: "festas" ‚Üí Voc√™ usa: df[df['NOMESEGMENTO'] == 'FESTAS']
- Usu√°rio diz: "higiene" ou "beleza" ‚Üí Voc√™ usa: df[df['NOMESEGMENTO'] == 'HIGIENE E BELEZA']
- Usu√°rio diz: "brinquedo" ou "brinquedos" ‚Üí Voc√™ usa: df[df['NOMESEGMENTO'] == 'BRINQUEDOS']
- Usu√°rio diz: "alimento" ou "alimentos" ‚Üí Voc√™ usa: df[df['NOMESEGMENTO'] == 'ALIMENTOS']
- Usu√°rio diz: "doce" ou "doces" ‚Üí Voc√™ usa: df[df['NOMESEGMENTO'] == 'DOCES E SALGADOS']

‚ö†Ô∏è NUNCA use .str.upper() ou .str.contains() em compara√ß√µes de segmento - use apenas == com o valor EXATO!

**üöÄ OTIMIZA√á√ÉO DE PERFORMANCE - PREDICATE PUSHDOWN:**
Quando houver filtros espec√≠ficos (segmento, UNE, produto), aplique os filtros O MAIS CEDO POSS√çVEL no c√≥digo:

‚úÖ **EFICIENTE (Predicate Pushdown):**
```python
df = load_data()
# Filtra IMEDIATAMENTE ap√≥s carregar (menos mem√≥ria, mais r√°pido)
df = df[df['NOMESEGMENTO'] == 'TECIDOS']
# Agora trabalha com dataset reduzido
df_top10 = df.nlargest(10, 'VENDA_30DD')
result = px.bar(df_top10, x='NOME', y='VENDA_30DD')
```

‚ùå **INEFICIENTE (Sem pushdown):**
```python
df = load_data()  # Carrega tudo (lento)
# Processa dataset inteiro
df_sorted = df.sort_values('VENDA_30DD', ascending=False)
# Filtra tarde demais
df_filtered = df_sorted[df_sorted['NOMESEGMENTO'] == 'TECIDOS'].head(10)
```

**REGRA:** Se a query mencionar filtros espec√≠ficos (segmento, UNE, categoria), aplique-os na PRIMEIRA LINHA ap√≥s load_data()!

Siga as instru√ß√µes do usu√°rio E fa√ßa o mapeamento inteligente de termos!"""

            # üöÄ PILAR 4: Adicionar avisos din√¢micos baseados em erros recentes
            if self.dynamic_prompt:
                try:
                    enhanced_prompt = self.dynamic_prompt.get_enhanced_prompt()
                    # Adicionar avisos ao system_prompt
                    system_prompt = system_prompt + "\n\n" + enhanced_prompt
                    self.logger.info("‚úÖ Prompt enriquecido com DynamicPrompt (Pilar 4)")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao enriquecer prompt: {e}")

            # O agente agora usa o prompt diretamente, sem construir um novo.
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]

            start_llm_query = time.time()
            llm_response = self.llm.get_completion(messages=messages)
            end_llm_query = time.time()
            self.logger.info(f"Tempo de consulta LLM: {end_llm_query - start_llm_query:.4f} segundos")

            if "error" in llm_response:
                self.logger.error(f"Erro ao obter resposta do LLM: {llm_response['error']}")
                return {"type": "error", "output": "N√£o foi poss√≠vel gerar o c√≥digo de an√°lise."}

            code_to_execute = self._extract_python_code(llm_response.get("content", ""))

            if not code_to_execute:
                self.logger.warning("Nenhum c√≥digo Python foi gerado pelo LLM.")
                return {"type": "text", "output": "N√£o consegui gerar um script para responder √† sua pergunta."}

            # üöÄ QUICK WIN 1: Validar e corrigir Top N automaticamente
            # user_query j√° foi definido no in√≠cio da fun√ß√£o
            code_to_execute = self._validate_top_n(code_to_execute, user_query)

            # ‚úÖ FASE 1: Validar c√≥digo antes de executar
            validation_result = self.code_validator.validate(code_to_execute, user_query)

            if not validation_result['valid']:
                self.logger.warning(f"‚ö†Ô∏è C√≥digo com problemas: {validation_result['errors']}")

                # Tentar corre√ß√£o autom√°tica
                fix_result = self.code_validator.auto_fix(validation_result, user_query)

                if fix_result['fixed']:
                    self.logger.info(f"‚úÖ C√≥digo corrigido automaticamente: {fix_result['fixes_applied']}")
                    code_to_execute = fix_result['code']
                else:
                    self.logger.warning(f"‚ö†Ô∏è Corre√ß√£o autom√°tica falhou. Erros restantes: {fix_result.get('remaining_errors', [])}")
                    # Continuar mesmo assim, mas com log

            # Valida√ß√µes adicionais com warnings (n√£o bloqueiam execu√ß√£o)
            if validation_result.get('warnings'):
                self.logger.info(f"‚ÑπÔ∏è Avisos: {validation_result['warnings']}")

            if validation_result.get('suggestions'):
                self.logger.debug(f"üí° Sugest√µes: {validation_result['suggestions']}")

            self.code_cache[cache_key] = code_to_execute

        self.logger.info(f"\nC√≥digo a ser executado:\n---\n{code_to_execute}\n---")

        try:
            # ‚ö†Ô∏è IMPORTANTE: Reutilizar a fun√ß√£o load_data() definida em _execute_generated_code
            # que j√° usa Dask e l√™ TODOS os arquivos Parquet (*.parquet)

            local_scope = {
                "pd": pd,
                "px": px,
                "result": None,
                "df_raw_data": pd.DataFrame(raw_data) if raw_data else None,
                # load_data ser√° injetado em _execute_generated_code
            }
            
            px.defaults.template = "plotly_white"

            start_code_execution = time.time()
            result = self._execute_generated_code(code_to_execute, local_scope)
            end_code_execution = time.time()
            self.logger.info(f"Tempo de execu√ß√£o do c√≥digo: {end_code_execution - start_code_execution:.4f} segundos")

            # ‚ö†Ô∏è VALIDA√á√ÉO CR√çTICA: Verificar se resultado √© Dask n√£o computado
            if hasattr(result, '_name') and 'dask' in str(type(result)).lower():
                self.logger.error(f"‚ùå ERRO: C√≥digo retornou Dask object n√£o computado: {type(result)}")
                self.logger.error(f"   O c√≥digo gerado deve chamar .compute() antes de retornar o resultado!")
                return {
                    "type": "error",
                    "output": "Erro interno: O c√≥digo gerou um resultado Dask n√£o computado. Tentando novamente..."
                }

            # An√°lise do tipo de resultado
            if isinstance(result, pd.DataFrame):
                self.logger.info(f"Resultado: DataFrame com {len(result)} linhas.")
                # üöÄ QUICK WIN 2: Registrar query bem-sucedida
                self._log_successful_query(user_query, code_to_execute, len(result))
                return {"type": "dataframe", "output": result}
            elif isinstance(result, pd.Series):
                self.logger.info(f"Resultado: Series com {len(result)} elementos.")
                # Converter Series para DataFrame para consist√™ncia
                result_df = result.reset_index()
                self._log_successful_query(user_query, code_to_execute, len(result_df))
                return {"type": "dataframe", "output": result_df}
            elif 'plotly' in str(type(result)):
                self.logger.info(f"Resultado: Gr√°fico Plotly.")
                # üöÄ QUICK WIN 2: Registrar query bem-sucedida (gr√°fico)
                self._log_successful_query(user_query, code_to_execute, 1)
                return {"type": "chart", "output": pio.to_json(result)}
            else:
                self.logger.info(f"Resultado: Texto.")
                return {"type": "text", "output": str(result)}
        
        except TimeoutError as e:
            self.logger.error("A execu√ß√£o do c√≥digo excedeu o tempo limite.")
            # üöÄ QUICK WIN 3: Registrar erro
            self._log_error(user_query, code_to_execute, "timeout", str(e))
            return {"type": "error", "output": "A an√°lise demorou muito e foi interrompida."}
        except Exception as e:
            error_msg = str(e)
            error_type = type(e).__name__

            # üîÑ AUTO-RECOVERY: Detectar erros comuns e limpar cache
            should_retry = False

            if "'DataFrame' object has no attribute 'compute'" in error_msg or \
               "'Series' object has no attribute 'compute'" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo com .compute() inv√°lido")

            elif "boolean value of NA is ambiguous" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo sem tratamento de NA")

            elif "Invalid comparison between dtype=" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo sem convers√£o de tipos")

            elif "'Series' object has no attribute 'sort_values'" in error_msg or \
                 "AttributeError: 'Series'" in error_msg:
                should_retry = True
                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo com erro em Series (falta .reset_index()?)")

            if should_retry:

                self.logger.warning(f"‚ö†Ô∏è Detectado c√≥digo com .compute() inv√°lido em pandas object")
                self.logger.info(f"üîÑ Limpando cache e tentando novamente com prompt atualizado...")

                # Limpar apenas o cache desta query espec√≠fica
                if cache_key in self.code_cache:
                    del self.code_cache[cache_key]
                    self.logger.info(f"‚úÖ Cache da query removido: {cache_key[:50]}...")

                # Tentar novamente (recursivo) - APENAS UMA VEZ
                if not hasattr(self, '_retry_flag'):
                    self._retry_flag = True
                    try:
                        result = self.generate_and_execute_code(user_query, raw_data, **kwargs)
                        return result
                    finally:
                        delattr(self, '_retry_flag')
                else:
                    self.logger.error(f"‚ùå Retry falhou. Erro persistente ap√≥s limpeza de cache.")

            self.logger.error(f"Erro ao executar o c√≥digo gerado: {e}", exc_info=True)
            # üöÄ QUICK WIN 3: Registrar erro
            self._log_error(user_query, code_to_execute, error_type, error_msg)
            return {"type": "error", "output": f"Ocorreu um erro ao executar a an√°lise: {error_msg}"}
    def _extract_python_code(self, text: str) -> str | None:
        """Extrai o bloco de c√≥digo Python da resposta do LLM."""
        match = re.search(r'```python\n(.*)```', text, re.DOTALL)
        return match.group(1).strip() if match else None

    # üöÄ QUICK WIN METHODS
    def _validate_top_n(self, code: str, user_query: str) -> str:
        """
        QUICK WIN 1: Valida se c√≥digo tem .head(N) quando usu√°rio pede 'top N'.
        Corrige automaticamente se necess√°rio.
        """
        query_lower = user_query.lower()

        # Verificar se usu√°rio pediu "top N"
        top_match = re.search(r'top\s+(\d+)', query_lower)

        # ‚úÖ N√ÉO adicionar .head() se o c√≥digo est√° gerando um gr√°fico Plotly
        # Gr√°ficos j√° devem ter o filtro aplicado antes do px.bar/px.pie/etc
        is_plotly_chart = any(func in code for func in ['px.bar(', 'px.pie(', 'px.line(', 'px.scatter(', 'px.histogram('])

        if top_match and '.head(' not in code and not is_plotly_chart:
            n = top_match.group(1)
            self.logger.warning(f"‚ö†Ô∏è Query pede top {n} mas c√≥digo n√£o tem .head(). Corrigindo automaticamente...")

            # Tentar adicionar .head(N) antes de .reset_index()
            if '.reset_index()' in code:
                code = code.replace('.reset_index()', f'.head({n}).reset_index()')
            # Ou antes do resultado final
            elif 'result = ' in code:
                # Encontrar a √∫ltima atribui√ß√£o a result
                lines = code.split('\n')
                for i in range(len(lines) - 1, -1, -1):
                    if lines[i].strip().startswith('result = '):
                        # Adicionar .head(N) se ainda n√£o existir
                        if '.head(' not in lines[i]:
                            lines[i] = lines[i].replace('result = ', f'result = ').rstrip()
                            if not lines[i].endswith(')'):
                                lines[i] = f"{lines[i]}.head({n})"
                        break
                code = '\n'.join(lines)

            self.logger.info(f"‚úÖ C√≥digo corrigido automaticamente com .head({n})")
        elif is_plotly_chart:
            self.logger.info(f"‚ÑπÔ∏è C√≥digo gera gr√°fico Plotly - n√£o adicionando .head() autom√°tico")

        return code

    def _log_successful_query(self, user_query: str, code: str, result_rows: int):
        """
        QUICK WIN 2: Registra queries bem-sucedidas para an√°lise futura.
        """
        from datetime import datetime

        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'query': user_query,
            'code': code,
            'rows': result_rows,
            'success': True
        }

        # Salvar em arquivo di√°rio
        date_str = datetime.now().strftime('%Y%m%d')
        log_file = os.path.join(self.logs_dir, f'successful_queries_{date_str}.jsonl')

        try:
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
            self.logger.debug(f"‚úÖ Query registrada em {log_file}")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao registrar query: {e}")

    def _log_error(self, user_query: str, code: str, error_type: str, error_message: str):
        """
        QUICK WIN 3: Registra erros por tipo para an√°lise de padr√µes.
        """
        from datetime import datetime

        # Incrementar contador
        self.error_counts[error_type] += 1

        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'query': user_query,
            'code': code,
            'error_type': error_type,
            'error_message': str(error_message),
            'success': False
        }

        # Salvar em arquivo di√°rio
        date_str = datetime.now().strftime('%Y%m%d')
        log_file = os.path.join(self.logs_dir, f'error_log_{date_str}.jsonl')

        try:
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')

            # Tamb√©m salvar contador consolidado
            counter_file = os.path.join(self.logs_dir, f'error_counts_{date_str}.json')
            with open(counter_file, 'w', encoding='utf-8') as f:
                json.dump(dict(self.error_counts), f, indent=2, ensure_ascii=False)

            self.logger.debug(f"‚ö†Ô∏è Erro registrado: {error_type} (total: {self.error_counts[error_type]})")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao registrar erro: {e}")

    def _clean_old_cache(self, max_age_hours=2):
        """Limpa cache antigo automaticamente (padr√£o: 2 horas)"""
        import os
        import time
        from pathlib import Path

        try:
            cache_dirs = [
                Path('data/cache'),
                Path('data/cache_agent_graph')
            ]

            now = time.time()
            max_age = max_age_hours * 60 * 60  # Converte horas para segundos
            removed_count = 0

            for cache_dir in cache_dirs:
                if not cache_dir.exists():
                    continue

                for cache_file in cache_dir.glob('*'):
                    if cache_file.is_file():
                        file_age = now - cache_file.stat().st_mtime
                        if file_age > max_age:
                            cache_file.unlink()
                            removed_count += 1

            if removed_count > 0:
                self.logger.info(f"üßπ Cache limpo: {removed_count} arquivos removidos (> 24h)")

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao limpar cache: {e}")

    def _check_and_invalidate_cache_if_prompt_changed(self):
        """
        üîÑ VERSIONING DE CACHE: Invalida cache se o prompt mudou

        Calcula hash do prompt atual e compara com o hash salvo.
        Se diferente, limpa o cache para for√ßar regenera√ß√£o com novo prompt.
        """
        import hashlib
        from pathlib import Path
        import json

        try:
            # Calcular hash do prompt atual (baseado em column_descriptions + segmentos v√°lidos)
            prompt_components = {
                'columns': list(self.column_descriptions.keys()),
                'descriptions': list(self.column_descriptions.values()),
                # Adicionar outros componentes que afetam o prompt
                'version': '2.0_temporal_fix'  # Incrementar quando houver mudan√ßas significativas
            }

            prompt_str = json.dumps(prompt_components, sort_keys=True)
            current_hash = hashlib.md5(prompt_str.encode()).hexdigest()

            # Arquivo para armazenar hash do prompt
            version_file = Path('data/cache/.prompt_version')

            # Verificar se h√° vers√£o anterior
            if version_file.exists():
                try:
                    with open(version_file, 'r') as f:
                        saved_hash = f.read().strip()

                    if saved_hash != current_hash:
                        # PROMPT MUDOU! Limpar cache
                        self.logger.warning(f"‚ö†Ô∏è  PROMPT MUDOU! Limpando cache para for√ßar regenera√ß√£o...")
                        self.logger.info(f"   Hash anterior: {saved_hash}")
                        self.logger.info(f"   Hash novo: {current_hash}")

                        # Limpar todos os caches
                        cache_dirs = [
                            Path('data/cache'),
                            Path('data/cache_agent_graph')
                        ]

                        removed_count = 0
                        for cache_dir in cache_dirs:
                            if cache_dir.exists():
                                for cache_file in cache_dir.glob('*'):
                                    if cache_file.is_file() and cache_file.name != '.prompt_version':
                                        cache_file.unlink()
                                        removed_count += 1

                        self.logger.info(f"‚úÖ Cache invalidado: {removed_count} arquivos removidos")

                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao ler vers√£o do cache: {e}")

            # Salvar hash atual
            version_file.parent.mkdir(parents=True, exist_ok=True)
            with open(version_file, 'w') as f:
                f.write(current_hash)

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao verificar vers√£o do cache: {e}")
