# Solu√ß√£o para Transfer√™ncias no Streamlit Cloud

## Problema Identificado

Os arquivos Parquet (`admmat.parquet` e `admmat_extended.parquet`) s√£o muito grandes (94-100 MB) e est√£o no `.gitignore`, portanto **n√£o sobem para o Streamlit Cloud**.

## Solu√ß√£o Implementada

### 1. Fallback Autom√°tico de Arquivos

O c√≥digo agora detecta automaticamente qual arquivo est√° dispon√≠vel:

```python
# Tenta usar extended, se n√£o existir usa o padr√£o
if os.path.exists(PARQUET_PATH_EXTENDED):
    PARQUET_PATH = PARQUET_PATH_EXTENDED  # Local development
elif os.path.exists(PARQUET_PATH_DEFAULT):
    PARQUET_PATH = PARQUET_PATH_DEFAULT   # Streamlit Cloud
```

### 2. Mapeamento de Colunas

Como `admmat.parquet` tem nomes de colunas diferentes:
- `estoque_lv` ‚Üí `linha_verde`
- `media_considerada_lv` ‚Üí `mc`

Foi implementada fun√ß√£o `_normalize_dataframe()` que faz o mapeamento automaticamente.

### 3. Compatibilidade Total

Ambas as ferramentas agora funcionam com qualquer um dos arquivos:
- ‚úÖ `validar_transferencia_produto()`
- ‚úÖ `sugerir_transferencias_automaticas()`

## Op√ß√µes para Streamlit Cloud

### Op√ß√£o 1: Usar Parquet (Atual) ‚ö†Ô∏è

**Pr√≥s:**
- J√° implementado e funcionando
- R√°pido (dados em cache)

**Contras:**
- Arquivo grande (94 MB)
- Dados podem ficar desatualizados
- Requer upload manual do arquivo

**Como fazer:**
```bash
# Remover apenas extended do .gitignore (manter admmat.parquet)
# Editar .gitignore linha 63:
# De:
data/parquet/

# Para:
data/parquet/admmat_extended.parquet
data/parquet_cleaned/
```

### Op√ß√£o 2: Usar SQL Server Direto (Recomendado) ‚úÖ

**Pr√≥s:**
- Dados sempre atualizados
- N√£o ocupa espa√ßo no reposit√≥rio
- Mesma fonte que HybridAdapter usa

**Contras:**
- Requer conex√£o com banco
- Um pouco mais lento

**Implementa√ß√£o:**

Criar `core/tools/une_tools_sql.py` que usa o mesmo SQL Server do `HybridAdapter`:

```python
from core.connectivity.hybrid_adapter import HybridDataAdapter

def _get_data_from_sql(filters: dict = None):
    """Busca dados do SQL Server"""
    adapter = HybridDataAdapter()
    # Usar m√©todo interno do adapter
    df = adapter.execute_query(filters or {})
    return df
```

### Op√ß√£o 3: GitHub LFS (Large File Storage) üí°

**Pr√≥s:**
- Git gerencia o arquivo
- Dados no reposit√≥rio
- Deploy autom√°tico

**Contras:**
- Requer configura√ß√£o LFS
- Limites de tamanho/bandwidth

**Como fazer:**
```bash
# Instalar Git LFS
git lfs install

# Rastrear arquivos Parquet
git lfs track "data/parquet/admmat.parquet"

# Adicionar e comitar
git add .gitattributes
git add data/parquet/admmat.parquet
git commit -m "Add Parquet via LFS"
git push
```

## Recomenda√ß√£o Final

**Para PRODU√á√ÉO:** Use **Op√ß√£o 2 (SQL Server direto)**

Vantagens:
1. Dados sempre atualizados em tempo real
2. N√£o adiciona peso ao reposit√≥rio
3. Usa mesma infraestrutura que j√° funciona
4. Mais profissional e escal√°vel

**Para DESENVOLVIMENTO:** Mantenha Parquet local para testes r√°pidos

## Pr√≥ximos Passos

Se quiser implementar Op√ß√£o 2 (SQL), posso criar:

1. `une_tools_sql.py` - Vers√£o que usa SQL direto
2. Configura√ß√£o autom√°tica de fallback: Parquet local ‚Üí SQL em cloud
3. Cache inteligente das consultas SQL
4. Testes de integra√ß√£o

**O que prefere?**
