# Plano de Implementa√ß√£o Cir√∫rgica (PIC) - Agent_BI Refactoring

**Vers√£o:** 1.0
**Data de Cria√ß√£o:** 21 de Novembro de 2025
**Escopo:** Refatora√ß√£o completa do projeto `Agents_Solution_BI` para implementar Governan√ßa de Prompts (CO-STAR), Seguran√ßa de Dados (PII Masking) e Experi√™ncia de Usu√°rio (Streaming)
**N√≠vel de Precis√£o:** Cir√∫rgico (Instru√ß√µes linha por linha)

---

## üìã √çNDICE

1. [Vis√£o Geral e Contexto](#vis√£o-geral-e-contexto)
2. [Estrutura de Arquivos Esperada](#estrutura-de-arquivos-esperada)
3. [Pilar 1: Governan√ßa de Prompts (CO-STAR)](#pilar-1-governan√ßa-de-prompts-co-star)
4. [Pilar 2: Seguran√ßa de Dados (PII Masking)](#pilar-2-seguran√ßa-de-dados-pii-masking)
5. [Pilar 3: Experi√™ncia de Usu√°rio (Streaming)](#pilar-3-experi√™ncia-de-usu√°rio-streaming)
6. [Testes e Valida√ß√£o](#testes-e-valida√ß√£o)
7. [Crit√©rio de Sucesso](#crit√©rio-de-sucesso)

---

## Vis√£o Geral e Contexto

### Objetivo Geral
Refatorar o projeto `Agent_BI` (localizado em `/home/ubuntu/Agents_Solution_BI`) para implementar tr√™s pilares de melhoria:
1. **Governan√ßa de Prompts:** Estrutura√ß√£o robusta usando o paradigma CO-STAR
2. **Seguran√ßa de Dados:** Mascaramento de PII antes/depois da chamada ao LLM
3. **Experi√™ncia de Usu√°rio:** Streaming de respostas sem indicadores de carregamento bloqueantes

### Restri√ß√µes Cr√≠ticas
- **N√£o quebrar a funcionalidade existente:** Todas as altera√ß√µes devem ser retrocompat√≠veis
- **N√£o remover c√≥digo:** Apenas adicionar, refatorar ou estender
- **N√£o alterar depend√™ncias:** Usar apenas bibliotecas j√° presentes no projeto
- **Manter a arquitetura atual:** Streamlit como frontend, Python como backend

### Arquitetura de Alto N√≠vel
```
Agents_Solution_BI/
‚îú‚îÄ‚îÄ streamlit_app.py (FRONTEND - Ser√° modificado)
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompt_loader.py (Ser√° modificado)
‚îÇ   ‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt_analise.md (Ser√° modificado)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt_desambiguacao.md (NOVO - Ser√° criado)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [outros prompts]
‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py (Assumido - Ser√° criado/modificado)
‚îÇ   ‚îú‚îÄ‚îÄ security/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_masking.py (NOVO - Ser√° criado)
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îî‚îÄ‚îÄ logging_config.py (Existente - N√£o modificar)
‚îî‚îÄ‚îÄ [outros diret√≥rios]
```

---

## Estrutura de Arquivos Esperada

Antes de iniciar a implementa√ß√£o, verifique se os seguintes arquivos existem:

```json
{
  "arquivos_esperados": [
    {
      "caminho": "/home/ubuntu/Agents_Solution_BI/streamlit_app.py",
      "tipo": "Python",
      "status": "DEVE EXISTIR",
      "descri√ß√£o": "Arquivo principal do Streamlit"
    },
    {
      "caminho": "/home/ubuntu/Agents_Solution_BI/core/agents/prompt_loader.py",
      "tipo": "Python",
      "status": "DEVE EXISTIR",
      "descri√ß√£o": "Carregador de prompts"
    },
    {
      "caminho": "/home/ubuntu/Agents_Solution_BI/core/prompts/",
      "tipo": "Diret√≥rio",
      "status": "DEVE EXISTIR",
      "descri√ß√£o": "Diret√≥rio de templates de prompts"
    },
    {
      "caminho": "/home/ubuntu/Agents_Solution_BI/core/llm_service.py",
      "tipo": "Python",
      "status": "PODE N√ÉO EXISTIR",
      "descri√ß√£o": "Servi√ßo de LLM (ser√° criado se n√£o existir)"
    },
    {
      "caminho": "/home/ubuntu/Agents_Solution_BI/core/security/",
      "tipo": "Diret√≥rio",
      "status": "SER√Å CRIADO",
      "descri√ß√£o": "Diret√≥rio de m√≥dulos de seguran√ßa"
    }
  ]
}
```

**A√ß√£o Pr√©via:** Se algum arquivo n√£o existir, criar com conte√∫do m√≠nimo vi√°vel.

---

## PILAR 1: Governan√ßa de Prompts (CO-STAR)

### 1.1. Modifica√ß√£o: `core/agents/prompt_loader.py`

**Objetivo:** Estender a classe `PromptLoader` para suportar templates Markdown com inje√ß√£o din√¢mica de contexto.

**Arquivo:** `/home/ubuntu/Agents_Solution_BI/core/agents/prompt_loader.py`

**A√ß√µes:**

#### 1.1.1. Adicionar M√©todo de Carregamento de Template Markdown

**Localiza√ß√£o:** Ap√≥s a linha 136 (final da classe `PromptLoader`)

**C√≥digo a Adicionar:**

```python
    def load_prompt_template(self, prompt_name: str) -> Optional[str]:
        """
        Carrega um template de prompt em formato Markdown (.md)
        
        Args:
            prompt_name (str): Nome do arquivo de prompt (com ou sem extens√£o .md)
            
        Returns:
            str: Conte√∫do do template ou None se ocorrer erro
        """
        # Adiciona a extens√£o .md se n√£o estiver presente
        if not prompt_name.endswith(".md"):
            prompt_file = f"{prompt_name}.md"
        else:
            prompt_file = prompt_name
        
        # Constr√≥i o caminho completo do arquivo
        prompt_path = os.path.join(self.prompts_dir, prompt_file)
        
        # Verifica se o arquivo existe
        if not os.path.exists(prompt_path):
            logger.error(f"Template de prompt n√£o encontrado: {prompt_path}")
            return None
        
        # Carrega o arquivo Markdown
        try:
            with open(prompt_path, "r", encoding="utf-8") as file:
                template_content = file.read()
                logger.info(f"Template de prompt carregado com sucesso: {prompt_name}")
                return template_content
        except Exception as e:
            logger.error(f"Erro ao carregar template de prompt {prompt_name}: {e}")
            return None
    
    def inject_context_into_template(self, template: str, context: Dict[str, Any]) -> str:
        """
        Injeta contexto din√¢mico em um template de prompt usando placeholders.
        
        Placeholders esperados no template:
        - [CONTEXTO_DADOS]: Ser√° substitu√≠do pelo esquema de banco de dados
        - [OBJETIVO_AT√îMICO]: Ser√° substitu√≠do pelo objetivo da tarefa
        - [FORMATO_RESPOSTA]: Ser√° substitu√≠do pelas instru√ß√µes de formato
        
        Args:
            template (str): Conte√∫do do template com placeholders
            context (Dict[str, Any]): Dicion√°rio com valores para substitui√ß√£o
            
        Returns:
            str: Template com contexto injetado
        """
        result = template
        
        # Substitui placeholders por valores do contexto
        for placeholder, value in context.items():
            placeholder_key = f"[{placeholder}]"
            if isinstance(value, dict):
                # Se o valor √© um dicion√°rio, converte para string formatada
                value_str = json.dumps(value, ensure_ascii=False, indent=2)
            else:
                value_str = str(value)
            
            result = result.replace(placeholder_key, value_str)
            logger.debug(f"Placeholder {placeholder_key} substitu√≠do com sucesso")
        
        return result
```

**Justificativa:**
- Permite carregar templates Markdown em vez de apenas JSON
- Suporta inje√ß√£o din√¢mica de contexto (esquema de dados, objetivos)
- Mant√©m compatibilidade com o m√©todo `load_prompt` existente

---

### 1.2. Cria√ß√£o: `core/prompts/prompt_desambiguacao.md`

**Objetivo:** Criar um prompt focado em desambiguar perguntas vagas do usu√°rio.

**Arquivo:** `/home/ubuntu/Agents_Solution_BI/core/prompts/prompt_desambiguacao.md` (NOVO)

**Conte√∫do:**

```markdown
# PROMPT DE DESAMBIGUA√á√ÉO - Agent_BI

## PERSONA E PAPEL

**QUEM VOC√ä √â:**
Voc√™ √© um Analista de Dados Interativo e especialista em Business Intelligence. Sua fun√ß√£o √© clarificar perguntas vagas de usu√°rios finais, ajudando-os a refinar suas consultas de dados.

## CONTEXTO

O usu√°rio fez uma pergunta que √© potencialmente amb√≠gua ou vaga. Sua tarefa √© fazer perguntas de esclarecimento para entender melhor a inten√ß√£o do usu√°rio, permitindo que o agente de BI gere uma consulta mais precisa e relevante.

**Esquema de Dados Dispon√≠vel:**
[CONTEXTO_DADOS]

## OBJETIVO AT√îMICO

Formular entre 2 e 3 perguntas de esclarecimento que ajudem a refinar a consulta do usu√°rio. As perguntas devem ser:
- Espec√≠ficas e focadas
- Baseadas no esquema de dados dispon√≠vel
- Apresentadas em formato de m√∫ltipla escolha ou aberta
- Sem gerar c√≥digo SQL ou resposta final

## TAREFA

Analise a pergunta do usu√°rio abaixo e formule perguntas de esclarecimento:

**Pergunta do Usu√°rio:**
[PERGUNTA_USUARIO]

## INSTRU√á√ïES DE FORMATO DE SA√çDA

Retorne **apenas** um objeto JSON v√°lido, sem texto introdut√≥rio ou conclusivo, com a seguinte estrutura:

```json
{
  "pergunta_original": "A pergunta exata do usu√°rio",
  "ambiguidades_detectadas": [
    "Descri√ß√£o da primeira ambiguidade",
    "Descri√ß√£o da segunda ambiguidade"
  ],
  "perguntas_esclarecimento": [
    {
      "numero": 1,
      "pergunta": "Qual √© o per√≠odo de tempo que voc√™ deseja analisar?",
      "opcoes": ["√öltimos 7 dias", "√öltimos 30 dias", "√öltimos 90 dias", "Personalizado"]
    },
    {
      "numero": 2,
      "pergunta": "Qual dimens√£o de an√°lise voc√™ prefere?",
      "opcoes": ["Por Produto", "Por Regi√£o", "Por Cliente", "Todas"]
    }
  ],
  "sugestao_proxima_etapa": "Ap√≥s o usu√°rio responder, o agente poder√° gerar uma consulta SQL precisa."
}
```

## RESTRI√á√ïES

- **N√ÉO** gere c√≥digo SQL
- **N√ÉO** retorne dados ou resultados
- **N√ÉO** fa√ßa suposi√ß√µes sobre a inten√ß√£o do usu√°rio
- **SEMPRE** use apenas as tabelas e colunas dispon√≠veis no esquema de dados
- **SEMPRE** retorne um JSON v√°lido

## TOM E ESTILO

Mantenha um tom consultivo, profissional e focado em clareza. Seja conciso e direto.
```

**Justificativa:**
- Reduz erros de interpreta√ß√£o de consultas vagas
- Melhora a precis√£o das respostas do agente
- Alinha-se com a Feature 4.1.1 (PLN) do PRD

---

### 1.3. Modifica√ß√£o: `core/prompts/prompt_analise.md`

**Objetivo:** Atualizar o prompt principal para incluir o formato CO-STAR e exigir sa√≠da JSON estruturada.

**Arquivo:** `/home/ubuntu/Agents_Solution_BI/core/prompts/prompt_analise.md`

**A√ß√µes:**

#### 1.3.1. Substituir Conte√∫do Completo

**Localiza√ß√£o:** Linhas 1-26 (todo o arquivo)

**Novo Conte√∫do:**

```markdown
# PROMPT PRINCIPAL DE AN√ÅLISE - Agent_BI (CO-STAR)

## CONTEXTO (C)

**Esquema de Banco de Dados:**
[CONTEXTO_DADOS]

O usu√°rio est√° consultando este banco de dados atrav√©s de uma interface conversacional. Voc√™ tem acesso a todas as tabelas e colunas listadas acima.

## OBJETIVO AT√îMICO (O)

Sua tarefa √© traduzir a pergunta do usu√°rio em uma consulta SQL otimizada, execut√°-la (simuladamente), analisar os resultados e formular uma resposta em linguagem natural que seja clara, acion√°vel e relevante para o contexto de neg√≥cio.

**Tarefas Espec√≠ficas:**
1. Interpretar a inten√ß√£o da pergunta do usu√°rio
2. Gerar uma consulta SQL otimizada
3. Simular a execu√ß√£o (ou indicar que ser√° executada)
4. Analisar os dados resultantes
5. Formular uma resposta em portugu√™s claro
6. Sugerir um tipo de gr√°fico apropriado

## ESTILO (S)

Mantenha um estilo de comunica√ß√£o:
- **Conciso:** Sem floreios desnecess√°rios
- **Profissional:** Focado em fatos e dados
- **Anal√≠tico:** Baseado em evid√™ncias
- **Formatado:** Use Markdown para tabelas e listas quando apropriado

## TOM (T)

Adote um tom **consultivo e t√©cnico**. Se a pergunta for amb√≠gua, **n√£o fa√ßa suposi√ß√µes**. Em vez disso, indique a ambiguidade na resposta JSON (campo `ambiguidades_detectadas`).

## P√öBLICO-ALVO (A)

O p√∫blico √© composto por:
- Diretores e Gestores (necessitam de resumos executivos)
- Analistas de Neg√≥cios (necessitam de detalhes t√©cnicos)
- Compradores e Opera√ß√µes (necessitam de dados espec√≠ficos)

Adapte o n√≠vel de detalhe conforme apropriado.

## FORMATO DE RESPOSTA (R)

**SA√çDA OBRIGAT√ìRIA:** Retorne **apenas** um objeto JSON v√°lido, sem texto introdut√≥rio ou conclusivo, com a seguinte estrutura:

```json
{
  "interpretacao_pergunta": "Resumo da inten√ß√£o do usu√°rio",
  "ambiguidades_detectadas": [
    "Se houver ambiguidades, liste aqui"
  ],
  "sql_query": "SELECT ... FROM ... WHERE ...",
  "sql_explicacao": "Explica√ß√£o breve da l√≥gica SQL",
  "data_summary": {
    "total_registros": 0,
    "colunas_retornadas": ["col1", "col2"],
    "resumo_estatistico": "Descri√ß√£o dos dados"
  },
  "natural_language_response": "Resposta em portugu√™s claro e profissional",
  "suggested_chart_type": "bar|line|pie|scatter|table",
  "chart_config": {
    "titulo": "T√≠tulo do gr√°fico",
    "eixo_x": "Nome da dimens√£o",
    "eixo_y": "Nome da m√©trica",
    "filtros_aplicados": ["filtro1", "filtro2"]
  }
}
```

## RESTRI√á√ïES CR√çTICAS

- Use **apenas** as tabelas e colunas fornecidas no CONTEXTO_DADOS
- **N√ÉO** invente dados ou colunas
- **N√ÉO** execute queries perigosas (DROP, DELETE sem WHERE)
- **N√ÉO** retorne dados sens√≠veis sem indica√ß√£o de mascaramento
- **SEMPRE** retorne um JSON v√°lido e bem formatado

## EXEMPLOS DE ENTRADA E SA√çDA

### Exemplo 1: Pergunta Clara
**Entrada:** "Qual foi o faturamento total do √∫ltimo trimestre?"
**Sa√≠da:** JSON com SQL, resumo e gr√°fico de barras

### Exemplo 2: Pergunta Amb√≠gua
**Entrada:** "Me mostre as vendas"
**Sa√≠da:** JSON com `ambiguidades_detectadas` preenchido, sugerindo refinamento

## INSTRU√á√ïES FINAIS

1. Sempre priorize a **precis√£o** sobre a velocidade
2. Se n√£o tiver certeza, indique a ambiguidade
3. Retorne **sempre** um JSON v√°lido
4. Inclua explica√ß√µes t√©cnicas no campo `sql_explicacao`
5. Sugira gr√°ficos apropriados para o tipo de dado
```

**Justificativa:**
- Implementa o paradigma CO-STAR completo
- Garante sa√≠da estruturada em JSON
- Reduz ambiguidades e erros de interpreta√ß√£o

---

## PILAR 2: Seguran√ßa de Dados (PII Masking)

### 2.1. Cria√ß√£o: `core/security/data_masking.py`

**Objetivo:** Implementar fun√ß√µes de mascaramento de PII (Informa√ß√µes Pessoais Identific√°veis).

**Arquivo:** `/home/ubuntu/Agents_Solution_BI/core/security/data_masking.py` (NOVO)

**Conte√∫do:**

```python
"""
M√≥dulo de Seguran√ßa: Mascaramento de PII (Informa√ß√µes Pessoais Identific√°veis)
Fornece fun√ß√µes para identificar e mascarar dados sens√≠veis antes de enviar ao LLM.
"""

import re
import logging
from typing import Dict, List, Tuple

logger = logging.getLogger("data_masking")

# Padr√µes de regex para identificar PII
PII_PATTERNS = {
    "email": r"[\w\.-]+@[\w\.-]+\.\w+",
    "cpf": r"\d{3}\.\d{3}\.\d{3}-\d{2}",
    "telefone": r"\(\d{2}\)\s?\d{4,5}-\d{4}",
    "cartao_credito": r"\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}",
    "nome_proprio": r"\b[A-Z][a-z]+\s[A-Z][a-z]+\b",  # Heur√≠stico b√°sico
}

# Mapeamento de padr√£o para token de mascaramento
PII_MASKS = {
    "email": "[EMAIL_MASKED]",
    "cpf": "[CPF_MASKED]",
    "telefone": "[TELEFONE_MASKED]",
    "cartao_credito": "[CARTAO_MASKED]",
    "nome_proprio": "[NOME_MASKED]",
}


class PIIMasker:
    """
    Classe respons√°vel por mascarar dados sens√≠veis em textos.
    """
    
    def __init__(self, patterns: Dict[str, str] = None, masks: Dict[str, str] = None):
        """
        Inicializa o mascarador de PII.
        
        Args:
            patterns (Dict[str, str]): Dicion√°rio de padr√µes regex customizados
            masks (Dict[str, str]): Dicion√°rio de m√°scaras customizadas
        """
        self.patterns = patterns or PII_PATTERNS
        self.masks = masks or PII_MASKS
        self.masked_items: List[Tuple[str, str]] = []  # Hist√≥rico de mascaramentos
    
    def mask_text(self, text: str) -> str:
        """
        Mascara todos os padr√µes de PII em um texto.
        
        Args:
            text (str): Texto a ser mascarado
            
        Returns:
            str: Texto com PII mascarado
        """
        if not text:
            return text
        
        masked_text = text
        self.masked_items = []
        
        for pii_type, pattern in self.patterns.items():
            mask = self.masks.get(pii_type, "[MASKED]")
            
            # Encontra todas as ocorr√™ncias do padr√£o
            matches = re.finditer(pattern, masked_text)
            
            for match in matches:
                original_value = match.group(0)
                # Registra o mascaramento
                self.masked_items.append((pii_type, original_value))
                logger.debug(f"PII detectado ({pii_type}): {original_value[:10]}...")
            
            # Substitui todas as ocorr√™ncias
            masked_text = re.sub(pattern, mask, masked_text)
        
        logger.info(f"Mascaramento conclu√≠do: {len(self.masked_items)} itens de PII mascarados")
        return masked_text
    
    def mask_dict(self, data: Dict) -> Dict:
        """
        Mascara valores de PII em um dicion√°rio.
        
        Args:
            data (Dict): Dicion√°rio com dados potencialmente sens√≠veis
            
        Returns:
            Dict: Dicion√°rio com PII mascarado
        """
        masked_data = {}
        
        for key, value in data.items():
            if isinstance(value, str):
                masked_data[key] = self.mask_text(value)
            elif isinstance(value, dict):
                masked_data[key] = self.mask_dict(value)
            elif isinstance(value, list):
                masked_data[key] = [
                    self.mask_text(item) if isinstance(item, str) else item
                    for item in value
                ]
            else:
                masked_data[key] = value
        
        return masked_data
    
    def get_masked_items_summary(self) -> Dict:
        """
        Retorna um resumo dos itens mascarados.
        
        Returns:
            Dict: Resumo com contagem por tipo de PII
        """
        summary = {}
        for pii_type, _ in self.masked_items:
            summary[pii_type] = summary.get(pii_type, 0) + 1
        
        return summary


# Inst√¢ncia global do mascarador
_global_masker = PIIMasker()


def mask_pii(text: str) -> str:
    """
    Fun√ß√£o utilit√°ria para mascarar PII em um texto.
    
    Args:
        text (str): Texto a ser mascarado
        
    Returns:
        str: Texto com PII mascarado
    """
    return _global_masker.mask_text(text)


def mask_pii_dict(data: Dict) -> Dict:
    """
    Fun√ß√£o utilit√°ria para mascarar PII em um dicion√°rio.
    
    Args:
        data (Dict): Dicion√°rio com dados potencialmente sens√≠veis
        
    Returns:
        Dict: Dicion√°rio com PII mascarado
    """
    return _global_masker.mask_dict(data)


def get_pii_summary() -> Dict:
    """
    Retorna um resumo dos itens mascarados na sess√£o atual.
    
    Returns:
        Dict: Resumo com contagem por tipo de PII
    """
    return _global_masker.get_masked_items_summary()
```

**Justificativa:**
- Centraliza a l√≥gica de mascaramento de PII
- Suporta m√∫ltiplos padr√µes de dados sens√≠veis
- Registra hist√≥rico de mascaramentos para auditoria
- F√°cil de estender com novos padr√µes

---

### 2.2. Cria√ß√£o: `core/security/__init__.py`

**Objetivo:** Tornar o m√≥dulo `security` um pacote Python.

**Arquivo:** `/home/ubuntu/Agents_Solution_BI/core/security/__init__.py` (NOVO)

**Conte√∫do:**

```python
"""
Pacote de Seguran√ßa do Agent_BI
Fornece m√≥dulos para prote√ß√£o de dados, mascaramento de PII e valida√ß√£o.
"""

from .data_masking import mask_pii, mask_pii_dict, get_pii_summary, PIIMasker

__all__ = [
    "mask_pii",
    "mask_pii_dict",
    "get_pii_summary",
    "PIIMasker",
]
```

---

### 2.3. Modifica√ß√£o: `streamlit_app.py`

**Objetivo:** Integrar o mascaramento de PII no fluxo de entrada/sa√≠da do usu√°rio.

**Arquivo:** `/home/ubuntu/Agents_Solution_BI/streamlit_app.py`

**A√ß√µes:**

#### 2.3.1. Adicionar Import do M√≥dulo de Seguran√ßa

**Localiza√ß√£o:** Ap√≥s as linhas de import (aproximadamente linha 20)

**C√≥digo a Adicionar:**

```python
# Importar m√≥dulo de seguran√ßa
from core.security import mask_pii, mask_pii_dict, get_pii_summary
```

#### 2.3.2. Mascarar Input do Usu√°rio

**Localiza√ß√£o:** Onde a entrada do usu√°rio √© recebida (procure por `st.chat_input` ou similar)

**Padr√£o de C√≥digo Existente (Exemplo):**
```python
user_input = st.chat_input("Digite sua pergunta...")
```

**C√≥digo Modificado:**
```python
user_input = st.chat_input("Digite sua pergunta...")

if user_input:
    # Mascarar PII antes de processar
    masked_input = mask_pii(user_input)
    logger.info(f"Input mascarado: PII removido")
    
    # Usar masked_input para o resto do processamento
    user_input_for_llm = masked_input
else:
    user_input_for_llm = None
```

#### 2.3.3. Mascarar Output do LLM

**Localiza√ß√£o:** Onde a resposta do LLM √© exibida (procure por `st.write` ou similar)

**Padr√£o de C√≥digo Existente (Exemplo):**
```python
llm_response = call_llm(user_input_for_llm)
st.write(llm_response)
```

**C√≥digo Modificado:**
```python
llm_response = call_llm(user_input_for_llm)

# Mascarar PII na resposta do LLM (camada extra de prote√ß√£o)
masked_response = mask_pii(llm_response)

# Exibir resposta mascarada
st.write(masked_response)

# Log de seguran√ßa
pii_summary = get_pii_summary()
if pii_summary:
    logger.warning(f"PII detectado e mascarado: {pii_summary}")
```

---

## PILAR 3: Experi√™ncia de Usu√°rio (Streaming)

### 3.1. Cria√ß√£o: `core/llm_service.py`

**Objetivo:** Centralizar a l√≥gica de chamada ao LLM com suporte a streaming.

**Arquivo:** `/home/ubuntu/Agents_Solution_BI/core/llm_service.py` (NOVO)

**Conte√∫do:**

```python
"""
M√≥dulo de Servi√ßo LLM: Encapsula a l√≥gica de chamada ao Large Language Model
Fornece suporte a streaming e tratamento de erros.
"""

import logging
import json
from typing import Generator, Optional, Dict, Any
from core.agents.prompt_loader import PromptLoader

logger = logging.getLogger("llm_service")

# Importar a biblioteca do LLM (ajustar conforme o LLM utilizado)
# Exemplo: OpenAI, Anthropic, Google Gemini, etc.
# Para este exemplo, assumimos que existe um m√≥dulo de LLM configurado
try:
    from core.llm_client import get_llm_client
except ImportError:
    logger.warning("LLM client n√£o encontrado. Usando mock para testes.")
    def get_llm_client():
        return None


class LLMService:
    """
    Servi√ßo centralizado para chamadas ao LLM com suporte a streaming.
    """
    
    def __init__(self):
        """Inicializa o servi√ßo LLM."""
        self.client = get_llm_client()
        self.prompt_loader = PromptLoader()
        self.model_name = "gpt-4"  # Ajustar conforme o modelo utilizado
    
    def get_response(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """
        Obt√©m uma resposta completa do LLM (n√£o-streaming).
        
        Args:
            prompt (str): Prompt ou template de prompt
            context (Dict): Contexto para inje√ß√£o din√¢mica (opcional)
            
        Returns:
            str: Resposta completa do LLM
        """
        try:
            # Se o prompt √© um template, carregar e injetar contexto
            if context:
                full_prompt = self.prompt_loader.inject_context_into_template(prompt, context)
            else:
                full_prompt = prompt
            
            # Chamada ao LLM (ajustar conforme a biblioteca utilizada)
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": full_prompt}],
                temperature=0.7,
                max_tokens=2000
            )
            
            result = response.choices[0].message.content
            logger.info("Resposta do LLM obtida com sucesso (n√£o-streaming)")
            return result
            
        except Exception as e:
            logger.error(f"Erro ao obter resposta do LLM: {e}")
            return f"Erro ao processar sua pergunta: {str(e)}"
    
    def get_response_stream(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> Generator[str, None, None]:
        """
        Obt√©m uma resposta do LLM em modo streaming (chunks).
        
        Args:
            prompt (str): Prompt ou template de prompt
            context (Dict): Contexto para inje√ß√£o din√¢mica (opcional)
            
        Yields:
            str: Chunks de texto da resposta
        """
        try:
            # Se o prompt √© um template, carregar e injetar contexto
            if context:
                full_prompt = self.prompt_loader.inject_context_into_template(prompt, context)
            else:
                full_prompt = prompt
            
            # Chamada ao LLM com streaming (ajustar conforme a biblioteca utilizada)
            stream = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": full_prompt}],
                temperature=0.7,
                max_tokens=2000,
                stream=True  # Ativar streaming
            )
            
            logger.info("Streaming de resposta do LLM iniciado")
            
            # Iterar sobre os chunks
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
            
            logger.info("Streaming de resposta do LLM conclu√≠do")
            
        except Exception as e:
            logger.error(f"Erro ao fazer streaming da resposta do LLM: {e}")
            yield f"Erro ao processar sua pergunta: {str(e)}"
    
    def parse_json_response(self, response: str) -> Optional[Dict[str, Any]]:
        """
        Tenta fazer parse de uma resposta JSON do LLM.
        
        Args:
            response (str): Resposta do LLM (esperado ser JSON)
            
        Returns:
            Dict: Dicion√°rio parseado ou None se falhar
        """
        try:
            # Tenta fazer parse direto
            return json.loads(response)
        except json.JSONDecodeError:
            # Se falhar, tenta remover caracteres de escape ou formata√ß√£o
            try:
                # Remove markdown code blocks se presentes
                if "```json" in response:
                    response = response.split("```json")[1].split("```")[0]
                elif "```" in response:
                    response = response.split("```")[1].split("```")[0]
                
                return json.loads(response)
            except Exception as e:
                logger.error(f"Erro ao fazer parse de JSON: {e}")
                return None


# Inst√¢ncia global do servi√ßo LLM
_global_llm_service = LLMService()


def get_llm_service() -> LLMService:
    """
    Retorna a inst√¢ncia global do servi√ßo LLM.
    
    Returns:
        LLMService: Inst√¢ncia do servi√ßo
    """
    return _global_llm_service


def get_llm_response(prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
    """
    Fun√ß√£o utilit√°ria para obter resposta do LLM (n√£o-streaming).
    
    Args:
        prompt (str): Prompt
        context (Dict): Contexto (opcional)
        
    Returns:
        str: Resposta do LLM
    """
    return _global_llm_service.get_response(prompt, context)


def get_llm_response_stream(prompt: str, context: Optional[Dict[str, Any]] = None) -> Generator[str, None, None]:
    """
    Fun√ß√£o utilit√°ria para obter resposta do LLM em streaming.
    
    Args:
        prompt (str): Prompt
        context (Dict): Contexto (opcional)
        
    Yields:
        str: Chunks de texto
    """
    yield from _global_llm_service.get_response_stream(prompt, context)
```

**Justificativa:**
- Centraliza a l√≥gica de LLM em um √∫nico m√≥dulo
- Suporta tanto streaming quanto n√£o-streaming
- Facilita testes e manuten√ß√£o
- Separa a l√≥gica de LLM da interface Streamlit

---

### 3.2. Modifica√ß√£o: `streamlit_app.py` (Streaming)

**Objetivo:** Integrar o streaming de resposta do LLM no frontend.

**Arquivo:** `/home/ubuntu/Agents_Solution_BI/streamlit_app.py`

**A√ß√µes:**

#### 3.2.1. Adicionar Import do Servi√ßo LLM

**Localiza√ß√£o:** Ap√≥s as linhas de import (aproximadamente linha 25)

**C√≥digo a Adicionar:**

```python
# Importar servi√ßo LLM
from core.llm_service import get_llm_response_stream, get_llm_service
```

#### 3.2.2. Implementar Streaming na Exibi√ß√£o de Resposta

**Localiza√ß√£o:** Onde a resposta do LLM √© exibida (procure por `st.write` ou similar)

**Padr√£o de C√≥digo Existente (Exemplo):**
```python
if user_input:
    with st.spinner("Processando sua pergunta..."):
        llm_response = call_llm(user_input)
    st.write(llm_response)
```

**C√≥digo Modificado:**
```python
if user_input:
    # Criar um placeholder para a resposta
    response_placeholder = st.empty()
    
    # Criar um placeholder para status intermedi√°rio
    status_placeholder = st.empty()
    
    # Atualizar status
    status_placeholder.text("‚è≥ Analisando a inten√ß√£o...")
    
    try:
        # Usar streaming para exibir a resposta em tempo real
        with response_placeholder.container():
            st.write_stream(get_llm_response_stream(user_input_for_llm))
        
        # Limpar status ap√≥s conclus√£o
        status_placeholder.empty()
        
        logger.info("Resposta exibida com sucesso via streaming")
        
    except Exception as e:
        status_placeholder.error(f"Erro ao processar: {str(e)}")
        logger.error(f"Erro durante streaming: {e}")
```

#### 3.2.3. Adicionar Feedback Intermedi√°rio (Sub-status)

**Localiza√ß√£o:** Antes da chamada ao LLM

**C√≥digo a Adicionar:**

```python
# Criar colunas para layout do status
col1, col2 = st.columns([3, 1])

with col1:
    status_messages = [
        "‚è≥ Analisando a inten√ß√£o...",
        "üîç Gerando consulta SQL...",
        "üíæ Consultando a base de dados...",
        "üìä Formatando a resposta..."
    ]
    
    # Simular progresso (ajustar conforme a l√≥gica real)
    import time
    for i, msg in enumerate(status_messages):
        with st.spinner(msg):
            time.sleep(0.5)  # Simula√ß√£o - remover em produ√ß√£o
```

**Observa√ß√£o:** Este √© um exemplo simplificado. A implementa√ß√£o real deve integrar o feedback com as etapas reais do agente.

---

## Testes e Valida√ß√£o

### 4.1. Teste de Mascaramento de PII

**Arquivo de Teste:** `/home/ubuntu/Agents_Solution_BI/tests/test_data_masking.py` (NOVO)

**Conte√∫do:**

```python
"""
Testes para o m√≥dulo de mascaramento de PII
"""

import unittest
from core.security import mask_pii, mask_pii_dict


class TestDataMasking(unittest.TestCase):
    """Testes para a classe PIIMasker"""
    
    def test_mask_email(self):
        """Testa mascaramento de e-mail"""
        text = "Contato: joao@example.com"
        masked = mask_pii(text)
        self.assertNotIn("joao@example.com", masked)
        self.assertIn("[EMAIL_MASKED]", masked)
    
    def test_mask_cpf(self):
        """Testa mascaramento de CPF"""
        text = "CPF: 123.456.789-10"
        masked = mask_pii(text)
        self.assertNotIn("123.456.789-10", masked)
        self.assertIn("[CPF_MASKED]", masked)
    
    def test_mask_dict(self):
        """Testa mascaramento em dicion√°rio"""
        data = {
            "nome": "Jo√£o Silva",
            "email": "joao@example.com",
            "cpf": "123.456.789-10"
        }
        masked = mask_pii_dict(data)
        self.assertNotIn("joao@example.com", masked["email"])
        self.assertIn("[EMAIL_MASKED]", masked["email"])
    
    def test_no_false_positives(self):
        """Testa que n√£o h√° falsos positivos"""
        text = "O produto custa R$ 123.45"
        masked = mask_pii(text)
        # N√£o deve mascarar valores monet√°rios
        self.assertIn("123.45", masked)


if __name__ == "__main__":
    unittest.main()
```

**Execu√ß√£o:**
```bash
cd /home/ubuntu/Agents_Solution_BI
python -m pytest tests/test_data_masking.py -v
```

---

### 4.2. Teste de Streaming

**Arquivo de Teste:** `/home/ubuntu/Agents_Solution_BI/tests/test_llm_streaming.py` (NOVO)

**Conte√∫do:**

```python
"""
Testes para o m√≥dulo de streaming de LLM
"""

import unittest
from core.llm_service import get_llm_service


class TestLLMStreaming(unittest.TestCase):
    """Testes para o servi√ßo LLM com streaming"""
    
    def setUp(self):
        """Configura√ß√£o antes de cada teste"""
        self.llm_service = get_llm_service()
    
    def test_stream_generator(self):
        """Testa se o streaming retorna um generator"""
        prompt = "Ol√°, como voc√™ est√°?"
        stream = self.llm_service.get_response_stream(prompt)
        
        # Verificar se √© um generator
        self.assertTrue(hasattr(stream, '__iter__'))
        self.assertTrue(hasattr(stream, '__next__'))
    
    def test_stream_yields_strings(self):
        """Testa se o streaming retorna strings"""
        prompt = "Teste de streaming"
        stream = self.llm_service.get_response_stream(prompt)
        
        for chunk in stream:
            self.assertIsInstance(chunk, str)
            break  # Apenas verificar o primeiro chunk


if __name__ == "__main__":
    unittest.main()
```

**Execu√ß√£o:**
```bash
cd /home/ubuntu/Agents_Solution_BI
python -m pytest tests/test_llm_streaming.py -v
```

---

## Crit√©rio de Sucesso

A implementa√ß√£o ser√° considerada bem-sucedida quando **TODOS** os crit√©rios abaixo forem atendidos:

### ‚úÖ Crit√©rio 1: Governan√ßa de Prompts (CO-STAR)

```json
{
  "criterio": "Governan√ßa de Prompts",
  "validacoes": [
    {
      "item": "Prompt de Desambigua√ß√£o",
      "validacao": "Arquivo /home/ubuntu/Agents_Solution_BI/core/prompts/prompt_desambiguacao.md existe e cont√©m estrutura JSON",
      "teste": "Verificar se arquivo existe e cont√©m 'CONTEXTO_DADOS' e 'PERGUNTA_USUARIO'"
    },
    {
      "item": "Prompt Principal Atualizado",
      "validacao": "Arquivo /home/ubuntu/Agents_Solution_BI/core/prompts/prompt_analise.md cont√©m se√ß√µes CO-STAR",
      "teste": "Verificar se arquivo cont√©m 'CONTEXTO', 'OBJETIVO', 'ESTILO', 'TOM', 'P√öBLICO', 'FORMATO'"
    },
    {
      "item": "M√©todo de Inje√ß√£o de Contexto",
      "validacao": "Fun√ß√£o inject_context_into_template() existe em prompt_loader.py",
      "teste": "Chamar fun√ß√£o com template e contexto, verificar se placeholders foram substitu√≠dos"
    },
    {
      "item": "Sa√≠da JSON Estruturada",
      "validacao": "Resposta do agente √© um JSON v√°lido com chaves obrigat√≥rias",
      "teste": "Fazer pergunta, verificar se resposta √© JSON com 'sql_query', 'natural_language_response', 'suggested_chart_type'"
    }
  ]
}
```

### ‚úÖ Crit√©rio 2: Seguran√ßa de Dados (PII Masking)

```json
{
  "criterio": "Seguran√ßa de Dados",
  "validacoes": [
    {
      "item": "M√≥dulo de Mascaramento",
      "validacao": "Arquivo /home/ubuntu/Agents_Solution_BI/core/security/data_masking.py existe",
      "teste": "Importar m√≥dulo: from core.security import mask_pii"
    },
    {
      "item": "Mascaramento de E-mail",
      "validacao": "Fun√ß√£o mask_pii() mascara e-mails corretamente",
      "teste": "mask_pii('contato@example.com') retorna '[EMAIL_MASKED]'"
    },
    {
      "item": "Mascaramento de CPF",
      "validacao": "Fun√ß√£o mask_pii() mascara CPFs corretamente",
      "teste": "mask_pii('123.456.789-10') retorna '[CPF_MASKED]'"
    },
    {
      "item": "Integra√ß√£o no Streamlit",
      "validacao": "Input do usu√°rio √© mascarado antes de enviar ao LLM",
      "teste": "Verificar logs: 'Input mascarado: PII removido' aparece ap√≥s entrada do usu√°rio"
    },
    {
      "item": "Prote√ß√£o de Sa√≠da",
      "validacao": "Output do LLM √© mascarado antes de exibir",
      "teste": "Verificar logs: 'PII detectado e mascarado' aparece ap√≥s resposta do LLM"
    }
  ]
}
```

### ‚úÖ Crit√©rio 3: Experi√™ncia de Usu√°rio (Streaming)

```json
{
  "criterio": "Experi√™ncia de Usu√°rio",
  "validacoes": [
    {
      "item": "Servi√ßo LLM com Streaming",
      "validacao": "Arquivo /home/ubuntu/Agents_Solution_BI/core/llm_service.py existe",
      "teste": "Importar m√≥dulo: from core.llm_service import get_llm_response_stream"
    },
    {
      "item": "M√©todo get_response_stream()",
      "validacao": "Fun√ß√£o retorna um generator de strings",
      "teste": "Chamar fun√ß√£o, verificar se retorna generator com chunks de texto"
    },
    {
      "item": "Integra√ß√£o com st.write_stream()",
      "validacao": "Resposta do LLM √© exibida via st.write_stream() no Streamlit",
      "teste": "Fazer pergunta no Streamlit, verificar se resposta aparece gradualmente (n√£o de uma vez)"
    },
    {
      "item": "Sem Spinner Bloqueante",
      "validacao": "N√£o h√° st.spinner() bloqueante durante o streaming",
      "teste": "Fazer pergunta, verificar que a interface n√£o fica congelada com 'lupa rodando'"
    },
    {
      "item": "Feedback Intermedi√°rio",
      "validacao": "Mensagens de status aparecem durante o processamento",
      "teste": "Fazer pergunta, verificar se aparecem mensagens como '‚è≥ Analisando...' ou 'üîç Gerando SQL...'"
    }
  ]
}
```

### üéØ Teste de Integra√ß√£o Final

**Cen√°rio:** Um usu√°rio faz uma pergunta vaga no Streamlit.

**Fluxo Esperado:**
1. ‚úÖ Input do usu√°rio √© mascarado (PII removido)
2. ‚úÖ Sistema detecta ambiguidade e usa prompt_desambiguacao.md
3. ‚úÖ Resposta √© gerada em JSON estruturado
4. ‚úÖ Resposta √© mascarada (prote√ß√£o extra)
5. ‚úÖ Resposta √© exibida via streaming (sem spinner bloqueante)
6. ‚úÖ Usu√°rio v√™ a resposta aparecer gradualmente

**Comando de Teste:**
```bash
cd /home/ubuntu/Agents_Solution_BI
streamlit run streamlit_app.py
# Fazer pergunta: "Me mostre as vendas"
# Verificar: Resposta aparece gradualmente, sem "lupa rodando"
```

---

## Checklist de Implementa√ß√£o

Utilize este checklist para rastrear o progresso:

```
PILAR 1: Governan√ßa de Prompts (CO-STAR)
- [ ] Modificar core/agents/prompt_loader.py (adicionar load_prompt_template e inject_context_into_template)
- [ ] Criar core/prompts/prompt_desambiguacao.md
- [ ] Modificar core/prompts/prompt_analise.md (adicionar estrutura CO-STAR)

PILAR 2: Seguran√ßa de Dados (PII Masking)
- [ ] Criar core/security/__init__.py
- [ ] Criar core/security/data_masking.py (classe PIIMasker)
- [ ] Modificar streamlit_app.py (adicionar imports de seguran√ßa)
- [ ] Modificar streamlit_app.py (mascarar input do usu√°rio)
- [ ] Modificar streamlit_app.py (mascarar output do LLM)

PILAR 3: Experi√™ncia de Usu√°rio (Streaming)
- [ ] Criar core/llm_service.py (classe LLMService com streaming)
- [ ] Modificar streamlit_app.py (adicionar imports de LLM)
- [ ] Modificar streamlit_app.py (implementar st.write_stream)
- [ ] Modificar streamlit_app.py (remover st.spinner bloqueante)
- [ ] Modificar streamlit_app.py (adicionar feedback intermedi√°rio)

TESTES E VALIDA√á√ÉO
- [ ] Criar tests/test_data_masking.py
- [ ] Criar tests/test_llm_streaming.py
- [ ] Executar testes de mascaramento
- [ ] Executar testes de streaming
- [ ] Teste manual no Streamlit (pergunta vaga)
- [ ] Verificar logs de seguran√ßa

FINALIZA√á√ÉO
- [ ] Todos os crit√©rios de sucesso atendidos
- [ ] Documenta√ß√£o atualizada
- [ ] C√≥digo commitado no Git
```

---

## Notas Importantes

1. **Retrocompatibilidade:** Todas as altera√ß√µes devem ser retrocompat√≠veis. N√£o remova c√≥digo existente, apenas estenda.
2. **Logging:** Adicione logs em pontos cr√≠ticos para facilitar debugging e auditoria.
3. **Tratamento de Erros:** Sempre use try/except para evitar que erros de seguran√ßa ou streaming quebrem a aplica√ß√£o.
4. **Testes:** Execute os testes ap√≥s cada pilar implementado.
5. **Git:** Fa√ßa commits frequentes com mensagens descritivas.

---

## Refer√™ncias de Arquivos

| Arquivo | Tipo | Status | Descri√ß√£o |
| :--- | :--- | :--- | :--- |
| `/home/ubuntu/Agents_Solution_BI/streamlit_app.py` | Python | Modificar | Frontend Streamlit |
| `/home/ubuntu/Agents_Solution_BI/core/agents/prompt_loader.py` | Python | Modificar | Carregador de prompts |
| `/home/ubuntu/Agents_Solution_BI/core/prompts/prompt_analise.md` | Markdown | Modificar | Prompt principal |
| `/home/ubuntu/Agents_Solution_BI/core/prompts/prompt_desambiguacao.md` | Markdown | Criar | Prompt de desambigua√ß√£o |
| `/home/ubuntu/Agents_Solution_BI/core/security/__init__.py` | Python | Criar | Pacote de seguran√ßa |
| `/home/ubuntu/Agents_Solution_BI/core/security/data_masking.py` | Python | Criar | M√≥dulo de mascaramento PII |
| `/home/ubuntu/Agents_Solution_BI/core/llm_service.py` | Python | Criar | Servi√ßo LLM com streaming |
| `/home/ubuntu/Agents_Solution_BI/tests/test_data_masking.py` | Python | Criar | Testes de mascaramento |
| `/home/ubuntu/Agents_Solution_BI/tests/test_llm_streaming.py` | Python | Criar | Testes de streaming |

---

## Conclus√£o

Este **Plano de Implementa√ß√£o Cir√∫rgica (PIC)** fornece instru√ß√µes detalhadas e precisas para implementar as tr√™s melhorias propostas no Agent_BI. Cada a√ß√£o √© localizada, especificada e test√°vel, minimizando o risco de introduzir bugs ou quebrar a funcionalidade existente.

**Pr√≥ximas Etapas:**
1. Revisar este documento com a equipe de desenvolvimento
2. Executar as a√ß√µes na ordem especificada
3. Testar ap√≥s cada pilar
4. Documentar qualquer desvio ou ajuste necess√°rio
5. Fazer commit no Git com mensagens descritivas
