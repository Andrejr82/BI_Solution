# üìä Guia de Configura√ß√£o de Max Tokens

## Vis√£o Geral

O par√¢metro `max_tokens` controla o **n√∫mero m√°ximo de tokens** que o modelo pode gerar na resposta. Um token √© aproximadamente 3-4 caracteres em portugu√™s, ou cerca de 0.75 palavras.

## Limites por Modelo

### Gemini 2.5 Flash
- **Limite de Sa√≠da**: 8,192 tokens
- **Contexto Total**: 1,048,576 tokens (prompt + resposta)
- **Custo**: ~0.15 USD por 1M tokens de entrada / 0.60 USD por 1M tokens de sa√≠da

### DeepSeek V3
- **Limite de Sa√≠da**: 8,192 tokens
- **Contexto Total**: 64,000 tokens
- **Custo**: ~0.27 USD por 1M tokens de entrada / 1.10 USD por 1M tokens de sa√≠da

## Valores Recomendados por Caso de Uso

### üéØ **Produ√ß√£o (Agent_BI)**

| Caso de Uso | max_tokens | Justificativa |
|-------------|-----------|---------------|
| **Intent Classification** | 512-1024 | Resposta curta (apenas classifica√ß√£o) |
| **Code Generation** | 2048-4096 | C√≥digo Python + coment√°rios + explica√ß√£o |
| **Data Analysis** | 4096-6144 | An√°lise detalhada + insights + gr√°ficos |
| **Conversa√ß√£o Geral** | 1024-2048 | Respostas diretas ao usu√°rio |
| **Error Messages** | 512 | Mensagens de erro concisas |

### üß™ **Desenvolvimento e Testes**

| Caso de Uso | max_tokens | Justificativa |
|-------------|-----------|---------------|
| **Playground** | **4096** (padr√£o) | Experimenta√ß√£o sem cortes |
| **Testes Unit√°rios** | 50-100 | Valida√ß√£o r√°pida |
| **Health Checks** | 10 | Apenas verifica√ß√£o de conectividade |
| **Debugging** | 2048-4096 | An√°lise detalhada de erros |

## Configura√ß√µes Atuais do Sistema

### Core LLM Adapter
```python
# core/llm_adapter.py
def get_completion(self, messages, model=None, temperature=0, max_tokens=1024, ...):
```
**Status**: ‚úÖ **1024 tokens** (adequado para uso geral)

### Playground
```python
# pages/10_ü§ñ_Gemini_Playground.py
max_tokens = st.slider(
    "Max Tokens",
    min_value=512,
    max_value=8192,
    value=4096,  # ‚úÖ ATUALIZADO de 2048 ‚Üí 4096
    step=512
)
```
**Status**: ‚úÖ **4096 tokens** (ideal para experimenta√ß√£o)

### Intent Classifier
```python
# core/business_intelligence/intent_classifier.py
max_output_tokens=1024
```
**Status**: ‚úÖ **1024 tokens** (suficiente para classifica√ß√£o)

## Como Identificar Respostas Cortadas

### Sinais de Corte
1. **C√≥digo incompleto**: Falta `]`, `}`, ou quebra no meio de uma fun√ß√£o
2. **Texto interrompido**: Frase termina abruptamente sem pontua√ß√£o
3. **Aviso no log**: `finish_reason='length'`

### Como Verificar no C√≥digo
```python
response = llm.get_completion(messages, max_tokens=1024)

# Verificar se foi cortada
if response.get('finish_reason') == 'length':
    print("‚ö†Ô∏è Resposta cortada! Aumente max_tokens")
```

### Solu√ß√£o
1. **Aumentar max_tokens** gradualmente: 1024 ‚Üí 2048 ‚Üí 4096
2. **Otimizar o prompt**: Tornar mais conciso e direto
3. **Quebrar em m√∫ltiplas chamadas**: Para an√°lises muito longas

## Impacto no Custo

### Estimativa de Custo (Gemini 2.5 Flash)
| max_tokens | Tokens Reais | Custo/Chamada | Custo/1000 Chamadas |
|------------|--------------|---------------|---------------------|
| 1024 | ~800 | $0.00048 | $0.48 |
| 2048 | ~1600 | $0.00096 | $0.96 |
| 4096 | ~3200 | $0.00192 | $1.92 |
| 8192 | ~6400 | $0.00384 | $3.84 |

**Nota**: Custo real depende do tamanho da resposta gerada, n√£o do limite configurado.

## Melhores Pr√°ticas

### ‚úÖ DO (Fa√ßa)
- Use valores adequados ao caso de uso
- Monitore `finish_reason` para detectar cortes
- Implemente cache para economizar tokens
- Teste com valores baixos em desenvolvimento
- Use max_tokens alto apenas quando necess√°rio

### ‚ùå DON'T (N√£o Fa√ßa)
- Usar 8192 tokens por padr√£o (desperd√≠cio)
- Usar < 512 tokens para an√°lise de c√≥digo
- Ignorar avisos de resposta cortada
- Processar m√∫ltiplas an√°lises em uma √∫nica chamada
- Esquecer de documentar configura√ß√µes customizadas

## Troubleshooting

### Problema: "Resposta Vazia"
**Causa**: `max_tokens` muito baixo, modelo n√£o consegue gerar nada
**Solu√ß√£o**: Aumentar para no m√≠nimo 512 tokens

### Problema: "Resposta Cortada no Meio do C√≥digo"
**Causa**: `max_tokens` insuficiente para resposta completa
**Solu√ß√£o**: Aumentar para 2048-4096 tokens

### Problema: "Alto consumo de cr√©ditos"
**Causa**: `max_tokens` muito alto em todas as chamadas
**Solu√ß√£o**: Ajustar por caso de uso, usar cache

### Problema: "Timeout na API"
**Causa**: Resposta muito longa + processamento demorado
**Solu√ß√£o**: Reduzir max_tokens ou quebrar em m√∫ltiplas chamadas

## Monitoramento

### M√©tricas a Acompanhar
1. **Taxa de corte**: Quantas respostas foram cortadas (finish_reason='length')
2. **Tokens m√©dios**: M√©dia de tokens usados por resposta
3. **Custo total**: Consumo de tokens √ó pre√ßo por token
4. **Cache hit rate**: Porcentagem de respostas do cache (sem consumir tokens)

### Alertas Recomendados
- Taxa de corte > 5% ‚Üí Aumentar max_tokens padr√£o
- Tokens m√©dios < 30% do max_tokens ‚Üí Reduzir max_tokens
- Custo mensal > $100 ‚Üí Revisar uso de tokens

## Refer√™ncias

- [Gemini API - Pricing](https://ai.google.dev/pricing)
- [OpenAI Tokenizer](https://platform.openai.com/tokenizer)
- [Token Counting Best Practices](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)

---

**√öltima Atualiza√ß√£o**: 13/10/2025
**Vers√£o**: 1.0
**Autor**: Agent_BI Team
