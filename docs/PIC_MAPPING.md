# Mapeamento: Pr√≥ximos Passos ‚Üî PIC (Plano de Implementa√ß√£o Cir√∫rgica)

**Data:** 22 de Novembro de 2025  
**Documento de Refer√™ncia:** `prompt_pic_agent_bi_implementation.md`

---

## ‚úÖ Resposta Direta

**SIM!** Os pr√≥ximos passos opcionais correspondem **EXATAMENTE** √†s se√ß√µes do PIC que ainda n√£o foram implementadas.

---

## üìä Mapeamento Detalhado

### Pr√≥ximos Passos Opcionais ‚Üí Se√ß√µes do PIC

| Pr√≥ximo Passo | Se√ß√£o do PIC | Linhas | Status |
|---------------|--------------|--------|--------|
| Integrar mascaramento no `streamlit_app.py` (input) | **2.3.2** - Mascarar Input do Usu√°rio | 623-645 | ‚è≥ Pendente |
| Integrar mascaramento no `streamlit_app.py` (output) | **2.3.3** - Mascarar Output do LLM | 647-671 | ‚è≥ Pendente |
| Integrar streaming no `streamlit_app.py` | **3.2** - Modifica√ß√£o: streamlit_app.py (Streaming) | 872-950 | ‚è≥ Pendente |
| Remover spinners bloqueantes | **3.2.2** - Implementar Streaming na Exibi√ß√£o | 891-930 | ‚è≥ Pendente |
| Implementar resposta progressiva | **3.2.2** - Streaming com placeholder | 901-930 | ‚è≥ Pendente |

---

## üìù Detalhamento das Se√ß√µes Pendentes

### 1. Se√ß√£o 2.3.2 do PIC: Mascarar Input do Usu√°rio

**Localiza√ß√£o no PIC:** Linhas 623-645  
**Objetivo:** Integrar mascaramento de PII no input do usu√°rio

**C√≥digo Especificado no PIC:**
```python
user_input = st.chat_input("Digite sua pergunta...")

if user_input:
    # Mascarar PII antes de processar
    masked_input = mask_pii(user_input)
    logger.info(f"Input mascarado: PII removido")
    
    # Usar masked_input para o resto do processamento
    user_input_for_llm = masked_input
else:
    user_input_for_llm = None
```

**Status:** ‚úÖ M√≥dulo criado, ‚è≥ Integra√ß√£o pendente

---

### 2. Se√ß√£o 2.3.3 do PIC: Mascarar Output do LLM

**Localiza√ß√£o no PIC:** Linhas 647-671  
**Objetivo:** Mascarar PII na resposta do LLM

**C√≥digo Especificado no PIC:**
```python
llm_response = call_llm(user_input_for_llm)

# Mascarar PII na resposta do LLM (camada extra de prote√ß√£o)
masked_response = mask_pii(llm_response)

# Exibir resposta mascarada
st.write(masked_response)

# Log de seguran√ßa
pii_summary = get_pii_summary()
if pii_summary:
    logger.warning(f"PII detectado e mascarado: {pii_summary}")
```

**Status:** ‚úÖ M√≥dulo criado, ‚è≥ Integra√ß√£o pendente

---

### 3. Se√ß√£o 3.2 do PIC: Streaming no streamlit_app.py

**Localiza√ß√£o no PIC:** Linhas 872-950  
**Objetivo:** Implementar streaming de respostas

**C√≥digo Especificado no PIC:**
```python
# 3.2.1. Adicionar Import
from core.llm_service import get_llm_response_stream, get_llm_service

# 3.2.2. Implementar Streaming
if user_input:
    # SEM spinner bloqueante
    response_placeholder = st.empty()
    full_response = ""
    
    for chunk in get_llm_response_stream(prompt, context):
        full_response += chunk
        response_placeholder.markdown(full_response + "‚ñå")
    
    response_placeholder.markdown(full_response)
```

**Status:** ‚úÖ Servi√ßo criado, ‚è≥ Integra√ß√£o pendente

---

## üéØ Resumo da Correspond√™ncia

### O que J√Å foi implementado (Pilares Core):

‚úÖ **Pilar 1: Governan√ßa de Prompts**
- ‚úÖ Se√ß√£o 1.1: `prompt_loader.py` estendido
- ‚úÖ Se√ß√£o 1.2: `prompt_desambiguacao.md` criado
- ‚úÖ Se√ß√£o 1.3: `prompt_analise.md` atualizado

‚úÖ **Pilar 2: Seguran√ßa de Dados (M√≥dulos)**
- ‚úÖ Se√ß√£o 2.1: `data_masking.py` criado
- ‚úÖ Se√ß√£o 2.2: `security/__init__.py` atualizado

‚úÖ **Pilar 3: Streaming (Servi√ßo)**
- ‚úÖ Se√ß√£o 3.1: `llm_service.py` criado

### O que FALTA implementar (Integra√ß√£o no Streamlit):

‚è≥ **Integra√ß√£o no streamlit_app.py:**
- ‚è≥ Se√ß√£o 2.3.1: Import do m√≥dulo de seguran√ßa
- ‚è≥ Se√ß√£o 2.3.2: Mascarar input do usu√°rio
- ‚è≥ Se√ß√£o 2.3.3: Mascarar output do LLM
- ‚è≥ Se√ß√£o 3.2.1: Import do servi√ßo LLM
- ‚è≥ Se√ß√£o 3.2.2: Implementar streaming na UI

---

## üìã Pr√≥ximos Passos Detalhados (Seguindo o PIC)

### Passo 1: Adicionar Imports no streamlit_app.py

**Localiza√ß√£o:** Ap√≥s linha ~20 (se√ß√£o de imports)

```python
# Importar m√≥dulo de seguran√ßa (Se√ß√£o 2.3.1 do PIC)
from core.security import mask_pii, mask_pii_dict, get_pii_summary

# Importar servi√ßo LLM (Se√ß√£o 3.2.1 do PIC)
from core.llm_service import get_llm_response_stream, get_llm_service
```

### Passo 2: Mascarar Input do Usu√°rio

**Localiza√ß√£o:** Onde `user_input` √© recebido (buscar por processamento de input)

```python
if user_input:
    # Mascarar PII antes de processar
    masked_input = mask_pii(user_input)
    logger.info("PII mascarado no input")
    
    # Usar masked_input para processamento
    user_input_for_llm = masked_input
```

### Passo 3: Implementar Streaming com Mascaramento

**Localiza√ß√£o:** Onde a resposta √© exibida

```python
# Obter resposta com streaming
response_placeholder = st.empty()
full_response = ""

for chunk in get_llm_response_stream(prompt, context):
    full_response += chunk
    response_placeholder.markdown(full_response + "‚ñå")

# Mascarar PII na resposta final
masked_response = mask_pii(full_response)
response_placeholder.markdown(masked_response)

# Log de seguran√ßa
pii_summary = get_pii_summary()
if pii_summary:
    logger.warning(f"PII detectado: {pii_summary}")
```

---

## ‚úÖ Conclus√£o

**Os pr√≥ximos passos opcionais s√£o EXATAMENTE as se√ß√µes 2.3 e 3.2 do PIC.**

**Implementa√ß√£o Core:** ‚úÖ 100% Conclu√≠da  
**Integra√ß√£o no Streamlit:** ‚è≥ 0% (Pr√≥xima fase)

**Refer√™ncias:**
- PIC Completo: [prompt_pic_agent_bi_implementation.md](file:///c:/Users/Andr√©/Documents/Agent_Solution_BI/prompt_pic_agent_bi_implementation.md)
- Se√ß√£o 2.3: Linhas 604-671
- Se√ß√£o 3.2: Linhas 872-950
