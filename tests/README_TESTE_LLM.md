# Teste de 80 Perguntas - 100% LLM

## üìã Vis√£o Geral

Este teste avalia a capacidade do sistema de processar 80 perguntas de neg√≥cio usando **100% LLM** (Gemini ou DeepSeek), sem cache ou padr√µes pr√©-definidos.

## üéØ Objetivo

Validar que a LLM consegue:
- Entender perguntas complexas de neg√≥cio
- Processar consultas em linguagem natural
- Gerar respostas completas e relevantes
- **Consumir tokens da API para cada pergunta**

## ‚ö†Ô∏è ATEN√á√ÉO - CONSUMO DE TOKENS

Este teste **consome tokens da API**!

- **Estimativa**: 5.000 - 10.000 tokens para 80 perguntas
- **Custo aproximado**:
  - Gemini: ~$0.005 - $0.010
  - DeepSeek: ~$0.0005 - $0.001

## üîë Pr√©-requisitos

### 1. Configurar API Key

Voc√™ precisa de pelo menos uma das seguintes chaves de API:

**Op√ß√£o A: Gemini (Google)**
```bash
# Windows (PowerShell)
$env:GEMINI_API_KEY = "sua-chave-aqui"

# Windows (CMD)
set GEMINI_API_KEY=sua-chave-aqui

# Linux/Mac
export GEMINI_API_KEY="sua-chave-aqui"
```

**Op√ß√£o B: DeepSeek**
```bash
# Windows (PowerShell)
$env:DEEPSEEK_API_KEY = "sua-chave-aqui"

# Windows (CMD)
set DEEPSEEK_API_KEY=sua-chave-aqui

# Linux/Mac
export DEEPSEEK_API_KEY="sua-chave-aqui"
```

### 2. Verificar .env (Opcional)

Ou adicione no arquivo `.env` na raiz do projeto:

```env
GEMINI_API_KEY=sua-chave-aqui
DEEPSEEK_API_KEY=sua-chave-aqui
```

## üöÄ Como Executar

### Op√ß√£o 1: Script Auxiliar (Recomendado)

```bash
cd C:\Users\Andr√©\Documents\Agent_Solution_BI
python tests/run_test_llm.py
```

### Op√ß√£o 2: Diretamente

```bash
cd C:\Users\Andr√©\Documents\Agent_Solution_BI
python tests/test_80_perguntas_llm.py
```

## üìä O que √© Testado

O teste cobre as mesmas 10 categorias do teste DirectQueryEngine:

1. üéØ Vendas por Produto (8 perguntas)
2. üè™ An√°lises por Segmento (8 perguntas)
3. üè¨ An√°lises por UNE/Loja (8 perguntas)
4. üìà An√°lises Temporais (8 perguntas)
5. üí∞ Performance e ABC (8 perguntas)
6. üì¶ Estoque e Log√≠stica (8 perguntas)
7. üè≠ An√°lises por Fabricante (8 perguntas)
8. üé® Categoria/Grupo (8 perguntas)
9. üìä Dashboards Executivos (8 perguntas)
10. üîç An√°lises Espec√≠ficas (8 perguntas)

**Total: 80 perguntas**

## üìà M√©tricas Avaliadas

- ‚úÖ **SUCCESS**: LLM processou com sucesso
- ‚ùå **ERROR**: Erro durante processamento
- ‚è±Ô∏è **TIMEOUT**: Excedeu 60 segundos

Adicionalmente:
- **Tokens consumidos** (estimativa)
- **Tempo de processamento** por pergunta
- **Preview da resposta** gerada

## üìÑ Relat√≥rio Gerado

```
tests/relatorio_teste_80_perguntas_llm_YYYYMMDD_HHMMSS.json
```

### Estrutura do Relat√≥rio

```json
{
  "metadata": {
    "timestamp": "2025-10-19T...",
    "total_perguntas": 80,
    "total_categorias": 10,
    "llm_usado": "Gemini",
    "modo": "100% LLM"
  },
  "estatisticas": {
    "SUCCESS": 75,
    "ERROR": 3,
    "TIMEOUT": 2,
    "total_tokens_estimados": 8450,
    "media_tokens_pergunta": 105.6
  },
  "resultados": [
    {
      "id": 1,
      "categoria": "üéØ Vendas por Produto",
      "pergunta": "Gere um gr√°fico...",
      "status": "SUCCESS",
      "mensagem": "Resposta gerada (542 chars)",
      "tokens_estimados": 135,
      "tempo_processamento": 2.45,
      "timestamp": "2025-10-19T...",
      "resposta_preview": "Para gerar o gr√°fico..."
    }
  ]
}
```

## üîç Interpreta√ß√£o dos Resultados

### Meta de Sucesso
- **Excelente**: SUCCESS > 90% (72+ perguntas)
- **√ìtimo**: SUCCESS > 80% (64+ perguntas)
- **Bom**: SUCCESS > 70% (56+ perguntas)
- **Aceit√°vel**: SUCCESS > 60% (48+ perguntas)

### An√°lise de Performance
- **Tempo m√©dio**: Ideal < 3 segundos/pergunta
- **Tokens m√©dios**: ~80-150 tokens/resposta
- **Taxa de timeout**: Ideal < 5%

### Compara√ß√£o com DirectQueryEngine

Use este teste para comparar:
- **Performance**: DirectQuery = ~0.5s, LLM = ~2-5s
- **Custo**: DirectQuery = $0, LLM = $0.005-0.01
- **Flexibilidade**: LLM tem maior capacidade de entender varia√ß√µes

## üéÆ Controle de Execu√ß√£o

### Cancelar o Teste

Pressione `Ctrl+C` para cancelar:
- Nos primeiros 5 segundos: cancela antes de iniciar
- Durante execu√ß√£o: interrompe e salva resultados parciais

### Timeout por Pergunta

Cada pergunta tem timeout de 60 segundos. Se exceder:
- Status = TIMEOUT
- Pergunta √© pulada
- Teste continua com a pr√≥xima

## üí° Dicas de Uso

### 1. Teste Pequeno Primeiro

Antes de rodar todas as 80 perguntas, teste com poucas:

```python
# Edite test_80_perguntas_llm.py
# Comente categorias que n√£o quer testar
PERGUNTAS = {
    "üéØ Vendas por Produto": [
        "Gere um gr√°fico de vendas do produto 369947 na UNE SCR",
        # ... apenas 2-3 perguntas para teste
    ]
}
```

### 2. Monitor de Custos

Acompanhe o consumo de tokens no dashboard da API:
- **Gemini**: https://makersuite.google.com/
- **DeepSeek**: https://platform.deepseek.com/

### 3. Escolha da LLM

- **Gemini**: Melhor qualidade, mais caro
- **DeepSeek**: Mais barato, boa qualidade

O script tenta Gemini primeiro, depois DeepSeek como fallback.

## üêõ Troubleshooting

### Erro: "API key not found"

```bash
# Verifique se configurou a vari√°vel
echo $env:GEMINI_API_KEY  # PowerShell
echo %GEMINI_API_KEY%     # CMD
```

### Erro: "Rate limit exceeded"

Aguarde alguns minutos. As APIs t√™m limites de requisi√ß√µes/minuto.

### Erro: "Module not found"

```bash
pip install -r requirements.txt
```

### Muitos Timeouts

- Verifique sua conex√£o de internet
- Tente usar DeepSeek (geralmente mais r√°pido)
- Aumente o timeout no c√≥digo (linha ~180)

## üìä Compara√ß√£o: DirectQuery vs LLM

| M√©trica | DirectQuery | 100% LLM |
|---------|-------------|----------|
| Velocidade | ~0.5s/pergunta | ~2-5s/pergunta |
| Custo | $0 | $0.005-0.01 |
| Flexibilidade | Padr√µes fixos | Alta |
| Novos tipos de pergunta | Precisa implementar | Entende naturalmente |
| Consumo de recursos | Baixo (CPU) | M√©dio (API) |
| Offline | ‚úÖ Sim | ‚ùå N√£o |

## üéØ Quando Usar Cada Abordagem

### Use DirectQuery quando:
- Perguntas s√£o repetitivas e conhecidas
- Velocidade √© cr√≠tica
- Quer economizar custos de API
- Sistema precisa funcionar offline

### Use 100% LLM quando:
- Perguntas s√£o variadas e imprevis√≠veis
- Flexibilidade √© mais importante que velocidade
- Custo de API √© aceit√°vel
- Quer capacidade de entender nuances

### Use H√≠brido quando:
- Quer o melhor dos dois mundos
- DirectQuery para padr√µes comuns
- LLM para casos complexos/novos

## üìù Exemplo de Sa√≠da

```
================================================================================
TESTE COMPLETO DAS 80 PERGUNTAS DE NEG√ìCIO - 100% LLM
================================================================================
In√≠cio: 2025-10-19 10:30:00

Inicializando GraphAgent com LLM...
[OK] GraphAgent inicializado com Gemini

================================================================================
[CATEGORIA] üéØ Vendas por Produto
================================================================================

[1/80] Testando: Gere um gr√°fico de vendas do produto 369947 na UNE SCR...
[OK] SUCCESS: Resposta gerada (542 chars) (2.45s, ~135 tokens)

[2/80] Testando: Mostre a evolu√ß√£o de vendas mensais do produto 369947...
[OK] SUCCESS: Resposta gerada (678 chars) (3.12s, ~169 tokens)

...

================================================================================
ESTAT√çSTICAS FINAIS
================================================================================
LLM usado: Gemini
Total de perguntas testadas: 80
[OK] Sucesso (SUCCESS):        75 (93.8%)
[XX] Erros (ERROR):            3 (3.8%)
[TO] Timeout:                  2 (2.5%)

[$$] Total de tokens estimados: 8,450
[$$] M√©dia de tokens por pergunta: 105.6
```

## üìû Suporte

Em caso de problemas:
1. Verifique as API keys
2. Consulte o relat√≥rio JSON gerado
3. Revise os logs de erro
4. Teste com uma pergunta simples primeiro

---

**√öltima atualiza√ß√£o**: 2025-10-19
**Vers√£o**: 1.0
